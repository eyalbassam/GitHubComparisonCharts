id,RepoId,Level,Language,article,class_index
3431,101782647,3,Python,"What's New:. May 2023 Released models for Scaling Speech Technology to 1,000 Languages  (Pratap, et al., 2023)(examples/mms/README.md) June 2022 Released code for wav2vec-U 2.0 from Towards End-to-end Unsupervised Speech Recognition (Liu, et al., 2022)(examples/wav2vec/unsupervised/README.md) May 2022 Integration with xFormers @link December 2021 Released Direct speech-to-speech translation code(examples/speech_to_speech/README.md) October 2021 Released VideoCLIP and VLM models(examples/MMPT/README.md) October 2021 Released multilingual finetuned XLSR-53 model(examples/wav2vec/README.md) September 2021 master branch renamed to main @link . July 2021 Released DrNMT code(examples/discriminative_reranking_nmt/README.md) July 2021 Released Robust wav2vec 2.0 model(examples/wav2vec/README.md) June 2021 Released XLMR-XL and XLMR-XXL models(examples/xlmr/README.md) May 2021 Released Unsupervised Speech Recognition code(examples/wav2vec/unsupervised/README.md) March 2021 Added full parameter and optimizer state sharding  CPU offloading(examples/fully_sharded_data_parallel/README.md) February 2021 Added LASER training code(examples/laser/README.md) December 2020: Added Adaptive Attention Span code(examples/adaptive_span/README.md) December 2020: GottBERT model and code released(examples/gottbert/README.md) November 2020: Adopted the Hydra @link  configuration framework see documentation explaining how to use it for new and existing projects(docs/hydra_integration.md) November 2020: fairseq 0.10.0 released @link October 2020: Added R3F/R4F (Better Fine-Tuning) code(examples/rxf/README.md) October 2020: Deep Transformer with Latent Depth code released(examples/latent_depth/README.md) October 2020: Added CRISS models and code(examples/criss/README.md) Previous updates September 2020: Added Linformer code(examples/linformer/README.md) September 2020: Added pointer-generator networks(examples/pointer_generator/README.md) August 2020: Added lexically constrained decoding(examples/constrained_decoding/README.md) August 2020: wav2vec2 models and code released(examples/wav2vec/README.md) July 2020: Unsupervised Quality Estimation code released(examples/unsupervised_quality_estimation/README.md) May 2020: Follow fairseq on Twitter @link April 2020: Monotonic Multihead Attention code released(examples/simultaneous_translation/README.md) April 2020: Quant-Noise code released(examples/quant_noise/README.md) April 2020: Initial model parallel support and 11B parameters unidirectional LM released(examples/megatron_11b/README.md) March 2020: Byte-level BPE code released(examples/byte_level_bpe/README.md) February 2020: mBART model and code released(examples/mbart/README.md) February 2020: Added tutorial for back-translation @link . December 2019: fairseq 0.9.0 released @link November 2019: VizSeq released (a visual analysis toolkit for evaluating fairseq models) @link November 2019: CamemBERT model and code released(examples/camembert/README.md) November 2019: BART model and code released(examples/bart/README.md) November 2019: XLM-R models and code released(examples/xlmr/README.md) September 2019: Nonautoregressive translation code released(examples/nonautoregressive_translation/README.md) August 2019: WMT'19 models released(examples/wmt19/README.md) July 2019: fairseq relicensed under MIT license July 2019: RoBERTa models and code released(examples/roberta/README.md) June 2019: wav2vec models and code released(examples/wav2vec/README.md)",1
3432,101782647,3,Python,"Features:. multi-GPU training on one machine or across multiple machines (data and model parallel) fast generation on both CPU and GPU with multiple search algorithms implemented: beam search Diverse Beam Search (Vijayakumar et al., 2016 @link ) sampling (unconstrained, top-k and top-p/nucleus) lexically constrained decoding(examples/constrained_decoding/README.md) (Post & Vilar, 2018) gradient accumulation @link  enables training with large mini-batches even on a single GPU. mixed precision training @link  (trains faster with less GPU memory on NVIDIA tensor cores @link ). extensible @link : easily register new models, criterions, tasks, optimizers and learning rate schedulers flexible configuration(docs/hydra_integration.md) based on Hydra @link  allowing a combination of code, command-line and file based configuration full parameter and optimizer state sharding(examples/fully_sharded_data_parallel/README.md) offloading parameters to CPU(examples/fully_sharded_data_parallel/README.md) We also provide pre-trained models for translation and language modeling(pre-trained-models-and-examples). with a convenient torch.hub interface: @Code See the PyTorch Hub tutorials for translation @link and RoBERTa @link  for more examples.",1
3433,101782647,1,Python,"Requirements and Installation. PyTorch @link  version  1.10.0 Python version  3.8 For training new models, you'll also need an NVIDIA GPU and NCCL @link To install fairseq and develop locally: @Code For faster training install NVIDIA's apex @link  library: @Code For large datasets install PyArrow @link : pip install pyarrow. If you use Docker make sure to increase the shared memory size either with --ipchost or --shm-size as command line options to nvidia-docker run .",2
3434,101782647,1,Python,"Getting Started. The full documentation @link  contains instructions for getting started, training new models and extending fairseq with new model types and tasks.",2
3435,101782647,1,Python,"Pre-trained models and examples. We provide pre-trained models and pre-processed, binarized test sets for several tasks listed below, as well as example training and evaluation commands. Translation(examples/translation/README.md): convolutional and transformer models are available Language Modeling(examples/language_model/README.md): convolutional and transformer models are available We also have more detailed READMEs to reproduce results from specific papers: XLS-R: Self-supervised Cross-lingual Speech Representation Learning at Scale (Babu et al., 2021)(examples/wav2vec/xlsr/README.md) Cross-lingual Retrieval for Iterative Self-Supervised Training (Tran et al., 2020)(examples/criss/README.md) wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations (Baevski et al., 2020)(examples/wav2vec/README.md) Unsupervised Quality Estimation for Neural Machine Translation (Fomicheva et al., 2020)(examples/unsupervised_quality_estimation/README.md) Training with Quantization Noise for Extreme Model Compression ({Fan, Stock} et al., 2020)(examples/quant_noise/README.md) Neural Machine Translation with Byte-Level Subwords (Wang et al., 2020)(examples/byte_level_bpe/README.md) Multilingual Denoising Pre-training for Neural Machine Translation (Liu et at., 2020)(examples/mbart/README.md) Reducing Transformer Depth on Demand with Structured Dropout (Fan et al., 2019)(examples/layerdrop/README.md) Jointly Learning to Align and Translate with Transformer Models (Garg et al., 2019)(examples/joint_alignment_translation/README.md) Levenshtein Transformer (Gu et al., 2019)(examples/nonautoregressive_translation/README.md) Facebook FAIR's WMT19 News Translation Task Submission (Ng et al., 2019)(examples/wmt19/README.md) RoBERTa: A Robustly Optimized BERT Pretraining Approach (Liu et al., 2019)(examples/roberta/README.md) wav2vec: Unsupervised Pre-training for Speech Recognition (Schneider et al., 2019)(examples/wav2vec/README.md) Mixture Models for Diverse Machine Translation: Tricks of the Trade (Shen et al., 2019)(examples/translation_moe/README.md) Pay Less Attention with Lightweight and Dynamic Convolutions (Wu et al., 2019)(examples/pay_less_attention_paper/README.md) Understanding Back-Translation at Scale (Edunov et al., 2018)(examples/backtranslation/README.md) Classical Structured Prediction Losses for Sequence to Sequence Learning (Edunov et al., 2018) @link Hierarchical Neural Story Generation (Fan et al., 2018)(examples/stories/README.md) Scaling Neural Machine Translation (Ott et al., 2018)(examples/scaling_nmt/README.md) Convolutional Sequence to Sequence Learning (Gehring et al., 2017)(examples/conv_seq2seq/README.md) Language Modeling with Gated Convolutional Networks (Dauphin et al., 2017)(examples/language_model/README.conv.md)",1
3436,101782647,1,Python,Join the fairseq community. Twitter:  @link  Facebook page:  @link  Google group:  @link,3
3437,101782647,1,Python,License. fairseq(-py) is MIT-licensed. The license applies to the pre-trained models as well.,3
3438,101782647,1,Python,Citation. Please cite as: @Code,3
3439,114747226,1,Python,deepfakes_faceswap. FaceSwap is a tool that utilizes deep learning to recognize and swap faces in pictures and videos. &nbsp;&nbsp;&nbsp;&nbsp; Emma Stone/Scarlett Johansson FaceSwap using the Phaze-A model Jennifer Lawrence/Steve Buscemi FaceSwap using the Villain model @link Make sure you check out INSTALL.md(INSTALL.md) before getting started. - deepfakes_faceswap(deepfakes_faceswap). - Manifesto(manifesto). - FaceSwap has ethical uses.(faceswap-has-ethical-uses). - How To setup and run the project(how-to-setup-and-run-the-project). - Overview(overview). - Extract(extract). - Train(train). - Convert(convert). - GUI(gui). - General notes:(general-notes). - Help I need support!(help-i-need-support). - Discord Server(discord-server). - FaceSwap Forum(faceswap-forum). - Donate(donate). - Patreon(patreon). - One time Donations(one-time-donations). - @torzdf(torzdf). - @andenixa(andenixa). - How to contribute(how-to-contribute). - For people interested in the generative models(for-people-interested-in-the-generative-models). - For devs(for-devs). - For non-dev advanced users(for-non-dev-advanced-users). - For end-users(for-end-users). - For haters(for-haters). - About github.com/deepfakes(about-githubcomdeepfakes). - What is this repo?(what-is-this-repo). - Why this repo?(why-this-repo). - Why is it named 'deepfakes' if it is not /u/deepfakes?(why-is-it-named-deepfakes-if-it-is-not-udeepfakes). - What if /u/deepfakes feels bad about that?(what-if-udeepfakes-feels-bad-about-that). - About machine learning(about-machine-learning). - How does a computer know how to recognize/shape faces? How does machine learning work? What is a neural network?(how-does-a-computer-know-how-to-recognizeshape-faces-how-does-machine-learning-work-what-is-a-neural-network).,1
3440,114747226,2,Python,"FaceSwap has ethical uses.. When faceswapping was first developed and published, the technology was groundbreaking, it was a huge step in AI development. It was also completely ignored outside of academia because the code was confusing and fragmentary. It required a thorough understanding of complicated AI techniques and took a lot of effort to figure it out. Until one individual brought it together into a single, cohesive collection. It ran, it worked, and as is so often the way with new technology emerging on the internet, it was immediately used to create inappropriate content. Despite the inappropriate uses the software was given originally, it was the first AI code that anyone could download, run and learn by experimentation without having a Ph.D. in math, computer theory, psychology, and more. Before 'deepfakes' these techniques were like black magic, only practiced by those who could understand all of the inner workings as described in esoteric and endlessly complicated books and papers. 'Deepfakes' changed all that and anyone could participate in AI development. To us, developers, the release of this code opened up a fantastic learning opportunity. It allowed us to build on ideas developed by others, collaborate with a variety of skilled coders, experiment with AI whilst learning new skills and ultimately contribute towards an emerging technology which will only see more mainstream use as it progresses. Are there some out there doing horrible things with similar software? Yes. And because of this, the developers have been following strict ethical standards. Many of us don't even use it to create videos, we just tinker with the code to see what it does. Sadly, the media concentrates only on the unethical uses of this software. That is, unfortunately, the nature of how it was first exposed to the public, but it is not representative of why it was created, how we use it now, or what we see in its future. Like any technology, it can be used for good or it can be abused. It is our intention to develop FaceSwap in a way that its potential for abuse is minimized whilst maximizing its potential as a tool for learning, experimenting and, yes, for legitimate faceswapping. We are not trying to denigrate celebrities or to demean anyone. We are programmers, we are engineers, we are Hollywood VFX artists, we are activists, we are hobbyists, we are human beings. To this end, we feel that it's time to come out with a standard statement of what this software is and isn't as far as us developers are concerned. - FaceSwap is not for creating inappropriate content. - FaceSwap is not for changing faces without consent or with the intent of hiding its use. - FaceSwap is not for any illicit, unethical, or questionable purposes. - FaceSwap exists to experiment and discover AI techniques, for social or political commentary, for movies, and for any number of ethical and reasonable uses. We are very troubled by the fact that FaceSwap can be used for unethical and disreputable things. However, we support the development of tools and techniques that can be used ethically as well as provide education and experience in AI for anyone who wants to learn it hands-on. We will take a zero tolerance approach to anyone using this software for any unethical purposes and will actively discourage any such uses.",3
3441,114747226,1,Python,"How To setup and run the project. FaceSwap is a Python program that will run on multiple Operating Systems including Windows, Linux, and MacOS. See INSTALL.md(INSTALL.md) for full installation instructions. You will need a modern GPU with CUDA support for best performance. Many AMD GPUs are supported through DirectML (Windows) and ROCm (Linux).",2
3442,114747226,1,Python,Overview. The project has multiple entry points. You will have to: - Gather photos and/or videos - Extract faces from your raw photos - Train a model on the faces extracted from the photos/videos - Convert your sources with the model Check out USAGE.md(USAGE.md) for more detailed instructions.,2
3443,114747226,2,Python,"Extract. From your setup folder, run python faceswap.py extract. This will take photos from src folder and extract faces into extract folder.",2
3444,114747226,2,Python,"Train. From your setup folder, run python faceswap.py train. This will take photos from two folders containing pictures of both faces and train a model that will be saved inside the models folder.",2
3445,114747226,2,Python,"Convert. From your setup folder, run python faceswap.py convert. This will take photos from original folder and apply new faces into modified folder.",2
3446,114747226,2,Python,"GUI. Alternatively, you can run the GUI by running python faceswap.py gui",2
3447,114747226,1,Python,"General notes:. - All of the scripts mentioned have -h/--help options with arguments that they will accept. You're smart, you can figure out how this works, right?! NB: there is a conversion tool for video. This can be accessed by running python tools.py effmpeg -h. Alternatively, you can use ffmpeg @link  to convert video into photos, process images, and convert images back to the video. Some tips: Reusing existing models will train much faster than starting from nothing. If there is not enough training data, start with someone who looks similar, then switch the data.",2
3449,114747226,2,Python,"Discord Server. Your best bet is to join the FaceSwap Discord server @link  where there are plenty of users willing to help. Please note that, like this repo, this is a SFW Server!",3
3450,114747226,2,Python,"FaceSwap Forum. Alternatively, you can post questions in the FaceSwap Forum @link . Please do not post general support questions in this repo as they are liable to be deleted without response.",3
3451,114747226,1,Python,"Donate. The developers work tirelessly to improve and develop FaceSwap. Many hours have been put in to provide the software as it is today, but this is an extremely time-consuming process with no financial reward. If you enjoy using the software, please consider donating to the devs, so they can spend more time implementing improvements.",3
3452,114747226,2,Python,Patreon. The best way to support us is through our Patreon page: @link,3
3453,114747226,2,Python,One time Donations. Alternatively you can give a one off donation to any of our Devs:,3
3454,114747226,3,Python,"@torzdf. There is very little FaceSwap code that hasn't been touched by torzdf. He is responsible for implementing the GUI, FAN aligner, MTCNN detector and porting the Villain, DFL-H128 and DFaker models to FaceSwap, as well as significantly improving many areas of the code. Bitcoin: bc1qpm22suz59ylzk0j7qk5e4c7cnkjmve2rmtrnc6 Ethereum: 0xd3e954dC241B87C4E8E1A801ada485DC1d530F01 Monero: 45dLrtQZ2pkHizBpt3P3yyJKkhcFHnhfNYPMSnz3yVEbdWm3Hj6Kr5TgmGAn3Far8LVaQf1th2n3DJVTRkfeB5ZkHxWozSX Paypal:  @link",3
3455,114747226,3,Python,"@andenixa. Creator of the Unbalanced and OHR models, as well as expanding various capabilities within the training process. Andenixa is currently working on new models and will take requests for donations. Paypal:  @link",3
3457,114747226,2,Python,For people interested in the generative models. - Go to the 'faceswap-model' to discuss/suggest/commit alternatives to the current algorithm.,3
3458,114747226,2,Python,"For devs. - Read this README entirely - Fork the repo - Play with it - Check issues with the 'dev' tag - For devs more interested in computer vision and openCV, look at issues with the 'opencv' tag. Also feel free to add your own alternatives/improvements",3
3459,114747226,2,Python,For non-dev advanced users. - Read this README entirely - Clone the repo - Play with it - Check issues with the 'advuser' tag - Also go to the 'faceswap Forum @link ' and help others.,3
3460,114747226,2,Python,For end-users. - Get the code here and play with it if you can - You can also go to the faceswap Forum @link  and help or get help from others. - Be patient. This is a relatively new technology for developers as well. Much effort is already being put into making this program easy to use for the average user. It just takes time! - Notice Any issue related to running the code has to be opened in the faceswap Forum @link !,3
3461,114747226,2,Python,"For haters. Sorry, no time for that.",3
3462,114747226,2,Python,What is this repo?. It is a community repository for active users.,3
3463,114747226,2,Python,Why this repo?. The joshua-wu repo seems not active. Simple bugs like missing _ @link in front of urls have not been solved since days.,3
3464,114747226,2,Python,Why is it named 'deepfakes' if it is not /u/deepfakes?. 1. Because a typosquat would have happened sooner or later as project grows 2. Because we wanted to recognize the original author 3. Because it will better federate contributors and users,3
3465,114747226,2,Python,"What if /u/deepfakes feels bad about that?. This is a friendly typosquat, and it is fully dedicated to the project. If /u/deepfakes wants to take over this repo/user and drive the project, he is welcomed to do so (Raise an issue, and he will be contacted on Reddit). Please do not send /u/deepfakes messages for help with the code you find here.",3
3467,114747226,2,Python,How does a computer know how to recognize/shape faces? How does machine learning work? What is a neural network?. It's complicated. Here's a good video that makes the process understandable: @link Here's a slightly more in depth video that tries to explain the basic functioning of a neural network: @link tl;dr: training data  trial and error,2
3468,118861276,3,Python,Frozen Set. Is immutable and hashable. That means it can be used as a key in a dictionary or as an element in a set. @Code Tuple ----- Tuple is an immutable and hashable list. @Code,2
3469,118861276,3,Python,Named Tuple. Tuple's subclass with named elements. @Code Range ----- Immutable and hashable sequence of integers. @Code @Code Enumerate --------- @Code Iterator -------- @Code,2
3470,118861276,3,Python,Itertools. @Code @Code @Code @Code Generator --------- Any function that contains a yield statement returns a generator. Generators and iterators are interchangeable. @Code @Code Type ---- Everything is an object. Every object has a type. Type and class are synonymous. @Code @Code,2
3471,118861276,4,Python,"Some types do not have built-in names, so they must be imported:. @Code",2
3472,118861276,3,Python,"Abstract Base Classes. Each abstract base class specifies a set of virtual subclasses. These classes are then recognized by isinstance() and issubclass() as subclasses of the ABC, although they are really not. ABC can also manually decide whether or not a specific class is its virtual subclass, usually based on which methods the class has implemented. For instance, Iterable ABC looks for method iter(), while Collection ABC looks for iter(), contains() and len(). @Code @Code @Code @Code String ------ Immutable sequence of characters. @Code @Code @Code @Code @Code Use 'unicodedata.normalize('NFC', )' on strings like 'Motörhead' before comparing them to other strings, because 'ö' can be stored as one or two characters. 'NFC' converts such characters to a single character, while 'NFD' converts them to two.",2
3473,118861276,3,Python,"Property Methods. @Code Regex ----- Functions for regular expression matching. @Code Raw string literals do not interpret escape sequences, thus enabling us to use regex-specific escape sequences that cause SyntaxWarning in normal string literals (since 3.12). Argument 'new' of re.sub() can be a function that accepts Match object and returns a str. Argument 'flagsre.IGNORECASE' can be used with all functions. Argument 'flagsre.MULTILINE' makes '' and '$' match the start/end of each line. Argument 'flagsre.DOTALL' makes '.' also accept the '\n'. 're.compile()' returns a Pattern object with methods sub(), findall(), …",2
3474,118861276,3,Python,"Special Sequences. @Code By default, decimal characters, alphanumerics and whitespaces from all alphabets are matched unless 'flagsre.ASCII' argument is used. It restricts special sequence matches to '\x00-\x7f' (the first 128 characters) and also prevents '\s' from accepting '\x1c-\x1f' (the so-called separator characters). Use a capital letter for negation (all non-ASCII characters will be matched when used in combination with ASCII flag). Format ------ @Code",2
3475,118861276,3,Python,"General Options. @Code Objects are rendered using 'format(, )'. Options can be generated dynamically: f'{:{}…}'. Adding '' to the expression prepends it to the output: f'{11}' returns '112'. Adding '!r' to the expression converts object to string by calling its repr()(class) method..",2
3476,118861276,4,Python,"Comparison of presentation types:. @Code @Code '{:g}' is '{:.6}' with stripped zeros, exponent starting at '1e06'. When both rounding up and rounding down are possible, the one that returns result with even last digit is chosen. That makes '{6.5:.0f}' a '6' and '{7.5:.0f}' an '8'. This rule only effects numbers that can be represented exactly by a float (.5, .25, …).",2
3477,118861276,3,Python,"Ints. @Code Numbers ------- @Code 'int()' and 'float()' raise ValueError on malformed strings. Decimal numbers are stored exactly, unlike most floats where '1.1  2.2 ! 3.3'. Floats can be compared with: 'math.isclose(, )'. Precision of decimal operations is set with: 'decimal.getcontext().prec  '.",2
3478,118861276,3,Python,"Bitwise Operators. @Code Combinatorics ------------- @Code @Code @Code @Code @Code @Code Datetime -------- Provides 'date', 'time', 'datetime' and 'timedelta' classes. All are immutable and hashable. @Code @Code Aware  time and datetime objects have defined timezone, while naive  don't. If object is naive, it is presumed to be in the system's timezone! 'fold1' means the second pass in case of time jumping back for one hour. Timedelta normalizes arguments to days, seconds (.weekday()' to get the day of the week as an int, with Monday being 0.",2
3479,118861276,3,Python,"Now. @Code To extract time use '.time()', '.time()' or '.timetz()'.",2
3480,118861276,3,Python,"Timezone. @Code Timezones returned by gettz(), tzlocal(), and implicit local timezone of naive objects have offsets that vary through time due to DST and historical changes of the zone's base offset. Standard library's zoneinfo.ZoneInfo() can be used instead of gettz() on Python 3.9 and later. It requires 'tzdata' package on Windows. It doesn't return local tz if arg. is omitted.",2
3481,118861276,3,Python,"Encode. @Code ISO strings come in following forms: 'YYYY-MM-DD', 'HH:MM:SS.mmmuuuHH:MM', or both separated by an arbitrary character. All parts following the hours are optional. Python uses the Unix Epoch: '1970-01-01 00:00 UTC', '1970-01-01 01:00 CET', ...",2
3482,118861276,3,Python,"Format. @Code '%z' accepts 'HH:MM' and returns 'HHMM' or empty string if datetime is naive. '%Z' accepts 'UTC/GMT' and local timezone's code and returns timezone's name, 'UTCHH:MM' if timezone is nameless, or an empty string if datetime is naive.",2
3483,118861276,3,Python,Inside Function Call. @Code,2
3484,118861276,3,Python,Inside Function Definition. @Code Default values are evaluated when function is first encountered in the scope. Any mutation of a mutable default value will persist between invocations! Splat Operator --------------,2
3485,118861276,3,Python,"Inside Function Call. Splat expands a collection into positional arguments, while splatty-splat expands a dictionary into keyword arguments. @Code",2
3487,118861276,3,Python,"Inside Function Definition. Splat combines zero or more positional arguments into a tuple, while splatty-splat combines zero or more keyword arguments into a dictionary. @Code @Code",2
3488,118861276,4,Python,Legal argument combinations:. @Code @Code @Code @Code,2
3489,118861276,3,Python,"Map, Filter, Reduce. @Code @Code",2
3490,118861276,3,Python,"Named Tuple, Enum, Dataclass. @Code @Code @Code Imports ------- Mechanism that makes code in one file available to another file. @Code Package is a collection of modules, but it can also define its own objects. On a filesystem this corresponds to a directory of Python files with an optional init script. Running 'import ' does not automatically provide access to the package's modules unless they are explicitly imported in its init script. Location of the file that is passed to python command serves as a root of all local imports. For relative imports use 'from .….… import '. Closure ------- We have/get a closure in Python when a nested function references a value of its enclosing function and then the enclosing function returns the nested function. @Code @Code Any value that is referenced from within multiple nested functions gets shared.",2
3491,118861276,3,Python,"Partial. @Code @Code Partial is also useful in cases when function needs to be passed as an argument because it enables us to set its arguments beforehand. A few examples being: 'defaultdict()', 'iter(, to_exc)' and dataclass's 'field(default_factory)'.",2
3492,118861276,3,Python,"Non-Local. If variable is being assigned to anywhere in the scope, it is regarded as a local variable, unless it is declared as a 'global' or a 'nonlocal'. @Code @Code Decorator --------- A decorator takes a function, adds some functionality and returns it. It can be any callable(callable), but is usually implemented as a function that returns a closure(closure).. @Code",2
3493,118861276,3,Python,"Debugger Example. Decorator that prints function's name every time the function is called. @Code Wraps is a helper decorator that copies the metadata of the passed function (func) to the function it is wrapping (out). Without it, 'add.__name__' would return 'out'.",2
3494,118861276,3,Python,"LRU Cache. Decorator that caches function's return values. All function's arguments must be hashable. @Code python from functools import wraps def debug(print_resultFalse): def decorator(func): @wraps(func) def out(args, kwargs): result  func(args, kwargs) print(func.__name__, result if print_result else '') return result return out return decorator @debug(print_resultTrue) def add(x, y): return x  y @Code python class MyClass: def __init__(self, a): self.a  a def __str__(self): return str(self.a) def __repr__(self): class_name  self.__class__.__name__ return f'{class_name}({self.a!r})' @classmethod def get_class_name(cls): return cls.__name__ @Code python obj  MyClass(1) obj.a, str(obj), repr(obj) (1, '1', 'MyClass(1)') @Code python print() f'{}' logging.warning() csv.writer().writerow() raise Exception() @Code python print/str/repr() print/str/repr({: }) f'{!r}' Z  dataclasses.make_dataclass('Z', 'a'); print/str/repr(Z()) @Code python class Person: def __init__(self, name): self.name  name class Employee(Person): def __init__(self, name, staff_num): super().__init__(name) self.staff_num  staff_num @Code python class A: pass class B: pass class C(A, B): pass @Code python C.mro() , , , @Code python from collections import abc :  : list/set/abc.Iterable/abc.Sequence     Since 3.9.. : dict/tuple, ...                        Since 3.9.. @Code python from dataclasses import dataclass, field, make_dataclass @dataclass(orderFalse, frozenFalse) class : : : : list/dict/set  field(default_factorylist/dict/set) @Code python make_dataclass('', ) make_dataclass('', ) ('',  , ) @Code python class Person: @property def name(self): return ' '.join(self._name) @name.setter def name(self, value): self._name  value.split() @Code python person  Person() person.name  '\t Guido  van Rossum \n' person.name 'Guido van Rossum' @Code python class MyClassWithSlots: __slots__  'a' def __init__(self): self.a  1 @Code python from copy import copy, deepcopy copy/deepcopy() @Code python class MyComparable: def __init__(self, a): self.a  a def __eq__(self, other): if isinstance(other, type(self)): return self.a  other.a return NotImplemented @Code python class MyHashable: def __init__(self, a): self._a  a @property def a(self): return self._a def __eq__(self, other): if isinstance(other, type(self)): return self.a  other.a return NotImplemented def __hash__(self): return hash(self.a) @Code python from functools import total_ordering @total_ordering class MySortable: def __init__(self, a): self.a  a def __eq__(self, other): if isinstance(other, type(self)): return self.a  other.a return NotImplemented def __lt__(self, other): if isinstance(other, type(self)): return self.a  counter  Counter() next(counter), next(counter), next(counter) (1, 2, 3) @Code python class Counter: def __init__(self): self.i  0 def __call__(self): self.i  1 return self.i @Code python counter  Counter() counter(), counter(), counter() (1, 2, 3) @Code python class MyOpen: def __init__(self, filename): self.filename  filename def __enter__(self): self.file  open(self.filename) return self.file def __exit__(self, exc_type, exception, traceback): self.file.close() @Code python with open('test.txt', 'w') as file: ...     file.write('Hello World!') with MyOpen('test.txt') as file: ...     print(file.read()) Hello World! @Code python class MyIterable: def __init__(self, a): self.a  a def __iter__(self): return iter(self.a) def __contains__(self, el): return el in self.a @Code python obj  MyIterable(1, 2, 3) el for el in obj 1, 2, 3 1 in obj True @Code python class MyCollection: def __init__(self, a): self.a  a def __iter__(self): return iter(self.a) def __contains__(self, el): return el in self.a def __len__(self): return len(self.a) @Code python class MySequence: def __init__(self, a): self.a  a def __iter__(self): return iter(self.a) def __contains__(self, el): return el in self.a def __len__(self): return len(self.a) def __getitem__(self, i): return self.ai def __reversed__(self): return reversed(self.a) @Code python from collections import abc class MyAbcSequence(abc.Sequence): def __init__(self, a): self.a  a def __len__(self): return len(self.a) def __getitem__(self, i): return self.ai @Code text -------------------------------------------------------------- -------------------------------------------------------------- -------------------------------------------------------------- @Code python from enum import Enum, auto @Code python class (Enum): auto()               Increment of the last numeric value or 1..",2
3495,118861276,1,Python,"Values don't have to be hashable.. ,      Values can be collections (like this tuple).. @Code python .          Returns a member. Raises AttributeError.. ''       Returns a member. Raises KeyError.. ()               Returns a member. Raises ValueError.. .name                 Returns member's name.. .value                Returns member's value.. @Code python list()                  Returns enum's members.. a.name for a in       Returns enum's member names.. a.value for a in      Returns enum's member values.. @Code python type()                Returns member's enum.. itertools.cycle()       Returns endless iterator of members.. random.choice(list())   Returns a random member.. @Code python Cutlery  Enum('Cutlery', 'FORK KNIFE SPOON') Cutlery  Enum('Cutlery', 'FORK', 'KNIFE', 'SPOON') Cutlery  Enum('Cutlery', {'FORK': 1, 'KNIFE': 2, 'SPOON': 3}) @Code python from functools import partial LogicOp  Enum('LogicOp', {'AND': partial(lambda l, r: l and r), 'OR':  partial(lambda l, r: l or r)}) @Code python try: except : @Code python try: except : except : else: finally: @Code python except : ... except  as : ... except (, ...): ... except (, ...) as : ... @Code python raise raise () raise ( , ...) @Code python except  as : ... raise @Code python arguments  .args exc_type   .__class__ filename   .__traceback__.tb_frame.f_code.co_filename func_name  .__traceback__.tb_frame.f_code.co_name line       linecache.getline(filename, .__traceback__.tb_lineno) trace_str  ''.join(traceback.format_tb(.__traceback__)) error_msg  ''.join(traceback.format_exception(type(), , .__traceback__)) @Code text BaseException -- SystemExit                    Raised by the sys.exit() function.. -- KeyboardInterrupt             Raised when the user hits the interrupt key (ctrl-c).. -- Exception                     User-defined exceptions should be derived from this class.. -- ArithmeticError          Base class for arithmetic errors such as ZeroDivisionError.. -- AssertionError           Raised by assert  if expression returns false value.. -- AttributeError           Raised when object doesn't have requested attribute/method.. -- EOFError                 Raised by input() when it hits an end-of-file condition.. -- LookupError              Base class for errors when a collection can't find an item.. -- MemoryError              Out of memory. May be too late to start deleting objects.. -- NameError                Raised when nonexistent name (variable/func/class) is used.. -- OSError                  Errors such as FileExistsError/TimeoutError (see Open).. -- RuntimeError             Raised by errors that don't fall into other categories.. -- StopIteration            Raised when an empty iterator is passed to next().. -- TypeError                When an argument of the wrong type is passed to function.. -- ValueError               When argument has the right type but inappropriate value.. @Code text ----------------------------------------------- ----------------------------------------------- ----------------------------------------------- @Code python raise TypeError('Argument is of the wrong type!') raise ValueError('Argument has the right type but an inappropriate value!') raise RuntimeError('I am too lazy to define my own exception!') @Code python class MyError(Exception): pass class MyInputError(MyError): pass @Code python import sys sys.exit()                         Exits with exit code 0 (success).. sys.exit()                     Prints to stderr and exits with 1.. sys.exit()                    Exits with the passed exit code.. @Code python print(, ..., sep' ', end'\n', filesys.stdout, flushFalse) @Code python from pprint import pprint pprint(, width80, depthNone, compactFalse, sort_dictsTrue) @Code python input(promptNone) @Code python import sys scripts_path  sys.argv0 arguments     sys.argv1: @Code python from argparse import ArgumentParser, FileType p  ArgumentParser(description)                              Returns a parser.. p.add_argument('-', '--', action'store_true')   Flag (defaults to False).. p.add_argument('-', '--', type)           Option (defaults to None).. p.add_argument('', type, nargs1)                     Mandatory first argument.. p.add_argument('', type, nargs'')                   Mandatory remaining args.. p.add_argument('', type, nargs'?/')                 Optional argument/s.. p.parse_args()                                            Exits on parsing error.. .                                             Returns ().. @Code python open(, mode'r', encodingNone, newlineNone) @Code python .seek(0)                       Moves to the start of the file.. .seek(offset)                  Moves 'offset' chars/bytes from the start.. .seek(0, 2)                    Moves to the end of the file.. .seek(offset, )   Anchor: 0 start, 1 current position, 2 end.. @Code python .read(size-1)   Reads 'size' chars/bytes or until EOF.. .readline()      Returns a line or empty string/bytes on EOF.. .readlines()     Returns a list of remaining lines.. next()           Returns a line using buffer. Do not mix.. @Code python .write()            Writes a string or bytes object.. .writelines()      Writes a coll. of strings or bytes objects.. .flush()                       Flushes write buffer. Runs every 4096/8192 B.. .close()                       Closes the file after flushing write buffer.. @Code python def read_file(filename): with open(filename, encoding'utf-8') as file: return file.readlines() @Code python def write_to_file(filename, text): with open(filename, 'w', encoding'utf-8') as file: file.write(text) @Code python import os, glob from pathlib import Path @Code python os.getcwd()                 Returns shell's working dir unless changed.. os.path.join(, ...)   Joins two or more pathname components.. os.path.realpath()    Resolves symlinks and calls path.abspath().. @Code python os.path.basename()    Returns final component of the path.. os.path.dirname()     Returns path without the final component.. os.path.splitext()    Splits on last period of the final component.. @Code python os.listdir(path'.')        Returns filenames located at the path.. glob.glob('')      Returns paths matching the wildcard pattern.. @Code python os.path.exists()      Or: .exists(). os.path.isfile()      Or: .is_file(). os.path.isdir()       Or: .is_dir(). @Code python os.stat()             Or: .stat(). .st_mtime/st_size/…   Modification time, size in bytes, .... @Code python os.scandir(path'.')        Returns DirEntry objects located at the path.. .path             Returns the whole path as a string.. .name             Returns final component as a string.. open()            Opens the file and returns a file object.. @Code python Path( , ...)        Accepts strings, Paths and DirEntry objects.. /  / ...     First or second path must be a Path object.. .resolve()            Returns absolute path with resolved symlinks.. @Code python Path()                      Returns relative cwd. Also Path('.').. Path.cwd()                  Returns absolute cwd. Also Path().resolve().. Path.home()                 Returns user's home directory (absolute).. Path(__file__).resolve()    Returns script's path if cwd wasn't changed.. @Code python .parent               Returns Path without the final component.. .name                 Returns final component as a string.. .stem                 Returns final component without extension.. .suffix               Returns final component's extension.. .parts                Returns all components as strings.. @Code python .iterdir()            Returns directory contents as Path objects.. .glob('')    Returns Paths matching the wildcard pattern.. @Code python str()                 Returns path as a string.. open()                Also .read/write_text/bytes().. @Code python import os, shutil, subprocess @Code python os.chdir()                     Changes the current working directory.. os.mkdir(, mode0o777)         Creates a directory. Permissions are in octal.. os.makedirs(, mode0o777)      Creates all path's dirs. Also exist_okFalse.. @Code python shutil.copy(from, to)                Copies the file. 'to' can exist or be a dir.. shutil.copy2(from, to)               Also copies creation and modification time.. shutil.copytree(from, to)            Copies the directory. 'to' must not exist.. @Code python os.rename(from, to)                  Renames/moves the file or directory.. os.replace(from, to)                 Same, but overwrites file 'to' even on Windows.. shutil.move(from, to)                Rename() that moves into 'to' if it's a dir.. @Code python os.remove()                    Deletes the file.. os.rmdir()                     Deletes the empty directory.. shutil.rmtree()                Deletes the directory.. @Code python os.popen('')       Executes command in sh/cmd. Returns its stdout pipe.. .read(size-1)        Reads 'size' chars or until EOF. Also readline/s().. .close()              Closes the pipe. Returns None on success (returncode 0).. @Code python subprocess.run('bc', input'1  1\n', capture_outputTrue, textTrue) CompletedProcess(args'bc', returncode0, stdout'2\n', stderr'') @Code python from shlex import split os.popen('echo 1  1  test.in') subprocess.run(split('bc -s'), stdinopen('test.in'), stdoutopen('test.out', 'w')) CompletedProcess(args'bc', '-s', returncode0) open('test.out').read() '2\n' @Code python import json json.dumps()      Converts object to JSON string.. json.loads()         Converts JSON string to object.. @Code python def read_json_file(filename): with open(filename, encoding'utf-8') as file: return json.load(file) @Code python def write_to_json_file(filename, an_object): with open(filename, 'w', encoding'utf-8') as file: json.dump(an_object, file, ensure_asciiFalse, indent2) @Code python import pickle pickle.dumps()    Converts object to bytes object.. pickle.loads()     Converts bytes object to object.. @Code python def read_pickle_file(filename): with open(filename, 'rb') as file: return pickle.load(file) @Code python def write_to_pickle_file(filename, an_object): with open(filename, 'wb') as file: pickle.dump(an_object, file) @Code python import csv @Code python csv.reader()        Also: dialect'excel', delimiter','.. next()            Returns next row as a list of strings.. list()            Returns a list of remaining rows.. @Code python csv.writer()        Also: dialect'excel', delimiter','.. .writerow()      Encodes objects using str().. .writerows()   Appends multiple rows.. @Code text ------------------------------------------------------------ ------------------------------------------------------------ ------------------------------------------------------------ @Code python def read_csv_file(filename, dialect'excel', params): with open(filename, encoding'utf-8', newline'') as file: return list(csv.reader(file, dialect, params)) @Code python def write_to_csv_file(filename, rows, mode'w', dialect'excel', params): with open(filename, mode, encoding'utf-8', newline'') as file: writer  csv.writer(file, dialect, params) writer.writerows(rows) @Code python import sqlite3 sqlite3.connect()                 Opens existing or new file. Also ':memory:'.. .close()                                   Closes the connection.. @Code python .execute('')             Can raise a subclass of sqlite3.Error.. .fetchone()                   Returns next row. Also next().. .fetchall()                   Returns remaining rows. Also list().. @Code python .execute('')                        Can raise a subclass of sqlite3.Error.. .commit()                                  Saves all changes since the last commit.. .rollback()                                Discards all changes since the last commit.. @Code python with :                                     Exits the block with commit() or rollback(),. .execute('')                    depending on whether any exception occurred.. @Code python .execute('', )          Replaces '?'s in query with values.. .execute('', )     Replaces ':'s with values.. .executemany('', )   Runs execute() multiple times.. @Code python conn  sqlite3.connect('test.db') conn.execute('CREATE TABLE person (person_id INTEGER PRIMARY KEY, name, height)') conn.execute('INSERT INTO person VALUES (NULL, ?, ?)', ('Jean-Luc', 187)).lastrowid 1 conn.execute('SELECT  FROM person').fetchall() (1, 'Jean-Luc', 187) @Code python",1
3496,118861276,1,Python,"$ pip3 install sqlalchemy. from sqlalchemy import create_engine, text create_engine('')                Url: 'dialect://user:password@host/dbname'.. .connect()                    Creates a connection. Also .close().. .execute(text(''), …)    Replaces ':'s with keyword arguments.. with .begin(): ...                         Exits the block with commit or rollback.. @Code text ---------------------------------------------------------------------- ---------------------------------------------------------------------- ---------------------------------------------------------------------- @Code python b''                        Only accepts ASCII characters and \x00-\xff.. index                  Returns an int in range from 0 to 255..",1
3497,118861276,1,Python,"Returns bytes even if it has only one element.. .join()   Joins elements using bytes as a separator.. @Code python bytes()           Ints must be in range from 0 to 255.. bytes(, 'utf-8')           Encodes the string. Also .encode().. bytes.fromhex('')          Hex pairs can be separated by whitespaces.. .to_bytes(n_bytes, …)      byteorder'big/little', signedFalse.. @Code python list()                   Returns ints in range from 0 to 255.. str(, 'utf-8')           Returns a string. Also .decode().. .hex()                   Returns hex pairs. Accepts sep.. int.from_bytes(, …)      byteorder'big/little', signedFalse.. @Code python def read_bytes(filename): with open(filename, 'rb') as file: return file.read() @Code python def write_bytes(filename, bytes_obj): with open(filename, 'wb') as file: file.write(bytes_obj) @Code python from struct import pack, unpack pack('',  , ...)   Packs objects according to format string.. unpack('', )        Use iter_unpack() to get iterator of tuples.. @Code python pack('hhl', 1, 2, 3) b'\x00\x01\x00\x02\x00\x00\x00\x03' unpack('hhl', b'\x00\x01\x00\x02\x00\x00\x00\x03') (1, 2, 3) @Code python from array import array @Code python array('', )   Array from collection of numbers.. array('', )          Array from bytes object.. array('', )          Treats array as a sequence of numbers.. .fromfile(, n_items)               Appends items from the binary file.. @Code python bytes()                        Returns a copy of array's memory.. .write()                           Writes array to the binary file.. @Code python memoryview()   Immutable if bytes, else mutable.. index                        Returns an int or a float..",1
3498,118861276,1,Python,"Returns mview with rearranged elements.. .cast('')            Only works between b/B/c and other types.. .release()                               Releases memory buffer of the base object.. @Code python bytes()                        Returns a new bytes object.. .join()        Joins mviews using bytes as a separator.. array('', )          Treats mview as a sequence of numbers.. .write()                           Writes mview to the binary file.. @Code python list()                         Returns a list of ints or floats.. str(, 'utf-8')                 Treats mview as a bytes object.. .hex()                         Returns hex pairs. Accepts sep.. @Code python from collections import deque @Code python deque()                   Use maxlen to set size limit.. .appendleft()                        Opposite element is dropped if full.. .extendleft()                Passed collection gets reversed.. .rotate(n1)                             Last element becomes first.. .popleft()                        Raises IndexError if deque is empty.. @Code python from threading import Thread, Lock, RLock, Semaphore, Event, Barrier from concurrent.futures import ThreadPoolExecutor, as_completed @Code python Thread(target)            Use args to set the arguments.. .start()                                Starts the thread. Also .is_alive().. .join()                                 Waits for the thread to finish.. @Code python Lock/RLock()                           RLock can only be released by acquirer.. .acquire()                                Waits for the lock to be available.. .release()                                Makes the lock available again.. @Code python with :                                    Enters the block by calling acquire() and. ...                                         exits it with release(), even on error.. @Code python Semaphore(value1)                Lock that can be acquired by 'value' threads.. Event()                           Method wait() blocks until set() is called.. Barrier(n_times)                  Wait() blocks until it's called n_times.. @Code python queue.Queue(maxsize0)                A thread-safe first-in-first-out queue.. .put()                               Blocks until queue stops being full.. .put_nowait()                        Raises queue.Full exception if full.. .get()                            Blocks until queue stops being empty.. .get_nowait()                     Raises queue.Empty exception if empty.. @Code python ThreadPoolExecutor(max_workersNone)   Or: with ThreadPoolExecutor() as : .... .map(, , ...)      Multithreaded and non-lazy map(). Keeps order.. .submit(, , ...)    Creates a thread and returns its Future obj.. .shutdown()                               Blocks until all threads finish executing.. @Code python .done()                        Checks if the thread has finished executing.. .result(timeoutNone)          Waits for thread to finish and returns result.. .cancel()                      Cancels or returns False if running/finished.. as_completed()        Next() waits for next completed Future.. @Code python import operator as op op.not_()                                         or, and, not (or/and missing). op.eq/ne/lt/le/gt/ge/is_/contains(, )        , !, , , is, in. op.or_/xor/and_(, )                     op.lshift/rshift(, )                         . op.add/sub/mul/truediv/floordiv/mod(, )      , -, , /, //, %. op.neg/invert()                                   -, . op.pow(, )                                   . op.itemgetter/attrgetter/methodcaller( , ...)   index/key, .name, .name(). @Code python elementwise_sum   map(op.add, list_a, list_b) sorted_by_second  sorted(, keyop.itemgetter(1)) sorted_by_both    sorted(, keyop.itemgetter(1, 0)) product_of_elems  functools.reduce(op.mul, ) first_element     op.methodcaller('pop', 0)() @Code python match : case  if : ... @Code python 1/'abc'/True/None/math.pi           Matches the literal or a dotted name.. ()                            Matches any object of that type.. _                                   Matches any object..",1
3499,118861276,1,Python,"Matches any object and binds it to name.. as                  Binds the match to the name.. , ...                    Matches sequence with matching items.. {: , ...}   Matches dictionary with matching items.. (, ...)     Matches object with matching attributes.. @Code python from pathlib import Path match Path('/home/gto/python-cheatsheet/README.md'): ...     case Path( ...         parts'/', 'home', user, _, ...         stemstem, ...         suffix('.md' ...     ) if stem.lower()  'readme': ...         print(f'{stem}{suffix} is a readme file that belongs to user {user}.') 'README.md is a readme file that belongs to user gto.' @Code python import logging @Code python logging.basicConfig(filename, level'DEBUG')   Configures the root logger (see Setup).. logging.debug/info/warning/error/critical()      Logs to the root logger.. logging.getLogger(__name__)                Logger named after the module.. .()                               Logs to the logger.. .exception()                             Error() that appends caught exception.. @Code python logging.basicConfig( filenameNone,                                    Logs to console (stderr) by default.. format'%(levelname)s:%(name)s:%(message)s',      Add '%(asctime)s' for local datetime.. levellogging.WARNING,                            Drops messages with lower priority.. handlerslogging.StreamHandler(sys.stderr)      Uses FileHandler if filename is set.. ) @Code python logging.Formatter('')           Creates a Formatter.. logging.FileHandler(, mode'a')     Creates a Handler. Also encodingNone.. .setFormatter()                   Adds Formatter to the Handler.. .setLevel()                         Processes all messages by default.. .addHandler()                        Adds Handler to the Logger.. .setLevel()                          What is sent to its/ancestors' handlers.. .propagate                             Cuts off ancestors' handlers if False.. @Code python logger  logging.getLogger('my_module') handler  logging.FileHandler('test.log', encoding'utf-8') handler.setFormatter(logging.Formatter('%(asctime)s %(levelname)s:%(name)s:%(message)s')) logger.addHandler(handler) logger.setLevel('DEBUG') logging.basicConfig() logging.root.handlers0.setLevel('WARNING') logger.critical('Running out of disk space.') CRITICAL:my_module:Running out of disk space. print(open('test.log').read()) 2023-02-07 23:21:01,430 CRITICAL:my_module:Running out of disk space. @Code python dir()                              Names of local variables, functions, classes, etc.. vars()                             Dict of local variables, etc. Also locals().. globals()                          Dict of global vars, etc. (incl. '__builtins__').. @Code python dir()                      Names of object's attributes (including methods).. vars()                     Dict of writable attributes. Also .__dict__.. hasattr(, '')   Checks if getattr() raises an AttributeError.. value   getattr(, '')   Default value can be passed as the third argument.. setattr(, '', value)     Only works on objects with __dict__ attribute.. delattr(, '')            Same. Also del ... @Code python inspect.signature()      Returns function's Signature object.. .parameters                   Dict of Parameter objects. Also .return_type.. .kind                       Member of ParameterKind enum (KEYWORD_ONLY, ...).. .default                    Returns param's default value or Parameter.empty.. .annotation                 Returns param's type hint or Parameter.empty.. @Code python import asyncio as aio @Code python ()          Creates a coroutine by calling async def function.. await                  Starts the coroutine and returns result.. aio.create_task()      Schedules the coroutine for execution.. await                       Returns result. Also .cancel().. @Code python aio.gather(, ...)      Schedules coroutines. Returns results when awaited.. aio.wait(, …)              aio.ALL/FIRST_COMPLETED. Returns (done, pending).. aio.as_completed()   Iter of coros. All return next result when awaited.. @Code python import asyncio, collections, curses, curses.textpad, enum, random, time P  collections.namedtuple('P', 'x y')     Position. D  enum.Enum('D', 'n e s w')              Direction. W, H  15, 7                               Width, Height. def main(screen): curses.curs_set(0)                     Makes cursor invisible.. screen.nodelay(True)                   Makes getch() non-blocking.. asyncio.run(main_coroutine(screen))    Starts running asyncio code.. async def main_coroutine(screen): moves  asyncio.Queue() state  {'': P(0, 0), {id_: P(W//2, H//2) for id_ in range(10)}} ai     random_controller(id_, moves) for id_ in range(10) mvc    human_controller(screen, moves), model(moves, state), view(state, screen) tasks  asyncio.create_task(cor) for cor in ai  mvc await asyncio.wait(tasks, return_whenasyncio.FIRST_COMPLETED) async def random_controller(id_, moves): while True: d  random.choice(list(D)) moves.put_nowait((id_, d)) await asyncio.sleep(random.triangular(0.01, 0.65)) async def human_controller(screen, moves): while True: key_mappings  {258: D.s, 259: D.n, 260: D.w, 261: D.e} if d : key_mappings.get(screen.getch()): moves.put_nowait(('', d)) await asyncio.sleep(0.005) async def model(moves, state): while state'' not in (stateid_ for id_ in range(10)): id_, d  await moves.get() deltas  {D.n: P(0, -1), D.e: P(1, 0), D.s: P(0, 1), D.w: P(-1, 0)} stateid_  P((stateid_.x  deltasd.x) % W, (stateid_.y  deltasd.y) % H) async def view(state, screen): offset  P(curses.COLS//2 - W//2, curses.LINES//2 - H//2) while True: screen.erase() curses.textpad.rectangle(screen, offset.y-1, offset.x-1, offset.yH, offset.xW) for id_, p in state.items(): screen.addstr(offset.y  (p.y - state''.y  H//2) % H, offset.x  (p.x - state''.x  W//2) % W, str(id_)) screen.refresh() await asyncio.sleep(0.005) if __name__  '__main__': curses.wrapper(main) @Code python",1
3500,118861276,1,Python,"$ pip3 install tqdm. import tqdm, time for el in tqdm.tqdm(1, 2, 3, desc'Processing'): ...     time.sleep(1) Processing: 100%plt.legend()                                           Adds a legend.. plt.savefig()                                    Saves the figure.. plt.show()                                             Displays the figure.. plt.clf()                                              Clears the figure.. @Code python",1
3501,118861276,1,Python,"$ pip3 install tabulate. import csv, tabulate with open('test.csv', encoding'utf-8', newline'') as file: rows  list(csv.reader(file)) print(tabulate.tabulate(rows, headers'firstrow')) @Code python",1
3502,118861276,1,Python,"$ pip3 install windows-curses. import curses, os from curses import A_REVERSE, KEY_DOWN, KEY_UP, KEY_LEFT, KEY_RIGHT, KEY_ENTER def main(screen): ch, first, selected, paths  0, 0, 0, os.listdir() while ch ! ord('q'): height, width  screen.getmaxyx() screen.erase() for y, filename in enumerate(pathsfirst : firstheight): color  A_REVERSE if filename  pathsselected else 0 screen.addnstr(y, 0, filename, width-1, color) ch  screen.getch() selected  (ch  KEY_DOWN) - (ch  KEY_UP) selected  max(0, min(len(paths)-1, selected)) first  (selected  first  height) - (selected   webdriver.Chrome/Firefox/Safari/Edge()          Opens the browser. Also .quit().. .get('')                                      Also .implicitly_wait(seconds).. .find_element('css selector', '')   '.'''.. .find_elements('xpath', '')     '//@'''.. .get_attribute/get_property()          Also .text/tag_name.. .click/clear()                                      Also .send_keys().. @Code python /// or //             Child: /, Descendant: //, Parent: /... ///following::          Next sibling. Also preceding/parent/….",1
3503,118861276,1,Python,"/a/…,   1/2/….. and/or           For negation use not().. @''                           .'' matches complete text.. contains(@, '')                Is  a substring of attr's value?. //                             Has matching child? Descendant if //.. @Code python",1
3504,118861276,1,Python,"$ pip3 install flask. import flask @Code python app  flask.Flask(__name__)                 Returns app object. Put at the top.. app.run(hostNone, portNone, debugNone)   Or: $ flask --app  run. @Code python @app.route('/img/') def serve_file(filename): return flask.send_from_directory('dirname/', filename) @Code python @app.route('/') def serve_html(sport): return flask.render_template_string('{{title}}', titlesport) @Code python @app.post('//odds') def serve_json(sport): team  flask.request.form'team' return {'team': team, 'odds': 2.09, 3.74, 3.68} @Code python",1
3505,118861276,1,Python,"$ pip3 install requests. import threading, requests threading.Thread(targetapp.run, daemonTrue).start() url  ' @link  request_data  {'team': 'arsenal f.c.'} response  requests.post(url, datarequest_data) response.json() {'team': 'arsenal f.c.', 'odds': 2.09, 3.74, 3.68} @Code python from time import perf_counter start_time  perf_counter() ... duration_in_seconds  perf_counter() - start_time @Code python from timeit import timeit timeit('list(range(10000))', number1000, globalsglobals(), setup'pass') 0.19373 @Code text $ pip3 install line_profiler $ echo '@profile def main(): a  list(range(10000)) b  set(range(10000)) main()'  test.py $ kernprof -lv test.py Line       Hits         Time  Per Hit   % Time  Line Contents. 1                                           @profile 2                                           def main(): 3         1        253.4    253.4     32.2      a  list(range(10000)) 4         1        534.1    534.1     67.8      b  set(range(10000)) @Code bash $ apt/brew install graphviz && pip3 install gprof2dot snakeviz   Or download installer.. $ tail --lines2 test.py  test.py                              Removes first line.. $ python3 -m cProfile -o test.prof test.py                       Runs built-in profiler.. $ gprof2dot --formatpstats test.prof $ xdg-open/open test.png                                         Displays call graph.. $ snakeviz test.prof                                             Displays flame graph.. @Code text ---------------------------------------------------------------------- ---------------------------------------------------------------------- ---------------------------------------------------------------------- @Code python",1
3506,118861276,1,Python,"$ pip3 install numpy. import numpy as np @Code python np.array()               Returns a 1d/2d/… NumPy array.. np.zeros/ones/empty()                   Also np.full(, ).. np.arange(from_inc, to_exc, step)             Also np.linspace(start, stop, len).. np.random.randint(from_inc, to_exc, )   Also np.random.random().. @Code python .reshape()                       Also .shape  .. .flatten()                              Also   .ravel().. .transpose()                            Or: .T. @Code python np.copy/abs/sqrt/log/int64()            Returns new array of the same shape.. .sum/max/mean/argmax/all(axis)          Aggregates specified dimension.. np.apply_along_axis(, axis, )     Func can return a scalar or array.. @Code python np.concatenate(, axis0)       Links arrays along first axis (rows).. np.row_stack/column_stack()    Treats 1d arrays as rows or columns.. np.tile/repeat(,  , axis)   Tiles array or repeats its elements.. @Code perl row_index, col_index                  Or: , , . row_index                             Or: , , . :, col_index                          Or: , , . from:to_row_i, from:to_col_i          Or: , , . @Code perl row_indices, col_indices              Or: , , . row_indices                           Or: , , . :, col_indices                        Or: , , . np.ix_(row_indices, col_indices)      Or: , , . @Code perl",1
3507,118861276,1,Python,1d object must have size of a row..,1
3508,118861276,1,Python,"1d_bools must have size of a column.. @Code python left   0.1, 0.6, 0.8                            Shape: (3, 1). right   0.1 ,  0.6 ,  0.8                             Shape: (3,). @Code python left   0.1, 0.6, 0.8                            Shape: (3, 1). right  0.1 ,  0.6 ,  0.8                            Shape: (1, 3)  1, 2, 1):. @Code Image ----- @Code @Code @Code @Code @Code",1
3509,118861276,3,Python,"Modes. 'L' - Lightness (i.e. greyscale). Each pixel is an int between 0 and 255. 'RGB' - Red, green, blue (i.e. true color). Each pixel is a tuple of three ints. 'RGBA' - RGB with alpha. Low alpha (forth int) means more transparency. 'HSV' - Hue, saturation, value color space.",2
3510,118861276,4,Python,Creates a PNG image of a rainbow gradient:. @Code,2
3511,118861276,4,Python,Adds noise to the PNG image and displays it:. @Code,2
3512,118861276,3,Python,"Image Draw. @Code Use 'fill' to set the primary color. Use 'width' to set the width of lines or contours. Use 'outline' to set the color of the contours. Color can be an int, tuple, 'rrggbbaa' string or a color name.. Animation ---------",2
3513,118861276,4,Python,"Creates a GIF of a bouncing ball:. @Code Audio ----- @Code @Code @Code Bytes object contains a sequence of frames, each consisting of one or more samples. In a stereo signal, the first sample of a frame belongs to the left channel. Each sample consists of one or more bytes that, when converted to an integer, indicate the displacement of a speaker membrane at a given moment. If sample width is one byte, then the integer should be encoded unsigned. For all other sizes, the integer should be encoded signed with little-endian byte order.",2
3514,118861276,3,Python,Read Float Samples from WAV File. @Code,2
3515,118861276,3,Python,Write Float Samples to WAV File. @Code,2
3516,118861276,4,Python,Saves a 440 Hz sine wave to a mono WAV file:. @Code,2
3517,118861276,4,Python,Adds noise to the mono WAV file:. @Code,2
3518,118861276,4,Python,Plays the WAV file:. @Code,2
3519,118861276,3,Python,Text to Speech. @Code Synthesizer -----------,2
3520,118861276,4,Python,Plays Popcorn by Gershon Kingsley:. @Code Pygame ------ @Code,2
3521,118861276,3,Python,Rectangle. Object for storing rectangular coordinates. @Code @Code,2
3522,118861276,3,Python,Surface. Object for representing images. @Code @Code @Code @Code,2
3523,118861276,3,Python,Basic Mario Brothers Example. @Code Pandas ------ @Code,2
3524,118861276,3,Python,Series. Ordered dictionary with a name. @Code @Code @Code @Code @Code @Code @Code,2
3525,118861276,4,Python,"Series — Aggregate, Transform, Map:. @Code @Code @Code @Code Indexing objects can't be tuples because 'objx, y' is converted to 'obj(x, y)'! Methods ffill(), interpolate(), fillna() and dropna() accept 'inplaceTrue'. Last result has a hierarchical index. Use 'key_1, key_2' to get its values.",2
3526,118861276,3,Python,DataFrame. Table with labeled rows and columns. @Code @Code @Code @Code @Code @Code,2
3527,118861276,4,Python,"DataFrame — Merge, Join, Concat:. @Code @Code",2
3528,118861276,4,Python,"DataFrame — Aggregate, Transform, Map:. @Code All operations operate on columns by default. Pass 'axis1' to process the rows instead. @Code @Code @Code Use 'col_key_1, col_key_2row_key' to get the fifth result's values.",2
3529,118861276,4,Python,"DataFrame — Plot, Encode, Decode:. @Code @Code @Code",2
3530,118861276,3,Python,GroupBy. Object that groups together rows of a dataframe based on the value of the passed column. @Code @Code,2
3531,118861276,4,Python,"GroupBy — Aggregate, Transform, Map:. @Code @Code @Code",2
3532,118861276,3,Python,Rolling. Object for rolling window calculations. @Code Plotly ------ @Code,2
3533,118861276,4,Python,Displays a line chart of total coronavirus deaths per million grouped by continent:. @Code,2
3534,118861276,4,Python,"Displays a multi-axis line chart of total coronavirus cases and changes in prices of Bitcoin, Dow Jones and gold:. @Code Appendix --------",2
3535,118861276,3,Python,Cython. Library that compiles Python code into C. @Code,2
3536,118861276,4,Python,"Definitions:. All 'cdef' definitions are optional, but they contribute to the speed-up. Script needs to be saved with a 'pyx' extension. @Code @Code @Code",2
3537,118861276,3,Python,Virtual Environments. System for installing libraries directly into project's directory. @Code,2
3538,118861276,3,Python,Basic Script Template. @Code Index ----- Only available in the PDF @link . CtrlF / F is usually sufficient. Searching '' on the webpage @link  will limit the search to the titles..,2
3539,123303402,1,Python,"Poetry: Python packaging and dependency management made easy. @link PyPI Releases PyPI Releases PyPI @link Discord Poetry helps you declare, manage and install dependencies of Python projects, ensuring you have the right stack everywhere. Poetry replaces setup.py, requirements.txt, setup.cfg, MANIFEST.in and Pipfile with a simple pyproject.toml based project format. @Code",1
3540,123303402,2,Python,"Installation. Poetry supports multiple installation methods, including a simple script found at install.python-poetry.org. For full installation instructions, including advanced usage of the script, alternate install methods, and CI best practices, see the full installation documentation.",2
3541,123303402,2,Python,Documentation. Documentation for the current version of Poetry (as well as the development branch and recently out of support versions) is available from the official website.,2
3542,123303402,2,Python,"Contribute. Poetry is a large, complex project always in need of contributors. For those new to the project, a list of suggested issues to work on in Poetry and poetry-core is available. The full contributing documentation also provides helpful guidance.",3
3543,123303402,2,Python,Resources. ReleasesPyPI Releases Official Website Documentation Issue Tracker Discord PyPI:  @link   PyPI Releases:  @link   Official Website:  @link   Documentation:  @link   Issue Tracker:  @link   Suggested Issues:  @link   Contributing Documentation:  @link   Discord:  @link   install.python-poetry.org:  @link   Installation Documentation:  @link,2
3544,123303402,2,Python,"Related Projects. poetry-core @link : PEP 517 build-system for Poetry projects, and dependency-free core functionality of the Poetry frontend poetry-plugin-export @link : Export Poetry projects/lock files to foreign formats like requirements.txt poetry-plugin-bundle @link : Install Poetry projects/lock files to external formats like virtual environments install.python-poetry.org @link : The official Poetry installation script website @link : The official Poetry website and blog",3
3546,125266328,3,Python,"Installation. _Black_ can be installed by running pip install black. It requires Python 3.8 to run. If you want to format Jupyter Notebooks, install with pip install 'blackjupyter'. If you can't wait for the latest _hotness_ and want to install from GitHub, use: pip install git @link",2
3547,125266328,3,Python,"Usage. To get started right away with sensible defaults: @Code You can run _Black_ as a package if running it as a script doesn't work: @Code Further information can be found in our docs: - Usage and Configuration @link _Black_ is already successfully used @link  by many. projects, small and big. _Black_ has a comprehensive test suite, with efficient parallel tests, and our own auto formatting and parallel Continuous Integration runner. Now that we have become stable, you should not expect large formatting changes in the future. Stylistic changes will mostly be responses to bug reports and support for new Python syntax. For more information please refer to The Black Code Style @link . Also, as a safety measure which slows down processing, _Black_ will check that the reformatted code still produces a valid AST that is effectively equivalent to the original (see the Pragmatism @link . section for details). If you're feeling confident, use --fast.",2
3548,125266328,2,Python,"The _Black_ code style. _Black_ is a PEP 8 compliant opinionated formatter. _Black_ reformats entire files in place. Style configuration options are deliberately limited and rarely added. It doesn't take previous formatting into account (see Pragmatism @link . for exceptions). Our documentation covers the current _Black_ code style, but planned changes to it are also documented. They're both worth taking a look at: - The _Black_ Code Style: Current style @link - The _Black_ Code Style: Future style @link Changes to the _Black_ code style are bound by the Stability Policy: - The _Black_ Code Style: Stability Policy @link . Please refer to this document before submitting an issue. What seems like a bug might be intended behaviour.",2
3549,125266328,3,Python,"Pragmatism. Early versions of _Black_ used to be absolutist in some respects. They took after its initial author. This was fine at the time as it made the implementation simpler and there were not many users anyway. Not many edge cases were reported. As a mature tool, _Black_ does make some exceptions to rules it otherwise holds. - The _Black_ code style: Pragmatism @link . Please refer to this document before submitting an issue just like with the document above. What seems like a bug might be intended behaviour.",2
3550,125266328,2,Python,Configuration. _Black_ is able to read project-specific default values for its command line options from a pyproject.toml file. This is especially useful for specifying custom --include and --exclude/--force-exclude/--extend-exclude patterns for your project. You can find more details in our documentation: - The basics: Configuration via a file @link . And if you're looking for more general configuration documentation: - Usage and Configuration @link Pro-tip: If you're asking yourself 'Do I need to configure anything?' the answer is 'No'. _Black_ is all about sensible defaults. Applying those defaults will have your code in compliance with many other _Black_ formatted projects.,2
3551,125266328,2,Python,"Used by. The following notable open-source projects trust _Black_ with enforcing a consistent code style: pytest, tox, Pyramid, Django, Django Channels, Hypothesis, attrs, SQLAlchemy, Poetry, PyPA applications (Warehouse, Bandersnatch, Pipenv, virtualenv), pandas, Pillow, Twisted, LocalStack, every Datadog Agent Integration, Home Assistant, Zulip, Kedro, OpenOA, FLORIS, ORBIT, WOMBAT, and many more. The following organizations use _Black_: Facebook, Dropbox, KeepTruckin, Lyft, Mozilla, Quora, Duolingo, QuantumBlack, Tesla, Archer Aviation. Are we missing anyone? Let us know.",3
3552,125266328,2,Python,"Testimonials. Mike Bayer, author of SQLAlchemy @link : I can't think of any single tool in my entire programming career that has given me a bigger productivity increase by its introduction. I can now do refactorings in about 1% of the keystrokes that it would have taken me previously when we had no way for code to format itself. Dusty Phillips, writer @link : _Black_ is opinionated so you don't have to be. Hynek Schlawack, creator of attrs @link , core developer of Twisted and CPython: An auto-formatter that doesn't suck is all I want for Xmas! Carl Meyer, Django @link  core developer: At least the name is good. Kenneth Reitz, creator of requests @link and pipenv @link : This vastly improves the formatting of our code. Thanks a ton!",3
3553,125266328,2,Python,Show your style. Use the badge in your project's README.md: @Code Using the badge in README.rst: @Code Looks like this: @link,3
3554,125266328,2,Python,Contributing. Welcome! Happy to see you willing to make the project better. You can get started by reading this: - Contributing: The basics @link You can also take a look at the rest of the contributing docs or talk with the developers: - Contributing documentation @link - Chat on Discord @link,3
3555,125266328,2,Python,Change log. The log has become rather long. It moved to its own file. See CHANGES @link .,1
3556,125266328,2,Python,"Authors. The author list is quite long nowadays, so it lives in its own file. See AUTHORS.md @link",3
3557,125266328,2,Python,"Code of Conduct. Everyone participating in the _Black_ project, and in particular in the issue tracker, pull requests, and social media activity, is expected to treat other people with respect and more generally to follow the guidelines articulated in the Python Community Code of Conduct @link . At the same time, humor is encouraged. In fact, basic familiarity with Monty Python's Flying Circus is expected. We are not savages. And if you _really_ need to slap somebody, do it with a fish while dancing.",3
3558,1362490,1,Python,"Requests. Requests is a simple, yet elegant, HTTP library. @Code Requests allows you to send HTTP/1.1 requests extremely easily. There’s no need to manually add query strings to your URLs, or to form-encode your PUT & POST data — but nowadays, just use the json method! Requests is one of the most downloaded Python packages today, pulling in around 30M downloads / week— according to GitHub, Requests is currently depended upon @link  by 1,000,000 repositories. You may certainly put your trust in this code. @link @link @link",1
3559,1362490,2,Python,Installing Requests and Supported Versions. Requests is available on PyPI: @Code Requests officially supports Python 3.8.,2
3560,1362490,2,Python,"Supported Features & Best–Practices. Requests is ready for the demands of building robust and reliable HTTP–speaking applications, for the needs of today. - Keep-Alive & Connection Pooling - International Domains and URLs - Sessions with Cookie Persistence - Browser-style TLS/SSL Verification - Basic & Digest Authentication - Familiar dict–like Cookies - Automatic Content Decompression and Decoding - Multi-part File Uploads - SOCKS Proxy Support - Connection Timeouts - Streaming Downloads - Automatic honoring of .netrc - Chunked HTTP Requests",1
3561,1362490,2,Python,API Reference and User Guide available on Read the Docs @link . @link,2
3562,1362490,2,Python,"Cloning the repository. When cloning the Requests repository, you may need to add the -c fetch.fsck.badTimezoneignore flag to avoid an error about a bad commit (see this issue @link  for more background): @Code You can also apply this setting to your global Git config: @Code --- @link   @link",3
3563,143328315,2,Python,"About us . MindsDB is the platform for building AI from enterprise data. With MindsDB, you can deploy, serve, and fine-tune models in real-time, utilizing data from databases, vector stores, or applications, to build AI-powered apps - using universal tools developers already know. MindsDB integrates with numerous data sources @link , including databases, vector stores, and applications, and popular AI/ML frameworks @link , including AutoML and LLMs. MindsDB connects data sources with AI/ML frameworks and automates routine workflows between them. By doing so, we bring data and AI together, enabling the intuitive implementation of customized AI systems. Learn more about features and use cases of MindsDB here @link .",1
3564,143328315,2,Python,"Get Started . To get started, install MindsDB locally via Docker @link  or Docker Desktop @link , following the instructions in linked doc pages. MindsDB enhances SQL syntax to enable seamless development and deployment of AI-powered applications. Furthermore, users can interact with MindsDB not only via SQL API @link  but also via REST APIs @link , Python SDK @link , JavaScript SDK @link , and MongoDB-QL @link .",2
3565,143328315,2,Python,"Examples . MindsDB enables you to deploy AI/ML models, send predictions to your application, and automate AI workflows. Discover more tutorials and use cases here @link .",2
3566,143328315,3,Python,"AI Workflow Automation . This category of use cases involves tasks that get data from a data source, pass it through an AI/ML model, and write the output to a data destination. Common use cases are anomaly detection, data indexing/labeling/cleaning, and data transformation. This example showcases the data enrichment flow, where input data comes from a PostgreSQL database and is passed through an OpenAI model to generate new content which is saved into a data destination. We take customer reviews from a PostgreSQL database. Then, we deploy an OpenAI model that analyzes all customer reviews and assigns sentiment values. Finally, to automate the workflow for incoming customer reviews, we create a job that generates and saves AI output into a data destination. @Code",2
3567,143328315,3,Python,"AI System Deployment . This category of use cases involves creating AI systems composed of multiple connected parts, including various AI/ML models and data sources, and exposing such AI systems via APIs. Common use cases are agents and assistants, recommender systems, forecasting systems, and semantic search. This example showcases AI agents, a feature developed by MindsDB. AI agents can be assigned certain skills, including text-to-SQL skills and knowledge bases. Skills provide an AI agent with input data that can be in the form of a database, a file, or a website. We create a text-to-SQL skill based on the car sales dataset and deploy a conversational model, which are both components of an agent. Then, we create an agent and assign this skill and this model to it. This agent can be queried to ask questions about data stored in assigned skills. @Code Agents are accessible via API endpoints @link .",2
3568,143328315,2,Python,"Contribute . If you’d like to contribute to MindsDB, install MindsDB for development following this instruction @link . You’ll find the contribution guide here @link . We are always open to suggestions, so feel free to open new issues with your ideas, and we can guide you! This project is released with a Contributor Code of Conduct @link . By participating in this project, you agree to follow its terms. Also, check out the rewards and community programs here @link .",3
3569,143328315,2,Python,"Support . If you find a bug, please submit an issue on GitHub here @link . Here is how you can get community support: Post a question at MindsDB Slack Community @link . Ask for help at our GitHub Discussions @link . Ask a question at Stackoverflow @link  with a MindsDB tag. If you need commercial support, please contact the MindsDB team @link .",3
3570,143328315,2,Python,Current contributors . Made with contributors-img @link .,3
3571,143328315,2,Python,"Subscribe to updates . Join our Slack community @link  and subscribe to the monthly Developer Newsletter @link  to get product updates, information about MindsDB events and contests, and useful content, like tutorials.",3
3572,143328315,2,Python,"License . For detailed licensing information, please refer to the LICENSE file @link .",3
3573,154747577,1,Python,"BERT. \\\\\ New March 11th, 2020: Smaller BERT Models \\\\\ This is a release of 24 smaller BERT models (English only, uncased, trained with WordPiece masking) referenced in Well-Read Students Learn Better: On the Importance of Pre-training Compact Models @link . We have shown that the standard BERT recipe (including model architecture and training objective) is effective on a wide range of model sizes, beyond BERT-Base and BERT-Large. The smaller BERT models are intended for environments with restricted computational resources. They can be fine-tuned in the same manner as the original BERT models. However, they are most effective in the context of knowledge distillation, where the fine-tuning labels are produced by a larger and more accurate teacher. Our goal is to enable research in institutions with fewer computational resources and encourage the community to seek directions of innovation alternative to increasing model capacity. You can download all 24 from hereall, or individually from the table below: Note that the BERT-Base model in this release is included for completeness only; it was re-trained under the same regime as the original model. Here are the corresponding GLUE scores on the test set: For each task, we selected the best fine-tuning hyperparameters from the lists below, and trained for 4 epochs: - batch sizes: 8, 16, 32, 64, 128 - learning rates: 3e-4, 1e-4, 5e-5, 3e-5 If you use these models, please cite the following paper: @Code 2_128:  @link 2_256:  @link 2_512:  @link 2_768:  @link 4_128:  @link 4_256:  @link 4_512:  @link 4_768:  @link 6_128:  @link 6_256:  @link 6_512:  @link 6_768:  @link 8_128:  @link 8_256:  @link 8_512:  @link 8_768:  @link 10_128:  @link 10_256:  @link 10_512:  @link 10_768:  @link 12_128:  @link 12_256:  @link 12_512:  @link 12_768:  @link all:  @link \\\\\ New May 31st, 2019: Whole Word Masking Models \\\\\ This is a release of several new models which were the result of an improvement the pre-processing code. In the original pre-processing code, we randomly select WordPiece tokens to mask. For example: Input Text: the man jumped up , put his basket on phil am mon ' s head. Original Masked Input: MASK man MASK up , put his MASK on phil MASK mon ' s head. The new technique is called Whole Word Masking. In this case, we always mask all of the the tokens corresponding to a word at once. The overall masking rate remains the same. Whole Word Masked Input: the man MASK up , put his basket on MASK MASK MASK ' s head The training is identical -- we still predict each masked WordPiece token independently. The improvement comes from the fact that the original prediction task was too 'easy' for words that had been split into multiple WordPieces. This can be enabled during data generation by passing the flag --do_whole_word_maskTrue to create_pretraining_data.py. Pre-trained models with Whole Word Masking are linked below. The data and training were otherwise identical, and the models have identical structure and vocab to the original models. We only include BERT-Large models. When using these models, please make it clear in the paper that you are using the Whole Word Masking variant of BERT-Large. BERT-Large, Uncased (Whole Word Masking) @link : 24-layer, 1024-hidden, 16-heads, 340M parameters BERT-Large, Cased (Whole Word Masking) @link : 24-layer, 1024-hidden, 16-heads, 340M parameters Model                                    ---------------------------------------- BERT-Large, Uncased (Original)           BERT-Large, Uncased (Whole Word Masking) BERT-Large, Cased (Original)             BERT-Large, Cased (Whole Word Masking) \\\\\ New February 7th, 2019: TfHub Module \\\\\ BERT has been uploaded to TensorFlow Hub @link . See run_classifier_with_tfhub.py for an example of how to use the TF Hub module, or run an example in the browser on Colab @link . \\\\\ New November 23rd, 2018: Un-normalized multilingual model  Thai Mongolian \\\\\ We uploaded a new multilingual model which does not perform any normalization on the input (no lower casing, accent stripping, or Unicode normalization), and additionally inclues Thai and Mongolian. It is recommended to use this version for developing multilingual models, especially on languages with non-Latin alphabets. This does not require any code changes, and can be downloaded here: BERT-Base, Multilingual Cased @link : 104 languages, 12-layer, 768-hidden, 12-heads, 110M parameters \\\\\ New November 15th, 2018: SOTA SQuAD 2.0 System \\\\\ We released code changes to reproduce our 83% F1 SQuAD 2.0 system, which is currently 1st place on the leaderboard by 3%. See the SQuAD 2.0 section of the README for details. \\\\\ New November 5th, 2018: Third-party PyTorch and Chainer versions of BERT available \\\\\ NLP researchers from HuggingFace made a PyTorch version of BERT available @link which is compatible with our pre-trained checkpoints and is able to reproduce our results. Sosuke Kobayashi also made a Chainer version of BERT available @link (Thanks!) We were not involved in the creation or maintenance of the PyTorch implementation so please direct any questions towards the authors of that repository. \\\\\ New November 3rd, 2018: Multilingual and Chinese models available \\\\\ We have made two new BERT models available: BERT-Base, Multilingual @link (Not recommended, use Multilingual Cased instead): 102 languages, 12-layer, 768-hidden, 12-heads, 110M parameters BERT-Base, Chinese @link : Chinese Simplified and Traditional, 12-layer, 768-hidden, 12-heads, 110M parameters We use character-based tokenization for Chinese, and WordPiece tokenization for all other languages. Both models should work out-of-the-box without any code changes. We did update the implementation of BasicTokenizer in tokenization.py to support Chinese character tokenization, so please update if you forked it. However, we did not change the tokenization API. For more, see the Multilingual README @link . \\\\\ End new information \\\\\",1
3574,154747577,2,Python,"Introduction. BERT, or Bidirectional Encoder Representations from Transformers, is a new method of pre-training language representations which obtains state-of-the-art results on a wide array of Natural Language Processing (NLP) tasks. Our academic paper which describes BERT in detail and provides full results on a number of tasks can be found here: @link @link . To give a few numbers, here are the results on the SQuAD v1.1 @link  question answering task: SQuAD v1.1 Leaderboard (Oct 8th 2018) ------------------------------------- 1st Place Ensemble - BERT             2nd Place Ensemble - nlnet            1st Place Single Model - BERT         2nd Place Single Model - nlnet And several natural language inference tasks: System                  ----------------------- BERT                    OpenAI GPT (Prev. SOTA) Plus many other tasks. Moreover, these results were all obtained with almost no task-specific neural network architecture design. If you already know what BERT is and you just want to get started, you can download the pre-trained models(pre-trained-models) and. run a state-of-the-art fine-tuning(fine-tuning-with-bert) in only a few. minutes.",1
3575,154747577,2,Python,"What is BERT?. BERT is a method of pre-training language representations, meaning that we train a general-purpose 'language understanding' model on a large text corpus (like Wikipedia), and then use that model for downstream NLP tasks that we care about (like question answering). BERT outperforms previous methods because it is the first unsupervised, deeply bidirectional system for pre-training NLP. Unsupervised means that BERT was trained using only a plain text corpus, which is important because an enormous amount of plain text data is publicly available on the web in many languages. Pre-trained representations can also either be context-free or contextual, and contextual representations can further be unidirectional or bidirectional. Context-free models such as word2vec @link  or GloVe @link  generate a single 'word embedding' representation for each word in the vocabulary, so bank would have the same representation in bank deposit and river bank. Contextual models instead generate a representation of each word that is based on the other words in the sentence. BERT was built upon recent work in pre-training contextual representations — including Semi-supervised Sequence Learning @link , Generative Pre-Training @link , ELMo @link , and ULMFit @link — but crucially these models are all unidirectional or shallowly bidirectional. This means that each word is only contextualized using the words to its left (or right). For example, in the sentence I made a bank deposit the unidirectional representation of bank is only based on I made a but not deposit. Some previous work does combine the representations from separate left-context and right-context models, but only in a 'shallow' manner. BERT represents 'bank' using both its left and right context — I made a ... deposit — starting from the very bottom of a deep neural network, so it is deeply bidirectional. BERT uses a simple approach for this: We mask out 15% of the words in the input, run the entire sequence through a deep bidirectional Transformer @link  encoder, and then predict only the masked words. For example: @Code In order to learn relationships between sentences, we also train on a simple task which can be generated from any monolingual corpus: Given two sentences A and B, is B the actual next sentence that comes after A, or just a random sentence from the corpus? @Code @Code We then train a large model (12-layer to 24-layer Transformer) on a large corpus (Wikipedia  BookCorpus @link ) for a long time (1M update steps), and that's BERT. Using BERT has two stages: Pre-training and fine-tuning. Pre-training is fairly expensive (four days on 4 to 16 Cloud TPUs), but is a one-time procedure for each language (current models are English-only, but multilingual models will be released in the near future). We are releasing a number of pre-trained models from the paper which were pre-trained at Google. Most NLP researchers will never need to pre-train their own model from scratch. Fine-tuning is inexpensive. All of the results in the paper can be replicated in at most 1 hour on a single Cloud TPU, or a few hours on a GPU, starting from the exact same pre-trained model. SQuAD, for example, can be trained in around 30 minutes on a single Cloud TPU to achieve a Dev F1 score of 91.0%, which is the single system state-of-the-art. The other important aspect of BERT is that it can be adapted to many types of NLP tasks very easily. In the paper, we demonstrate state-of-the-art results on sentence-level (e.g., SST-2), sentence-pair-level (e.g., MultiNLI), word-level (e.g., NER), and span-level (e.g., SQuAD) tasks with almost no task-specific modifications.",1
3576,154747577,2,Python,"What has been released in this repository?. We are releasing the following: TensorFlow code for the BERT model architecture (which is mostly a standard Transformer @link  architecture). Pre-trained checkpoints for both the lowercase and cased version of BERT-Base and BERT-Large from the paper. TensorFlow code for push-button replication of the most important fine-tuning experiments from the paper, including SQuAD, MultiNLI, and MRPC. All of the code in this repository works out-of-the-box with CPU, GPU, and Cloud TPU.",1
3577,154747577,2,Python,"Pre-trained models. We are releasing the BERT-Base and BERT-Large models from the paper. Uncased means that the text has been lowercased before WordPiece tokenization, e.g., John Smith becomes john smith. The Uncased model also strips out any accent markers. Cased means that the true case and accent markers are preserved. Typically, the Uncased model is better unless you know that case information is important for your task (e.g., Named Entity Recognition or Part-of-Speech tagging). These models are all released under the same license as the source code (Apache 2.0). For information about the Multilingual and Chinese model, see the Multilingual README @link . When using a cased model, make sure to pass --do_lowerFalse to the training scripts. (Or pass do_lower_caseFalse directly to FullTokenizer if you're using your own script.) The links to the models are here (right-click, 'Save link as...' on the name): BERT-Large, Uncased (Whole Word Masking) @link : 24-layer, 1024-hidden, 16-heads, 340M parameters BERT-Large, Cased (Whole Word Masking) @link : 24-layer, 1024-hidden, 16-heads, 340M parameters BERT-Base, Uncased @link : 12-layer, 768-hidden, 12-heads, 110M parameters BERT-Large, Uncased @link : 24-layer, 1024-hidden, 16-heads, 340M parameters BERT-Base, Cased @link : 12-layer, 768-hidden, 12-heads , 110M parameters BERT-Large, Cased @link : 24-layer, 1024-hidden, 16-heads, 340M parameters BERT-Base, Multilingual Cased (New, recommended) @link : 104 languages, 12-layer, 768-hidden, 12-heads, 110M parameters BERT-Base, Multilingual Uncased (Orig, not recommended) @link (Not recommended, use Multilingual Cased instead): 102 languages, 12-layer, 768-hidden, 12-heads, 110M parameters BERT-Base, Chinese @link : Chinese Simplified and Traditional, 12-layer, 768-hidden, 12-heads, 110M parameters Each .zip file contains three items: A TensorFlow checkpoint (bert_model.ckpt) containing the pre-trained weights (which is actually 3 files). A vocab file (vocab.txt) to map WordPiece to word id. A config file (bert_config.json) which specifies the hyperparameters of the model.",1
3578,154747577,2,Python,"Fine-tuning with BERT. Important: All results on the paper were fine-tuned on a single Cloud TPU, which has 64GB of RAM. It is currently not possible to re-produce most of the BERT-Large results on the paper using a GPU with 12GB - 16GB of RAM, because the maximum batch size that can fit in memory is too small. We are working on adding code to this repository which allows for much larger effective batch size on the GPU. See the section on out-of-memory issues(out-of-memory-issues) for. more details. This code was tested with TensorFlow 1.11.0. It was tested with Python2 and Python3 (but more thoroughly with Python2, since this is what's used internally in Google). The fine-tuning examples which use BERT-Base should be able to run on a GPU that has at least 12GB of RAM using the hyperparameters given.",2
3579,154747577,3,Python,"Fine-tuning with Cloud TPUs. Most of the examples below assumes that you will be running training/evaluation on your local machine, using a GPU like a Titan X or GTX 1080. However, if you have access to a Cloud TPU that you want to train on, just add the following flags to run_classifier.py or run_squad.py: @Code Please see the Google Cloud TPU tutorial @link for how to use Cloud TPUs. Alternatively, you can use the Google Colab notebook 'BERT FineTuning with Cloud TPUs @link '. On Cloud TPUs, the pretrained model and the output directory will need to be on Google Cloud Storage. For example, if you have a bucket named some_bucket, you might use the following flags instead: @Code The unzipped pre-trained model files can also be found in the Google Cloud Storage folder gs://bert_models/2018_10_18. For example: @Code",2
3580,154747577,3,Python,"Sentence (and sentence-pair) classification tasks. Before running this example you must download the GLUE data @link  by running this script @link and unpack it to some directory $GLUE_DIR. Next, download the BERT-Base checkpoint and unzip it to some directory $BERT_BASE_DIR. This example code fine-tunes BERT-Base on the Microsoft Research Paraphrase Corpus (MRPC) corpus, which only contains 3,600 examples and can fine-tune in a few minutes on most GPUs. @Code You should see output like this: @Code This means that the Dev set accuracy was 84.55%. Small sets like MRPC have a high variance in the Dev set accuracy, even when starting from the same pre-training checkpoint. If you re-run multiple times (making sure to point to different output_dir), you should see results between 84% and 88%. A few other pre-trained models are implemented off-the-shelf in run_classifier.py, so it should be straightforward to follow those examples to use BERT for any single-sentence or sentence-pair classification task. Note: You might see a message Running train on CPU. This really just means that it's running on something other than a Cloud TPU, which includes a GPU.",2
3581,154747577,4,Python,"Prediction from classifier. Once you have trained your classifier you can use it in inference mode by using the --do_predicttrue command. You need to have a file named test.tsv in the input folder. Output will be created in file called test_results.tsv in the output folder. Each line will contain output for each sample, columns are the class probabilities. @Code",2
3582,154747577,3,Python,"SQuAD 1.1. The Stanford Question Answering Dataset (SQuAD) is a popular question answering benchmark dataset. BERT (at the time of the release) obtains state-of-the-art results on SQuAD with almost no task-specific network architecture modifications or data augmentation. However, it does require semi-complex data pre-processing and post-processing to deal with (a) the variable-length nature of SQuAD context paragraphs, and (b) the character-level answer annotations which are used for SQuAD training. This processing is implemented and documented in run_squad.py. To run on SQuAD, you will first need to download the dataset. The SQuAD website @link  does not seem to link to the v1.1 datasets any longer, but the necessary files can be found here: train-v1.1.json @link dev-v1.1.json @link evaluate-v1.1.py @link Download these to some directory $SQUAD_DIR. The state-of-the-art SQuAD results from the paper currently cannot be reproduced on a 12GB-16GB GPU due to memory constraints (in fact, even batch size 1 does not seem to fit on a 12GB GPU using BERT-Large). However, a reasonably strong BERT-Base model can be trained on the GPU with these hyperparameters: @Code The dev set predictions will be saved into a file called predictions.json in the output_dir: @Code Which should produce an output like this: @Code You should see a result similar to the 88.5% reported in the paper for BERT-Base. If you have access to a Cloud TPU, you can train with BERT-Large. Here is a set of hyperparameters (slightly different than the paper) which consistently obtain around 90.5%-91.0% F1 single-system trained only on SQuAD: @Code For example, one random run with these parameters produces the following Dev scores: @Code If you fine-tune for one epoch on TriviaQA @link  before this the results will be even better, but you will need to convert TriviaQA into the SQuAD json format.",2
3583,154747577,3,Python,"SQuAD 2.0. This model is also implemented and documented in run_squad.py. To run on SQuAD 2.0, you will first need to download the dataset. The necessary files can be found here: train-v2.0.json @link dev-v2.0.json @link evaluate-v2.0.py @link Download these to some directory $SQUAD_DIR. On Cloud TPU you can run with BERT-Large as follows: @Code We assume you have copied everything from the output directory to a local directory called ./squad/. The initial dev set predictions will be at ./squad/predictions.json and the differences between the score of no answer ('') and the best non-null answer for each question will be in the file ./squad/null_odds.json Run this script to tune a threshold for predicting null versus non-null answers: python $SQUAD_DIR/evaluate-v2.0.py $SQUAD_DIR/dev-v2.0.json ./squad/predictions.json --na-prob-file ./squad/null_odds.json Assume the script outputs 'best_f1_thresh' THRESH. (Typical values are between -1.0 and -5.0). You can now re-run the model to generate predictions with the derived threshold or alternatively you can extract the appropriate answers from ./squad/nbest_predictions.json. @Code",2
3584,154747577,3,Python,"Out-of-memory issues. All experiments in the paper were fine-tuned on a Cloud TPU, which has 64GB of device RAM. Therefore, when using a GPU with 12GB - 16GB of RAM, you are likely to encounter out-of-memory issues if you use the same hyperparameters described in the paper. The factors that affect memory usage are: max_seq_length: The released models were trained with sequence lengths up to 512, but you can fine-tune with a shorter max sequence length to save substantial memory. This is controlled by the max_seq_length flag in our example code. train_batch_size: The memory usage is also directly proportional to the batch size. Model type, BERT-Base vs. BERT-Large: The BERT-Large model requires significantly more memory than BERT-Base. Optimizer: The default optimizer for BERT is Adam, which requires a lot of extra memory to store the m and v vectors. Switching to a more memory efficient optimizer can reduce memory usage, but can also affect the results. We have not experimented with other optimizers for fine-tuning. Using the default training scripts (run_classifier.py and run_squad.py), we benchmarked the maximum batch size on single Titan X GPU (12GB RAM) with TensorFlow 1.11.0: System       ------------ BERT-Base  ...          ...          ...          ...          ...          BERT-Large ...          ...          ...          ...          ... Unfortunately, these max batch sizes for BERT-Large are so small that they will actually harm the model accuracy, regardless of the learning rate used. We are working on adding code to this repository which will allow much larger effective batch sizes to be used on the GPU. The code will be based on one (or both) of the following techniques: Gradient accumulation: The samples in a minibatch are typically independent with respect to gradient computation (excluding batch normalization, which is not used here). This means that the gradients of multiple smaller minibatches can be accumulated before performing the weight update, and this will be exactly equivalent to a single larger update. Gradient checkpointing @link : The major use of GPU/TPU memory during DNN training is caching the intermediate activations in the forward pass that are necessary for efficient computation in the backward pass. 'Gradient checkpointing' trades memory for compute time by re-computing the activations in an intelligent way. However, this is not implemented in the current release.",2
3585,154747577,2,Python,"Using BERT to extract fixed feature vectors (like ELMo). In certain cases, rather than fine-tuning the entire pre-trained model end-to-end, it can be beneficial to obtained pre-trained contextual embeddings, which are fixed contextual representations of each input token generated from the hidden layers of the pre-trained model. This should also mitigate most of the out-of-memory issues. As an example, we include the script extract_features.py which can be used like this: @Code This will create a JSON file (one line per line of input) containing the BERT activations from each Transformer layer specified by layers (-1 is the final hidden layer of the Transformer, etc.) Note that this script will produce very large output files (by default, around 15kb for every input token). If you need to maintain alignment between the original and tokenized words (for projecting training labels), see the Tokenization(tokenization) section. below. Note: You may see a message like Could not find trained model in model_dir: /tmp/tmpuB5g5c, running initialization to predict. This message is expected, it just means that we are using the init_from_checkpoint() API rather than the saved model API. If you don't specify a checkpoint or specify an invalid checkpoint, this script will complain.",2
3586,154747577,2,Python,"Tokenization. For sentence-level tasks (or sentence-pair) tasks, tokenization is very simple. Just follow the example code in run_classifier.py and extract_features.py. The basic procedure for sentence-level tasks is: 1.  Instantiate an instance of tokenizer  tokenization.FullTokenizer 2.  Tokenize the raw text with tokens  tokenizer.tokenize(raw_text). 3.  Truncate to the maximum sequence length. (You can use up to 512, but you probably want to use shorter if possible for memory and speed reasons.) 4.  Add the CLS and SEP tokens in the right place. Word-level and span-level tasks (e.g., SQuAD and NER) are more complex, since you need to maintain alignment between your input text and output text so that you can project your training labels. SQuAD is a particularly complex example because the input labels are character-based, and SQuAD paragraphs are often longer than our maximum sequence length. See the code in run_squad.py to show how we handle this. Before we describe the general recipe for handling word-level tasks, it's important to understand what exactly our tokenizer is doing. It has three main steps: 1.  Text normalization: Convert all whitespace characters to spaces, and (for the Uncased model) lowercase the input and strip out accent markers. E.g., John Johanson's,  john johanson's,. 2.  Punctuation splitting: Split all punctuation characters on both sides (i.e., add whitespace around all punctuation characters). Punctuation characters are defined as (a) Anything with a P Unicode class, (b) any non-letter/number/space ASCII character (e.g., characters like $ which are technically not punctuation). E.g., john johanson's,  john johanson ' s , 3.  WordPiece tokenization: Apply whitespace tokenization to the output of the above procedure, and apply WordPiece @link tokenization to each token separately. (Our implementation is directly based on the one from tensor2tensor, which is linked). E.g., john johanson ' s ,  john johan son ' s ,. The advantage of this scheme is that it is 'compatible' with most existing English tokenizers. For example, imagine that you have a part-of-speech tagging task which looks like this: @Code The tokenized output will look like this: @Code Crucially, this would be the same output as if the raw text were John Johanson's house (with no space before the 's). If you have a pre-tokenized representation with word-level annotations, you can simply tokenize each input word independently, and deterministically maintain an original-to-tokenized alignment: @Code Now orig_to_tok_map can be used to project labels to the tokenized representation. There are common English tokenization schemes which will cause a slight mismatch between how BERT was pre-trained. For example, if your input tokenization splits off contractions like do n't, this will cause a mismatch. If it is possible to do so, you should pre-process your data to convert these back to raw-looking text, but if it's not possible, this mismatch is likely not a big deal.",2
3587,154747577,2,Python,"Pre-training with BERT. We are releasing code to do 'masked LM' and 'next sentence prediction' on an arbitrary text corpus. Note that this is not the exact code that was used for the paper (the original code was written in C, and had some additional complexity), but this code does generate pre-training data as described in the paper. Here's how to run the data generation. The input is a plain text file, with one sentence per line. (It is important that these be actual sentences for the 'next sentence prediction' task). Documents are delimited by empty lines. The output is a set of tf.train.Examples serialized into TFRecord file format. You can perform sentence segmentation with an off-the-shelf NLP toolkit such as spaCy @link . The create_pretraining_data.py script will concatenate segments until they reach the maximum sequence length to minimize computational waste from padding (see the script for more details). However, you may want to intentionally add a slight amount of noise to your input data (e.g., randomly truncate 2% of input segments) to make it more robust to non-sentential input during fine-tuning. This script stores all of the examples for the entire input file in memory, so for large data files you should shard the input file and call the script multiple times. (You can pass in a file glob to run_pretraining.py, e.g., tf_examples.tf_record.) The max_predictions_per_seq is the maximum number of masked LM predictions per sequence. You should set this to around max_seq_length  masked_lm_prob (the script doesn't do that automatically because the exact value needs to be passed to both scripts). @Code Here's how to run the pre-training. Do not include init_checkpoint if you are pre-training from scratch. The model configuration (including vocab size) is specified in bert_config_file. This demo code only pre-trains for a small number of steps (20), but in practice you will probably want to set num_train_steps to 10000 steps or more. The max_seq_length and max_predictions_per_seq parameters passed to run_pretraining.py must be the same as create_pretraining_data.py. @Code This will produce an output like this: @Code Note that since our sample_text.txt file is very small, this example training will overfit that data in only a few steps and produce unrealistically high accuracy numbers.",2
3588,154747577,3,Python,"Pre-training tips and caveats. If using your own vocabulary, make sure to change vocab_size in bert_config.json. If you use a larger vocabulary without changing this, you will likely get NaNs when training on GPU or TPU due to unchecked out-of-bounds access. If your task has a large domain-specific corpus available (e.g., 'movie reviews' or 'scientific papers'), it will likely be beneficial to run additional steps of pre-training on your corpus, starting from the BERT checkpoint. The learning rate we used in the paper was 1e-4. However, if you are doing additional steps of pre-training starting from an existing BERT checkpoint, you should use a smaller learning rate (e.g., 2e-5). Current BERT models are English-only, but we do plan to release a multilingual model which has been pre-trained on a lot of languages in the near future (hopefully by the end of November 2018). Longer sequences are disproportionately expensive because attention is quadratic to the sequence length. In other words, a batch of 64 sequences of length 512 is much more expensive than a batch of 256 sequences of length 128. The fully-connected/convolutional cost is the same, but the attention cost is far greater for the 512-length sequences. Therefore, one good recipe is to pre-train for, say, 90,000 steps with a sequence length of 128 and then for 10,000 additional steps with a sequence length of 512. The very long sequences are mostly needed to learn positional embeddings, which can be learned fairly quickly. Note that this does require generating the data twice with different values of max_seq_length. If you are pre-training from scratch, be prepared that pre-training is computationally expensive, especially on GPUs. If you are pre-training from scratch, our recommended recipe is to pre-train a BERT-Base on a single preemptible Cloud TPU v2 @link , which takes about 2 weeks at a cost of about $500 USD (based on the pricing in October 2018). You will have to scale down the batch size when only training on a single Cloud TPU, compared to what was used in the paper. It is recommended to use the largest batch size that fits into TPU memory.",2
3589,154747577,3,Python,"Pre-training data. We will not be able to release the pre-processed datasets used in the paper. For Wikipedia, the recommended pre-processing is to download the latest dump @link , extract the text with WikiExtractor.py @link , and then apply any necessary cleanup to convert it into plain text. Unfortunately the researchers who collected the BookCorpus @link  no longer have it available for public download. The Project Guttenberg Dataset @link is a somewhat smaller (200M word) collection of older books that are public domain. Common Crawl @link  is another very large collection of text, but you will likely have to do substantial pre-processing and cleanup to extract a usable corpus for pre-training BERT.",2
3590,154747577,3,Python,"Learning a new WordPiece vocabulary. This repository does not include code for learning a new WordPiece vocabulary. The reason is that the code used in the paper was implemented in C with dependencies on Google's internal libraries. For English, it is almost always better to just start with our vocabulary and pre-trained models. For learning vocabularies of other languages, there are a number of open source options available. However, keep in mind that these are not compatible with our tokenization.py library: Google's SentencePiece library @link tensor2tensor's WordPiece generation script @link Rico Sennrich's Byte Pair Encoding library @link",2
3591,154747577,2,Python,"Using BERT in Colab. If you want to use BERT with Colab @link , you can get started with the notebook 'BERT FineTuning with Cloud TPUs @link '. At the time of this writing (October 31st, 2018), Colab users can access a Cloud TPU completely for free. Note: One per user, availability limited, requires a Google Cloud Platform account with storage (although storage may be purchased with free credit for signing up with GCP), and this capability may not longer be available in the future. Click on the BERT Colab that was just linked for more information.",2
3592,154747577,4,Python,"Is this code compatible with Cloud TPUs? What about GPUs?. Yes, all of the code in this repository works out-of-the-box with CPU, GPU, and Cloud TPU. However, GPU training is single-GPU only.",1
3593,154747577,4,Python,"I am getting out-of-memory errors, what is wrong?. See the section on out-of-memory issues(out-of-memory-issues) for more. information.",2
3594,154747577,4,Python,"Is there a PyTorch version available?. There is no official PyTorch implementation. However, NLP researchers from HuggingFace made a PyTorch version of BERT available @link which is compatible with our pre-trained checkpoints and is able to reproduce our results. We were not involved in the creation or maintenance of the PyTorch implementation so please direct any questions towards the authors of that repository.",1
3595,154747577,4,Python,"Is there a Chainer version available?. There is no official Chainer implementation. However, Sosuke Kobayashi made a Chainer version of BERT available @link which is compatible with our pre-trained checkpoints and is able to reproduce our results. We were not involved in the creation or maintenance of the Chainer implementation so please direct any questions towards the authors of that repository.",1
3596,154747577,4,Python,"Will models in other languages be released?. Yes, we plan to release a multi-lingual BERT model in the near future. We cannot make promises about exactly which languages will be included, but it will likely be a single model which includes most of the languages which have a significantly-sized Wikipedia.",1
3597,154747577,4,Python,Will models larger than BERT-Large be released?. So far we have not attempted to train anything larger than BERT-Large. It is possible that we will release larger models if we are able to obtain significant improvements.,1
3598,154747577,4,Python,What license is this library released under?. All code and models are released under the Apache 2.0 license. See the LICENSE file for more information.,3
3599,154747577,4,Python,"How do I cite BERT?. For now, cite the Arxiv paper @link : @Code If we submit the paper to a conference or journal, we will update the BibTeX.",3
3600,154747577,2,Python,Disclaimer. This is not an official Google product.,3
3601,154747577,2,Python,"Contact information. For help or issues using BERT, please submit a GitHub issue. For personal communication related to BERT, please contact Jacob Devlin (jacobdevlin@google.com), Ming-Wei Chang (mingweichang@google.com), or Kenton Lee (kentonl@google.com).",3
3602,160919119,2,Python,"Opinions. '_... I'm using FastAPI a ton these days. ... I'm actually planning to use it for all of my team's ML services at Microsoft. Some of them are getting integrated into the core Windows product and some Office products._' Kabir Khan - Microsoft (ref) --- '_We adopted the FastAPI library to spawn a REST server that can be queried to obtain predictions. for Ludwig_' Piero Molino, Yaroslav Dudin, and Sai Sumanth Miryala - Uber (ref) --- '_Netflix is pleased to announce the open-source release of our crisis management orchestration framework: Dispatch! built with FastAPI_' Kevin Glisson, Marc Vilanova, Forest Monsen - Netflix (ref) --- '_I’m over the moon excited about FastAPI. It’s so fun!_' Brian Okken - Python Bytes podcast host (ref) --- '_Honestly, what you've built looks super solid and polished. In many ways, it's what I wanted Hug to be - it's really inspiring to see someone build that._' Timothy Crosley - Hug creator (ref) --- '_If you're looking to learn one modern framework for building REST APIs, check out FastAPI ... It's fast, easy to use and easy to learn ..._' '_We've switched over to FastAPI for our APIs ... I think you'll like it ..._' Ines Montani - Matthew Honnibal - Explosion AI founders - spaCy creators (ref) - (ref) --- '_If anyone is looking to build a production Python API, I would highly recommend FastAPI. It is beautifully designed, simple to use and highly scalable, it has become a key component in our API first development strategy and is driving many automations and services such as our Virtual TAC Engineer._' Deon Pillsbury - Cisco (ref) ---",1
3603,160919119,2,Python,"Typer, the FastAPI of CLIs. If you are building a CLI app to be used in the terminal instead of a web API, check out Typer. Typer is FastAPI's little sibling. And it's intended to be the FastAPI of CLIs.",3
3604,160919119,2,Python,Requirements. FastAPI stands on the shoulders of giants: Starlette for the web parts. Pydantic for the data parts.,2
3605,160919119,3,Python,"Create it. Create a file main.py with: @Code Or use async def... If your code uses async / await, use async def: @Code Note: If you don't know, check the _'In a hurry?'_ section about async and await in the docs..",2
3606,160919119,3,Python,"Run it. Run the server with: @Code About the command fastapi dev main.py... The command fastapi dev reads your main.py file, detects the FastAPI app in it, and starts a server using Uvicorn. By default, fastapi dev will start with auto-reload enabled for local development. You can read more about it in the FastAPI CLI docs.",2
3607,160919119,3,Python,Check it. Open your browser at  @link You will see the JSON response as: @Code You already created an API that: Receives HTTP requests in the _paths_ / and /items/{item_id}. Both _paths_ take GET operations (also known as HTTP _methods_). The _path_ /items/{item_id} has a _path parameter_ item_id that should be an int. The _path_ /items/{item_id} has an optional str _query parameter_ q.,2
3608,160919119,3,Python,Interactive API docs. Now go to  @link You will see the automatic interactive API documentation (provided by Swagger UI):,2
3609,160919119,3,Python,"Alternative API docs. And now, go to  @link You will see the alternative automatic documentation (provided by ReDoc):",2
3610,160919119,2,Python,"Example upgrade. Now modify the file main.py to receive a body from a PUT request. Declare the body using standard Python types, thanks to Pydantic. @Code The fastapi dev server should reload automatically.",2
3611,160919119,3,Python,"Interactive API docs upgrade. Now go to  @link The interactive API documentation will be automatically updated, including the new body: Click on the button 'Try it out', it allows you to fill the parameters and directly interact with the API: Then click on the 'Execute' button, the user interface will communicate with your API, send the parameters, get the results and show them on the screen:",2
3612,160919119,3,Python,"Alternative API docs upgrade. And now, go to  @link The alternative documentation will also reflect the new query parameter and body:",2
3613,160919119,3,Python,"Recap. In summary, you declare once the types of parameters, body, etc. as function parameters. You do that with standard modern Python types. You don't have to learn a new syntax, the methods or classes of a specific library, etc. Just standard Python. For example, for an int: @Code or for a more complex Item model: @Code ...and with that single declaration you get: Editor support, including: Completion. Type checks. Validation of data: Automatic and clear errors when the data is invalid. Validation even for deeply nested JSON objects. Conversion of input data: coming from the network to Python data and types. Reading from: JSON. Path parameters. Query parameters. Cookies. Headers. Forms. Files. Conversion of output data: converting from Python data and types to network data (as JSON): Convert Python types (str, int, float, bool, list, etc). datetime objects. UUID objects. Database models. ...and many more. Automatic interactive API documentation, including 2 alternative user interfaces: Swagger UI. ReDoc. --- Coming back to the previous code example, FastAPI will: Validate that there is an item_id in the path for GET and PUT requests. Validate that the item_id is of type int for GET and PUT requests. If it is not, the client will see a useful, clear error. Check if there is an optional query parameter named q (as in  @link for GET requests. As the q parameter is declared with  None, it is optional. Without the None it would be required (as is the body in the case with PUT). For PUT requests to /items/{item_id}, Read the body as JSON: Check that it has a required attribute name that should be a str. Check that it has a required attribute price that has to be a float. Check that it has an optional attribute is_offer, that should be a bool, if present. All this would also work for deeply nested JSON objects. Convert from and to JSON automatically. Document everything with OpenAPI, that can be used by: Interactive documentation systems. Automatic client code generation systems, for many languages. Provide 2 interactive documentation web interfaces directly. --- We just scratched the surface, but you already get the idea of how it all works. Try changing the line with: @Code ...from: @Code ...to: @Code ...and see how your editor will auto-complete the attributes and know their types: For a more complete example including more features, see the Tutorial - User Guide. Spoiler alert: the tutorial - user guide includes: Declaration of parameters from other different places as: headers, cookies, form fields and files. How to set validation constraints as maximum_length or regex. A very powerful and easy to use Dependency Injection system. Security and authentication, including support for OAuth2 with JWT tokens and HTTP Basic auth. More advanced (but equally easy) techniques for declaring deeply nested JSON models (thanks to Pydantic). GraphQL integration with Strawberry and other libraries. Many extra features (thanks to Starlette) as: WebSockets extremely easy tests based on HTTPX and pytest CORS Cookie Sessions ...and more.",2
3614,160919119,2,Python,"Performance. Independent TechEmpower benchmarks show FastAPI applications running under Uvicorn as one of the fastest Python frameworks available, only below Starlette and Uvicorn themselves (used internally by FastAPI). (). To understand more about it, see the section Benchmarks.",1
3615,160919119,2,Python,"Dependencies. Used by Pydantic: email_validator - for email validation. pydantic-settings - for settings management. pydantic-extra-types - for extra types to be used with Pydantic. Used by Starlette: httpx - Required if you want to use the TestClient. jinja2 - Required if you want to use the default template configuration. python-multipart - Required if you want to support form 'parsing', with request.form(). Used by FastAPI / Starlette: uvicorn - for the server that loads and serves your application. orjson - Required if you want to use ORJSONResponse. ujson - Required if you want to use UJSONResponse. fastapi-cli - to provide the fastapi command. When you install fastapi it comes these standard dependencies.",2
3616,160919119,2,Python,"fastapi-slim. If you don't want the extra standard optional dependencies, install fastapi-slim instead. When you install with: @Code ...it includes the same code and dependencies as: @Code The standard extra dependencies are the ones mentioned above.",2
3617,160919119,2,Python,License. This project is licensed under the terms of the MIT license.,3
3618,162405963,1,Python,"Gradio: Build Machine Learning Web Apps — in Python. Gradio is an open-source Python library that is used to build machine learning and data science demos and web applications. With Gradio, you can quickly create a beautiful user interface around your machine learning models or data science workflow and let people 'try it out' by dragging-and-dropping in their own images, pasting text, recording their own voice, and interacting with your demo, all through the browser. Gradio is useful for: - Demoing your machine learning models for clients/collaborators/users/students. - Deploying your models quickly with automatic shareable links and getting feedback on model performance. - Debugging your model interactively during development using built-in manipulation and interpretation tools.",1
3619,162405963,2,Python,"Quickstart. Prerequisite: Gradio requires Python 3.7 or higher, that's all!",2
3620,162405963,3,Python,"What Does Gradio Do?. One of the best ways to share your machine learning model, API, or data science workflow with others is to create an interactive app that allows your users or colleagues to try out the demo in their browsers. Gradio allows you to build demos and share them, all in Python. And usually in just a few lines of code! So let's get started.",1
3621,162405963,3,Python,"Hello, World. To get Gradio running with a simple 'Hello, World' example, follow these three steps: 1\. Install Gradio using pip: @Code 2\. Run the code below as a Python script or in a Jupyter Notebook (or Google Colab @link ): @Code 3\. The demo below will appear automatically within the Jupyter Notebook, or pop in a browser on  @link @link  if running from a script:",2
3622,162405963,3,Python,"The Interface Class. You'll notice that in order to make the demo, we created a gradio.Interface. This Interface class can wrap any Python function with a user interface. In the example above, we saw a simple text-based function, but the function could be anything from music generator to a tax calculator to the prediction function of a pretrained machine learning model. The core Interface class is initialized with three required parameters: - fn: the function to wrap a UI around - inputs: which component(s) to use for the input (e.g. 'text', 'image' or 'audio') - outputs: which component(s) to use for the output (e.g. 'text', 'image' or 'label') Let's take a closer look at these components used to provide input and output.",2
3623,162405963,3,Python,"Components Attributes. We saw some simple Textbox components in the previous examples, but what if you want to change how the UI components look or behave? Let's say you want to customize the input text field — for example, you wanted it to be larger and have a text placeholder. If we use the actual class for Textbox instead of using the string shortcut, you have access to much more customizability through component attributes. @Code",2
3624,162405963,3,Python,"Multiple Input and Output Components. Suppose you had a more complex function, with multiple inputs and outputs. In the example below, we define a function that takes a string, boolean, and number, and returns a string and number. Take a look how you pass a list of input and output components. @Code You simply wrap the components in a list. Each component in the inputs list corresponds to one of the parameters of the function, in order. Each component in the outputs list corresponds to one of the values returned by the function, again in order.",2
3625,162405963,3,Python,"An Image Example. Gradio supports many types of components, such as Image, DataFrame, Video, or Label. Let's try an image-to-image function to get a feel for these! @Code When using the Image component as input, your function will receive a NumPy array with the shape (width, height, 3), where the last dimension represents the RGB values. We'll return an image as well in the form of a NumPy array. You can also set the datatype used by the component with the type keyword argument. For example, if you wanted your function to take a file path to an image instead of a NumPy array, the input Image component could be written as: @Code Also note that our input Image component comes with an edit button , which allows for cropping and zooming into images. Manipulating images in this way can help reveal biases or hidden flaws in a machine learning model! You can read more about the many components and how to use them in the Gradio docs @link .",2
3626,162405963,3,Python,"Blocks: More Flexibility and Control. Gradio offers two classes to build apps: 1\. Interface, that provides a high-level abstraction for creating demos that we've been discussing so far. 2\. Blocks, a low-level API for designing web apps with more flexible layouts and data flows. Blocks allows you to do things like feature multiple data flows and demos, control where components appear on the page, handle complex data flows (e.g. outputs can serve as inputs to other functions), and update properties/visibility of components based on user interaction — still all in Python. If this customizability is what you need, try Blocks instead!",2
3627,162405963,3,Python,"Hello, Blocks. Let's take a look at a simple example. Note how the API here differs from Interface. @Code Things to note: - Blocks are made with a with clause, and any component created inside this clause is automatically added to the app. - Components appear vertically in the app in the order they are created. (Later we will cover customizing layouts!) - A Button was created, and then a click event-listener was added to this button. The API for this should look familiar! Like an Interface, the click method takes a Python function, input components, and output components.",2
3628,162405963,3,Python,"More Complexity. Here's an app to give you a taste of what's possible with Blocks: @Code A lot more going on here! We'll cover how to create complex Blocks apps like this in the building with blocks @link  section for you. Congrats, you're now familiar with the basics of Gradio!  Go to our next guide @link  to learn more about the key features of Gradio.",2
3629,162405963,2,Python,"Open Source Stack. Gradio is built with many wonderful open-source libraries, please support them as well! @link @link @link @link @link @link @link @link",1
3630,162405963,2,Python,License. Gradio is licensed under the Apache License 2.0 found in the LICENSE(LICENSE) file in the root directory of this repository.,3
3631,162405963,2,Python,"Citation. Also check out the paper Gradio: Hassle-Free Sharing and Testing of ML Models in the Wild @link , ICML HILL 2019, and please cite it if you use Gradio in your work. @Code",3
3632,162405963,2,Python,"See Also. The Gradio Discord Bot @link , a Discord bot that allows you to try any Hugging Face Space @link  that is running a Gradio demo as a Discord bot.",3
3633,168799526,1,Python,"PyTorch Image Models. - What's New(whats-new). - Introduction(introduction). - Models(models). - Features(features). - Results(results). - Getting Started (Documentation)(getting-started-documentation). - Train, Validation, Inference Scripts(train-validation-inference-scripts). - Awesome PyTorch Resources(awesome-pytorch-resources). - Licenses(licenses). - Citing(citing).",1
3634,168799526,2,Python,"What's New. Updates after Oct 10, 2022 are available in version  0.9 Many changes since the last 0.6.x stable releases. They were previewed in 0.8.x dev releases but not everyone transitioned. timm.models.layers moved to timm.layers: from timm.models.layers import name will still work via deprecation mapping (but please transition to timm.layers). import timm.models.layers.module or from timm.models.layers.module import name needs to be changed now. Builder, helper, non-model modules in timm.models have a _ prefix added, ie timm.models.helpers - timm.models._helpers, there are temporary deprecation mapping files but those will be removed. All models now support architecture.pretrained_tag naming (ex resnet50.rsb_a1). The pretrained_tag is the specific weight variant (different head) for the architecture. Using only architecture defaults to the first weights in the default_cfgs for that model architecture. In adding pretrained tags, many model names that existed to differentiate were renamed to use the tag  (ex: vit_base_patch16_224_in21k - vit_base_patch16_224.augreg_in21k). There are deprecation mappings for these. A number of models had their checkpoints remaped to match architecture changes needed to better support features_onlyTrue, there are checkpoint_filter_fn methods in any model module that was remapped. These can be passed to timm.models.load_checkpoint(..., filter_fntimm.models.swin_transformer_v2.checkpoint_filter_fn) to remap your existing checkpoint. The Hugging Face Hub  @link  is now the primary source for timm weights. Model cards include link to papers, original source, license. Previous 0.6.x can be cloned from 0.6.x @link  branch or installed via pip with version.",1
3635,168799526,3,Python,"June 12, 2024. MobileNetV4 models and initial set of timm trained weights added: Apple MobileCLIP  @link  image tower model support & weights added (part of OpenCLIP support). ViTamin  @link  CLIP image tower model & weights added (part of OpenCLIP support). OpenAI CLIP Modified ResNet image tower modelling & weight support (via ByobNet). Refactor AttentionPool2d.",1
3636,168799526,3,Python,"May 14, 2024. Support loading PaliGemma jax weights into SigLIP ViT models with average pooling. Add Hiera models from Meta  @link . Add normalize flag for transorms, return non-normalized torch.Tensor with original dytpe (for chug) Version 1.0.3 release",1
3637,168799526,3,Python,"May 11, 2024. Searching for Better ViT Baselines (For the GPU Poor) weights and vit variants released. Exploring model shapes between Tiny and Base. AttentionExtract helper added to extract attention maps from timm models. See example in  @link  forward_intermediates() API refined and added to more models including some ConvNets that have other extraction methods. 1017 of 1047 model architectures support features_onlyTrue feature extraction. Remaining 34 architectures can be supported but based on priority requests. Remove torch.jit.script annotated functions including old JIT activations. Conflict with dynamo and dynamo does a much better job when used.",1
3638,168799526,3,Python,"April 11, 2024. Prepping for a long overdue 1.0 release, things have been stable for a while now. Significant feature that's been missing for a while, features_onlyTrue support for ViT models with flat hidden states or non-std module layouts (so far covering  'vit_', 'twins_', 'deit', 'beit', 'mvitv2', 'eva', 'samvit_', 'flexivit') Above feature support achieved through a new forward_intermediates() API that can be used with a feature wrapping module or direclty. @Code @Code TinyCLIP vision tower weights added, thx Thien Tran @link",1
3639,168799526,3,Python,"Feb 19, 2024. Next-ViT models added. Adapted from  @link  HGNet and PP-HGNetV2 models added. Adapted from  @link by SeeFun @link Removed setup.py, moved to pyproject.toml based build supported by PDM Add updated model EMA impl using _for_each for less overhead Support device args in train script for non GPU devices Other misc fixes and small additions Min supported Python version increased to 3.8 Release 0.9.16",1
3640,168799526,3,Python,"Jan 8, 2024. Datasets & transform refactoring HuggingFace streaming (iterable) dataset support (--dataset hfids:org/dataset) Webdataset wrapper tweaks for improved split info fetching, can auto fetch splits from supported HF hub webdataset Tested HF datasets and webdataset wrapper streaming from HF hub with recent timm ImageNet uploads to  @link  Make input & target column/field keys consistent across datasets and pass via args Full monochrome support when using e:g: --input-size 1 224 224 or --in-chans 1, sets PIL image conversion appropriately in dataset Improved several alternate crop & resize transforms (ResizeKeepRatio, RandomCropOrPad, etc) for use in PixParse document AI project Add SimCLR style color jitter prob along with grayscale and gaussian blur options to augmentations and args Allow train without validation set (--val-split '') in train script Add --bce-sum (sum over class dim) and --bce-pos-weight (positive weighting) args for training as they're common BCE loss tweaks I was often hard coding",1
3641,168799526,3,Python,"Nov 23, 2023. Added EfficientViT-Large models, thanks SeeFun @link Fix Python 3.7 compat, will be dropping support for it soon Other misc fixes Release 0.9.12",1
3642,168799526,3,Python,"Nov 20, 2023. Added significant flexibility for Hugging Face Hub based timm models via model_args config entry. model_args will be passed as kwargs through to models on creation. See example at  @link    Usage:  @link  Updated imagenet eval and test set csv files with latest models vision_transformer.py typing and doc cleanup by Laureηt @link 0.9.11 release",1
3643,168799526,3,Python,"Nov 3, 2023. DFN (Data Filtering Networks) @link  and MetaCLIP @link  ViT weights added DINOv2 'register' ViT model weights added  @link Add quickgelu ViT variants for OpenAI, DFN, MetaCLIP weights that use it (less efficient) Improved typing added to ResNet, MobileNet-v3 thanks to Aryan @link ImageNet-12k fine-tuned (from LAION-2B CLIP) convnext_xxlarge 0.9.9 release",1
3644,168799526,3,Python,"Oct 20, 2023. SigLIP @link  image tower weights supported in vision_transformer.py. Great potential for fine-tune and downstream feature use. Experimental 'register' support in vit models as per Vision Transformers Need Registers @link Updated RepViT with new weight release. Thanks wangao @link Add patch resizing support (on pretrained weight load) to Swin models 0.9.8 release pending",1
3645,168799526,3,Python,"Sep 1, 2023. TinyViT added by SeeFun @link Fix EfficientViT (MIT) to use torch.autocast so it works back to PT 1.10 0.9.7 release",1
3646,168799526,3,Python,"Aug 28, 2023. Add dynamic img size support to models in vision_transformer.py, vision_transformer_hybrid.py, deit.py, and eva.py w/o breaking backward compat. Add dynamic_img_sizeTrue to args at model creation time to allow changing the grid size (interpolate abs and/or ROPE pos embed each forward pass). Add dynamic_img_padTrue to allow image sizes that aren't divisible by patch size (pad bottom right to patch size each forward pass). Enabling either dynamic mode will break FX tracing unless PatchEmbed module added as leaf. Existing method of resizing position embedding by passing different img_size (interpolate pretrained embed weights once) on creation still works. Existing method of changing patch_size (resize pretrained patch_embed weights once) on creation still works. Example validation cmd python validate.py /imagenet --model vit_base_patch16_224 --amp --amp-dtype bfloat16 --img-size 255 --crop-pct 1.0 --model-kwargs dynamic_img_sizeTrue dyamic_img_padTrue",1
3647,168799526,3,Python,"Aug 25, 2023. Many new models since last release FastViT -  @link    MobileOne -  @link    InceptionNeXt -  @link    RepGhostNet -  @link (thanks  @link    GhostNetV2 -  @link (thanks  @link    EfficientViT (MSRA) -  @link (thanks  @link    EfficientViT (MIT) -  @link (thanks  @link  Add --reparam arg to benchmark.py, onnx_export.py, and validate.py to trigger layer reparameterization / fusion for models with any one of reparameterize(), switch_to_deploy() or fuse() Including FastViT, MobileOne, RepGhostNet, EfficientViT (MSRA), RepViT, RepVGG, and LeViT Preparing 0.9.6 'back to school' release",1
3648,168799526,3,Python,"Aug 11, 2023. Swin, MaxViT, CoAtNet, and BEiT models support resizing of image/window size on creation with adaptation of pretrained weights Example validation cmd to test w/ non-square resize python validate.py /imagenet --model swin_base_patch4_window7_224.ms_in22k_ft_in1k --amp --amp-dtype bfloat16 --input-size 3 256 320 --model-kwargs window_size8,10 img_size256,320",1
3649,168799526,3,Python,"Aug 3, 2023. Add GluonCV weights for HRNet w18_small and w18_small_v2. Converted by SeeFun @link Fix selecsls model naming regression Patch and position embedding for ViT/EVA works for bfloat16/float16 weights on load (or activations for on-the-fly resize) v0.9.5 release prep",1
3650,168799526,3,Python,"July 27, 2023. Added timm trained seresnextaa201d_32x8d.sw_in12k_ft_in1k_384 weights (and .sw_in12k pretrain) with 87.3% top-1 on ImageNet-1k, best ImageNet ResNet family model I'm aware of. RepViT model and weights  @link  added by wangao @link I-JEPA ViT feature weights (no classifier) added by SeeFun @link SAM-ViT (segment anything) feature weights (no classifier) added by SeeFun @link Add support for alternative feat extraction methods and -ve indices to EfficientNet Add NAdamW optimizer Misc fixes",1
3651,168799526,3,Python,"May 11, 2023. timm 0.9 released, transition from 0.8.xdev releases",1
3652,168799526,3,Python,"May 10, 2023. Hugging Face Hub downloading is now default, 1132 models on  @link 1163 weights in timm DINOv2 vit feature backbone weights added thanks to Leng Yue @link FB MAE vit feature backbone weights added OpenCLIP DataComp-XL L/14 feat backbone weights added MetaFormer (poolformer-v2, caformer, convformer, updated poolformer (v1)) w/ weights added by Fredo Guan @link Experimental get_intermediate_layers function on vit/deit models for grabbing hidden states (inspired by DINO impl). This is WIP and may change significantly... feedback welcome. Model creation throws error if pretrainedTrue and no weights exist (instead of continuing with random initialization) Fix regression with inception / nasnet TF sourced weights with 1001 classes in original classifiers bitsandbytes  @link  optimizers added to factory, use bnb prefix, ie bnbadam8bit Misc cleanup and fixes Final testing before switching to a 0.9 and bringing timm out of pre-release state",1
3653,168799526,3,Python,"April 27, 2023. 97% of timm models uploaded to HF Hub and almost all updated to support multi-weight pretrained configs Minor cleanup and refactoring of another batch of models as multi-weight added. More fused_attn (F.sdpa) and features_only support, and torchscript fixes.",1
3654,168799526,3,Python,"April 21, 2023. Gradient accumulation support added to train script and tested (--grad-accum-steps), thanks Taeksang Kim @link More weights on HF Hub (cspnet, cait, volo, xcit, tresnet, hardcorenas, densenet, dpn, vovnet, xception_aligned) Added --head-init-scale and --head-init-bias to train.py to scale classiifer head and set fixed bias for fine-tune Remove all InplaceABN (inplace_abn) use, replaced use in tresnet with standard BatchNorm (modified weights accordingly).",1
3655,168799526,3,Python,"April 12, 2023. Add ONNX export script, validate script, helpers that I've had kicking around for along time. Tweak 'same' padding for better export w/ recent ONNX  pytorch. Refactor dropout args for vit and vit-like models, separate drop_rate into drop_rate (classifier dropout), proj_drop_rate (block mlp / out projections), pos_drop_rate (position embedding drop), attn_drop_rate (attention dropout). Also add patch dropout (FLIP) to vit and eva models. fused F.scaled_dot_product_attention support to more vit models, add env var (TIMM_FUSED_ATTN) to control, and config interface to enable/disable Add EVA-CLIP backbones w/ image tower weights, all the way up to 4B param 'enormous' model, and 336x336 OpenAI ViT mode that was missed.",1
3656,168799526,3,Python,"April 5, 2023. ALL ResNet models pushed to Hugging Face Hub with multi-weight support All past timm trained weights added with recipe based tags to differentiate All ResNet strikes back A1/A2/A3 (seed 0) and R50 example B/C1/C2/D weights available Add torchvision v2 recipe weights to existing torchvision originals See comparison table in  @link  New ImageNet-12k  ImageNet-1k fine-tunes available for a few anti-aliased ResNet models resnetaa50d.sw_in12k_ft_in1k - 81.7 @ 224, 82.6 @ 288 resnetaa101d.sw_in12k_ft_in1k - 83.5 @ 224, 84.1 @ 288 seresnextaa101d_32x8d.sw_in12k_ft_in1k - 86.0 @ 224, 86.5 @ 288 seresnextaa101d_32x8d.sw_in12k_ft_in1k_288 - 86.5 @ 288, 86.7 @ 320",1
3657,168799526,3,Python,"March 31, 2023. Add first ConvNext-XXLarge CLIP - IN-1k fine-tune and IN-12k intermediate fine-tunes for convnext-base/large CLIP models. Add EVA-02 MIM pretrained and fine-tuned weights, push to HF hub and update model cards for all EVA models. First model over 90% top-1 (99% top-5)! Check out the original code & weights at  @link for more details on their work blending MIM, CLIP w/ many model, dataset, and train recipe tweaks. Multi-weight and HF hub for DeiT and MLP-Mixer based models",1
3658,168799526,3,Python,"March 22, 2023. More weights pushed to HF hub along with multi-weight support, including: regnet.py, rexnet.py, byobnet.py, resnetv2.py, swin_transformer.py, swin_transformer_v2.py, swin_transformer_v2_cr.py Swin Transformer models support feature extraction (NCHW feat maps for swinv2_cr_, and NHWC for all others) and spatial embedding outputs. FocalNet (from  @link models and weights added with significant refactoring, feature extraction, no fixed resolution / sizing constraint RegNet weights increased with HF hub push, SWAG, SEER, and torchvision v2 weights. SEER is pretty poor wrt to performance for model size, but possibly useful. More ImageNet-12k pretrained and 1k fine-tuned timm weights: rexnetr_200.sw_in12k_ft_in1k - 82.6 @ 224, 83.2 @ 288 rexnetr_300.sw_in12k_ft_in1k - 84.0 @ 224, 84.5 @ 288 regnety_120.sw_in12k_ft_in1k - 85.0 @ 224, 85.4 @ 288 regnety_160.lion_in12k_ft_in1k - 85.6 @ 224, 86.0 @ 288 regnety_160.sw_in12k_ft_in1k - 85.6 @ 224, 86.0 @ 288  (compare to SWAG PT  1k FT this is same BUT much lower res, blows SEER FT away) Model name deprecation  remapping functionality added (a milestone for bringing 0.8.x out of pre-release). Mappings being added... Minor bug fixes and improvements.",1
3659,168799526,3,Python,"Feb 26, 2023. Add ConvNeXt-XXLarge CLIP pretrained image tower weights for fine-tune & features (fine-tuning TBD) -- see model card @link Update convnext_xxlarge default LayerNorm eps to 1e-5 (for CLIP weights, improved stability) 0.8.15dev0",1
3660,168799526,3,Python,"Feb 20, 2023. Add 320x320 convnext_large_mlp.clip_laion2b_ft_320 and convnext_lage_mlp.clip_laion2b_ft_soup_320 CLIP image tower weights for features & fine-tune 0.8.13dev0 pypi release for latest changes w/ move to huggingface org",1
3661,168799526,3,Python,"Feb 16, 2023. safetensor checkpoint support added Add ideas from 'Scaling Vision Transformers to 22 B. Params'  @link  -- qk norm, RmsNorm, parallel block Add F.scaled_dot_product_attention support (PyTorch 2.0 only) to vit_, vit_relpos, coatnet / maxxvit (to start) Lion optimizer (w/ multi-tensor option) added  @link gradient checkpointing works with features_onlyTrue",1
3662,168799526,2,Python,"Introduction. PyTorch Image Models (timm) is a collection of image models, layers, utilities, optimizers, schedulers, data-loaders / augmentations, and reference training / validation scripts that aim to pull together a wide variety of SOTA models with ability to reproduce ImageNet training results. The work of many others is present here. I've tried to make sure all source material is acknowledged via links to github, arxiv papers, etc in the README, documentation, and code docstrings. Please let me know if I missed anything.",1
3663,168799526,3,Python,"Models. All model architecture families include variants with pretrained weights. There are specific model variants without any weights, it is NOT a bug. Help training new or better weights is always appreciated. Aggregating Nested Transformers -  @link  BEiT -  @link  Big Transfer ResNetV2 (BiT) -  @link  Bottleneck Transformers -  @link  CaiT (Class-Attention in Image Transformers) -  @link  CoaT (Co-Scale Conv-Attentional Image Transformers) -  @link  CoAtNet (Convolution and Attention) -  @link  ConvNeXt -  @link  ConvNeXt-V2 -  @link  ConViT (Soft Convolutional Inductive Biases Vision Transformers)-  @link  CspNet (Cross-Stage Partial Networks) -  @link  DeiT -  @link  DeiT-III -  @link  DenseNet -  @link  DLA -  @link  DPN (Dual-Path Network) -  @link  EdgeNeXt -  @link  EfficientFormer -  @link  EfficientNet (MBConvNet Family) EfficientNet NoisyStudent (B0-B7, L2) -  @link      EfficientNet AdvProp (B0-B8) -  @link      EfficientNet (B0-B7) -  @link      EfficientNet-EdgeTPU (S, M, L) -  @link      EfficientNet V2 -  @link      FBNet-C -  @link      MixNet -  @link      MNASNet B1, A1 (Squeeze-Excite), and Small -  @link      MobileNet-V2 -  @link      Single-Path NAS -  @link      TinyNet -  @link  EfficientViT (MIT) -  @link  EfficientViT (MSRA) -  @link  EVA -  @link  EVA-02 -  @link  FastViT -  @link  FlexiViT -  @link  FocalNet (Focal Modulation Networks) -  @link  GCViT (Global Context Vision Transformer) -  @link  GhostNet -  @link  GhostNet-V2 -  @link  gMLP -  @link  GPU-Efficient Networks -  @link  Halo Nets -  @link  HGNet / HGNet-V2 - TBD HRNet -  @link  InceptionNeXt -  @link  Inception-V3 -  @link  Inception-ResNet-V2 and Inception-V4 -  @link  Lambda Networks -  @link  LeViT (Vision Transformer in ConvNet's Clothing) -  @link  MaxViT (Multi-Axis Vision Transformer) -  @link  MetaFormer (PoolFormer-v2, ConvFormer, CAFormer) -  @link  MLP-Mixer -  @link  MobileCLIP -  @link  MobileNet-V3 (MBConvNet w/ Efficient Head) -  @link    FBNet-V3 -  @link    HardCoRe-NAS -  @link    LCNet -  @link  MobileNetV4 -  @link  MobileOne -  @link  MobileViT -  @link  MobileViT-V2 -  @link  MViT-V2 (Improved Multiscale Vision Transformer) -  @link  NASNet-A -  @link  NesT -  @link  Next-ViT -  @link  NFNet-F -  @link  NF-RegNet / NF-ResNet -  @link  PNasNet -  @link  PoolFormer (MetaFormer) -  @link  Pooling-based Vision Transformer (PiT) -  @link  PVT-V2 (Improved Pyramid Vision Transformer) -  @link  RegNet -  @link  RegNetZ -  @link  RepVGG -  @link  RepGhostNet -  @link  RepViT -  @link  ResMLP -  @link  ResNet/ResNeXt ResNet (v1b/v1.5) -  @link      ResNeXt -  @link      'Bag of Tricks' / Gluon C, D, E, S variations -  @link      Weakly-supervised (WSL) Instagram pretrained / ImageNet tuned ResNeXt101 -  @link      Semi-supervised (SSL) / Semi-weakly Supervised (SWSL) ResNet/ResNeXts -  @link      ECA-Net (ECAResNet) -  @link      Squeeze-and-Excitation Networks (SEResNet) -  @link      ResNet-RS -  @link  Res2Net -  @link  ResNeSt -  @link  ReXNet -  @link  SelecSLS -  @link  Selective Kernel Networks -  @link  Sequencer2D -  @link  Swin S3 (AutoFormerV2) -  @link  Swin Transformer -  @link  Swin Transformer V2 -  @link  Transformer-iN-Transformer (TNT) -  @link  TResNet -  @link  Twins (Spatial Attention in Vision Transformers) -  @link  Visformer -  @link  Vision Transformer -  @link  ViTamin -  @link  VOLO (Vision Outlooker) -  @link  VovNet V2 and V1 -  @link  Xception -  @link  Xception (Modified Aligned, Gluon) -  @link  Xception (Modified Aligned, TF) -  @link  XCiT (Cross-Covariance Image Transformers) -  @link",1
3664,168799526,3,Python,Optimizers. Included optimizers available via create_optimizer / create_optimizer_v2 factory methods: adabelief an implementation of AdaBelief adapted from  @link -  @link  adafactor adapted from FAIRSeq impl @link  -  @link  adahessian by David Samuel @link  -  @link  adamp and sgdp by Naver ClovAI @link  -  @link  adan an implementation of Adan adapted from  @link -  @link  lamb an implementation of Lamb and LambC (w/ trust-clipping) cleaned up and modified to support use with XLA -  @link  lars an implementation of LARS and LARC (w/ trust-clipping) -  @link  lion and implementation of Lion adapted from  @link -  @link  lookahead adapted from impl by Liam @link  -  @link  madgrad - and implementation of MADGRAD adapted from  @link -  @link  nadam an implementation of Adam w/ Nesterov momentum nadamw an impementation of AdamW (Adam w/ decoupled weight-decay) w/ Nesterov momentum. A simplified impl based on  @link  novograd by Masashi Kimura @link  -  @link  radam by Liyuan Liu @link  -  @link  rmsprop_tf adapted from PyTorch RMSProp by myself. Reproduces much improved Tensorflow RMSProp behaviour sgdw and implementation of SGD w/ decoupled weight-decay fused optimizers by name with NVIDIA Apex @link  installed bits optimizers by name with BitsAndBytes @link  installed,1
3665,168799526,3,Python,"Augmentations. Random Erasing from Zhun Zhong @link  -  @link  Mixup -  @link  CutMix -  @link  AutoAugment  @link  and RandAugment  @link  ImageNet configurations modeled after impl for EfficientNet training  @link AugMix w/ JSD loss, JSD w/ clean  augmented mixing support works with AutoAugment and RandAugment as well -  @link  SplitBachNorm - allows splitting batch norm layers between clean and augmented (auxiliary batch norm) data",1
3666,168799526,3,Python,Regularization. DropPath aka 'Stochastic Depth' -  @link  DropBlock -  @link  Blur Pooling -  @link,1
3667,168799526,3,Python,"Other. Several (less common) features that I often utilize in my projects are included. Many of their additions are the reason why I maintain my own set of models, instead of using others' via PIP: All models have a common default configuration interface and API for accessing/changing the classifier - get_classifier and reset_classifier doing a forward pass on just the features - forward_features (see documentation @link ) these makes it easy to write consistent network wrappers that work with any of the models All models support multi-scale feature map extraction (feature pyramids) via create_model (see documentation @link ) create_model(name, features_onlyTrue, out_indices..., output_stride...) out_indices creation arg specifies which feature maps to return, these indices are 0 based and generally correspond to the C(i  1) feature level. output_stride creation arg controls output stride of the network by using dilated convolutions. Most networks are stride 32 by default. Not all networks support this. feature map channel counts, reduction level (stride) can be queried AFTER model creation via the .feature_info member All models have a consistent pretrained weight loader that adapts last linear if necessary, and from 3 to 1 channel input if desired High performance reference training, validation, and inference scripts @link  that work in several process/GPU modes: NVIDIA DDP w/ a single GPU per process, multiple processes with APEX present (AMP mixed-precision optional) PyTorch DistributedDataParallel w/ multi-gpu, single process (AMP disabled as it crashes when enabled) PyTorch w/ single GPU single process (AMP optional) A dynamic global pool implementation that allows selecting from average pooling, max pooling, average  max, or concat(average, max) at model creation. All global pooling is adaptive average by default and compatible with pretrained weights. A 'Test Time Pool' wrapper that can wrap any of the included models and usually provides improved performance doing inference with input images larger than the training size. Idea adapted from original DPN implementation when I ported  @link Learning rate schedulers Ideas adopted from AllenNLP schedulers @link FAIRseq lr_scheduler @link SGDR: Stochastic Gradient Descent with Warm Restarts  @link Schedulers include step, cosine w/ restarts, tanh w/ restarts, plateau Space-to-Depth by mrT23 @link   @link  -- original paper? Adaptive Gradient Clipping  @link An extensive selection of channel and/or spatial attention modules: Bottleneck Transformer -  @link      CBAM -  @link      Effective Squeeze-Excitation (ESE) -  @link      Efficient Channel Attention (ECA) -  @link      Gather-Excite (GE) -  @link      Global Context (GC) -  @link      Halo -  @link      Involution -  @link      Lambda Layer -  @link      Non-Local (NL) -   @link      Squeeze-and-Excitation (SE) -  @link      Selective Kernel (SK) -  @link  -  @link      Shifted Window (SWIN) -  @link",1
3668,168799526,2,Python,Results. Model validation results can be found in the results tables(results/README.md),1
3669,168799526,2,Python,Getting Started (Documentation). The official documentation can be found at  @link Documentation contributions are welcome. Getting Started with PyTorch Image Models (timm): A Practitioner’s Guide @link  by Chris Hughes @link  is an extensive blog post covering many aspects of timm in detail. timmdocs @link  is an alternate set of documentation for timm. A big thanks to Aman Arora @link  for his efforts creating timmdocs. paperswithcode @link  is a good resource for browsing the models within timm.,3
3670,168799526,2,Python,"Train, Validation, Inference Scripts. The root folder of the repository contains reference train, validation, and inference scripts that work with the included models and other features of this repository. They are adaptable for other datasets and use cases with a little hacking. See documentation @link .",2
3671,168799526,2,Python,Awesome PyTorch Resources. One of the greatest assets of PyTorch is the community and their contributions. A few of my favourite resources that pair well with the models and components here are listed below.,3
3672,168799526,3,Python,"Object Detection, Instance and Semantic Segmentation. Detectron2 -  @link  Segmentation Models (Semantic) -  @link  EfficientDet (Obj Det, Semantic soon) -  @link",1
3673,168799526,3,Python,Computer Vision / Image Augmentation. Albumentations -  @link  Kornia -  @link,1
3674,168799526,3,Python,Knowledge Distillation. RepDistiller -  @link  torchdistill -  @link,1
3675,168799526,3,Python,Metric Learning. PyTorch Metric Learning -  @link,1
3676,168799526,3,Python,Training / Frameworks. fastai -  @link,1
3677,168799526,3,Python,"Code. The code here is licensed Apache 2.0. I've taken care to make sure any third party code included or adapted has compatible (permissive) licenses such as MIT, BSD, etc. I've made an effort to avoid any GPL / LGPL conflicts. That said, it is your responsibility to ensure you comply with licenses here and conditions of any dependent licenses. Where applicable, I've linked the sources/references for various components in docstrings. If you think I've missed anything please create an issue.",3
3678,168799526,3,Python,Pretrained Weights. So far all of the pretrained weights available here are pretrained on ImageNet with a select few that have some additional pretraining (see extra note below). ImageNet was released for non-commercial research purposes only  @link . It's not clear what the implications of that are for the use of pretrained weights from that dataset. Any models I have trained with ImageNet are done for research purposes and one should assume that the original dataset license applies to the weights. It's best to seek legal advice if you intend to use the pretrained weights in a commercial product.,3
3679,168799526,4,Python,"Pretrained on more than ImageNet. Several weights included or references here were pretrained with proprietary datasets that I do not have access to. These include the Facebook WSL, SSL, SWSL ResNe(Xt) and the Google Noisy Student EfficientNet models. The Facebook models have an explicit non-commercial license (CC-BY-NC 4.0,  @link  @link The Google models do not appear to have any restriction beyond the Apache 2.0 license (and ImageNet concerns). In either case, you should contact Facebook or Google with any questions.",3
3680,188660663,1,Python,"Real-Time Voice Cloning. This repository is an implementation of Transfer Learning from Speaker Verification to Multispeaker Text-To-Speech Synthesis @link  (SV2TTS) with a vocoder that works in real-time. This was my master's thesis @link . SV2TTS is a deep learning framework in three stages. In the first stage, one creates a digital representation of a voice from a few seconds of audio. In the second and third stages, this representation is used as reference to generate speech given arbitrary text. Video demonstration (click the picture): @link",1
3682,188660663,2,Python,"Heads up. Like everything else in Deep Learning, this repo has quickly gotten old. Many SaaS apps (often paying) will give you a better audio quality than this repository will. If you wish for an open-source solution with a high voice quality: - Check out paperswithcode @link  for other repositories and recent research in the field of speech synthesis. - Check out CoquiTTS @link  for a repository with a better voice cloning quality and more functionalities. - Check out MetaVoice-1B @link  for a large voice model with high voice quality",3
3683,188660663,3,Python,"1. Install Requirements. 1. Both Windows and Linux are supported. A GPU is recommended for training and for inference speed, but is not mandatory. 2. Python 3.7 is recommended. Python 3.5 or greater should work, but you'll probably have to tweak the dependencies' versions. I recommend setting up a virtual environment using venv, but this is optional. 3. Install ffmpeg @link . This is necessary for reading audio files.. 4. Install PyTorch @link . Pick the latest stable version, your operating system, your package manager (pip by default) and finally pick any of the proposed CUDA versions if you have a GPU, otherwise pick CPU. Run the given command. 5. Install the remaining requirements with pip install -r requirements.txt",2
3684,188660663,3,Python,"2. (Optional) Download Pretrained Models. Pretrained models are now downloaded automatically. If this doesn't work for you, you can manually download them here @link .",2
3685,188660663,3,Python,"3. (Optional) Test Configuration. Before you download any dataset, you can begin by testing your configuration with: python demo_cli.py If all tests pass, you're good to go.",2
3686,188660663,3,Python,"4. (Optional) Download Datasets. For playing with the toolbox alone, I only recommend downloading LibriSpeech/train-clean-100 @link . Extract the contents as /LibriSpeech/train-clean-100 where  is a directory of your choosing. Other datasets are supported in the toolbox, see here @link . You're free not to download any dataset, but then you will need your own data as audio files or you will have to record it with the toolbox..",2
3687,188660663,3,Python,"5. Launch the Toolbox. You can then try the toolbox: python demo_toolbox.py -d or python demo_toolbox.py depending on whether you downloaded any datasets. If you are running an X-server or if you have the error Aborted (core dumped), see this issue @link ..",2
3688,204086862,1,Python,"Welcome to Streamlit :wave:. The fastest way to build and share data apps. Streamlit lets you turn data scripts into shareable web apps in minutes, not weeks. It’s all Python, open-source, and free! And once you’ve created an app you can use our Community Cloud platform @link  to deploy, manage, and share your app!",1
3689,204086862,2,Python,"Installation. @Code Streamlit can also be installed in a virtual environment on Windows @link , Mac @link , and Linux @link ..",2
3690,204086862,2,Python,A little example. Streamlit makes it incredibly easy to build interactive apps: @Code,2
3691,204086862,2,Python,"A bigger example. Streamlit's simple and focused API lets you build incredibly rich and powerful tools.  This demo project @link  lets you browse the entire Udacity self-driving-car dataset @link  and run inference in real-time using the YOLO object detection net @link . The complete demo is implemented in less than 300 lines of Python. In fact, the app contains only 23 Streamlit calls @link  which illustrates all the major building blocks of Streamlit. You can try it right now at share.streamlit.io/streamlit/demo-self-driving @link .",2
3692,204086862,2,Python,"The Streamlit GitHub badge. Streamlit's GitHub badge helps others find and play with your Streamlit app. @link Once you deploy your app, you can embed this badge right into your GitHub readme.md as follows: @Code",3
3693,204086862,2,Python,"More Information. - Our launch post @link  explaining why we created Streamlit - Our Community Cloud platform announcement @link - Our amazing community @link  where Streamlit users share apps, ask questions, and help each other out - Streamlit documentation @link  and blog @link  for the latest Streamlit info - More demo projects @link  to inspire you - And if you would like to contribute, see instructions here @link",3
3694,204086862,2,Python,"Community Cloud. With Community Cloud @link  you can deploy, manage, and share your apps with the world, directly from Streamlit — all for free. Sign-up here @link .",3
3695,204086862,2,Python,License. Streamlit is completely free and open-source and licensed under the Apache 2.0 @link  license.,3
3696,206660580,2,Python,Learn More about Detectron2. Explain Like I’m 5: Detectron2            :-------------------------: @link,1
3697,206660580,2,Python,"What's New. Includes new capabilities such as panoptic segmentation, Densepose, Cascade R-CNN, rotated bounding boxes, PointRend, DeepLab, ViTDet, MViTv2 etc. Used as a library to support building research projects(projects/) on top of it. Models can be exported to TorchScript format or Caffe2 format for deployment. It trains much faster @link . See our blog post @link to see more demos and learn about detectron2.",1
3698,206660580,2,Python,Installation. See installation instructions @link .,2
3699,206660580,2,Python,"Getting Started. See Getting Started with Detectron2 @link , and the Colab Notebook @link to learn about basic usage. Learn more at our documentation @link . And see projects/(projects/) for some projects that are built on top of detectron2.",2
3700,206660580,2,Python,Model Zoo and Baselines. We provide a large set of baseline results and trained models available for download in the Detectron2 Model Zoo(MODEL_ZOO.md).,1
3701,206660580,2,Python,License. Detectron2 is released under the Apache 2.0 license(LICENSE).,3
3702,206660580,2,Python,"Citing Detectron2. If you use Detectron2 in your research or wish to refer to the baseline results published in the Model Zoo(MODEL_ZOO.md), please use the following BibTeX entry. @Code",3
3703,21467110,1,Python,"spaCy: Industrial-strength NLP. spaCy is a library for advanced Natural Language Processing in Python and Cython. It's built on the very latest research, and was designed from day one to be used in real products. spaCy comes with pretrained pipelines @link  and currently supports tokenization and training for 70 languages. It features state-of-the-art speed and neural network models for tagging, parsing, named entity recognition, text classification and more, multi-task learning with pretrained transformers like BERT, as well as a production-ready training system @link  and easy model packaging, deployment and workflow management. spaCy is commercial open-source software, released under the MIT license @link . Version 3.7 out now! Check out the release notes here. @link @link @link @link @link @link @link @link @link @link",1
3704,21467110,2,Python,Documentation. spacy 101:  @link new in v3.0:  @link usage guides:  @link api reference:  @link gpu processing:  @link models:  @link large language models:  @link universe:  @link spacy vs code extension:  @link videos:  @link online course:  @link blog:  @link project templates:  @link changelog:  @link contribute:  @link swag:  @link,2
3705,21467110,2,Python,"Where to ask questions. The spaCy project is maintained by the spaCy team @link . Please understand that we won't be able to provide individual support via email. We also believe that help is much more valuable if it's shared publicly, so that more people can benefit from it. github issue tracker:  @link github discussions:  @link stack overflow:  @link",3
3706,21467110,2,Python,"Features. - Support for 70 languages - Trained pipelines for different languages and tasks - Multi-task learning with pretrained transformers like BERT - Support for pretrained word vectors and embeddings - State-of-the-art speed - Production-ready training system - Linguistically-motivated tokenization - Components for named entity recognition, part-of-speech-tagging, dependency parsing, sentence segmentation, text classification, lemmatization, morphological analysis, entity linking and more - Easily extensible with custom components and attributes - Support for custom models in PyTorch, TensorFlow and other frameworks - Built in visualizers for syntax and NER - Easy model packaging, deployment and workflow management - Robust, rigorously evaluated accuracy For more details, see the facts, figures and benchmarks @link .",1
3707,21467110,2,Python,"Install spaCy. For detailed installation instructions, see the documentation @link . - Operating system: macOS / OS X · Linux · Windows (Cygwin, MinGW, Visual Studio) - Python version: Python 3.7 (only 64 bit) - Package managers: pip · conda (via conda-forge) pip:  @link conda:  @link",2
3708,21467110,3,Python,"pip. Using pip, spaCy releases are available as source packages and binary wheels. Before you install spaCy and its dependencies, make sure that your pip, setuptools and wheel are up to date. @Code To install additional data tables for lemmatization and normalization you can run pip install spacylookups or install spacy-lookups-data @link separately. The lookups package is needed to create blank models with lemmatization data, and to lemmatize in languages that don't yet come with pretrained models and aren't powered by third-party libraries. When using pip it is generally recommended to install packages in a virtual environment to avoid modifying system state: @Code",2
3709,21467110,3,Python,"conda. You can also install spaCy from conda via the conda-forge channel. For the feedstock including the build recipe and configuration, check out this repository @link . @Code",2
3710,21467110,3,Python,"Updating spaCy. Some updates to spaCy may require downloading new statistical models. If you're running spaCy v2.0 or higher, you can use the validate command to check if your installed models are compatible and if not, print details on how to update them: @Code If you've trained your own models, keep in mind that your training and runtime inputs must match. After updating spaCy, we recommend retraining your models with the new version. For details on upgrading from spaCy 2.x to spaCy 3.x, see the migration guide @link ..",2
3711,21467110,2,Python,"Download model packages. Trained pipelines for spaCy can be installed as Python packages. This means that they're a component of your application, just like any other module. Models can be installed using spaCy's download @link . command, or manually by pointing pip to a path or URL. available pipelines:  @link models documentation:  @link training:  @link @Code",2
3712,21467110,3,Python,"Loading and using models. To load a model, use spacy.load() @link . with the model name or a path to the model data directory. @Code You can also import a model directly via its full name and then call its load() method with no arguments. @Code For more info and examples, check out the models documentation @link .",2
3713,21467110,2,Python,"Compile from source. The other way to install spaCy is to clone its GitHub repository @link  and build it from source. That is the common way if you want to make changes to the code base. You'll need to make sure that you have a development environment consisting of a Python distribution including header files, a compiler, pip @link , virtualenv @link  and git @link  installed. The compiler part is the trickiest. How to do that depends on your system. For more details and instructions, see the documentation on compiling spaCy from source @link  and the. quickstart widget @link  to get the right. commands for your platform and Python version. @Code To install with extras: @Code",2
3714,21467110,2,Python,"Run tests. spaCy comes with an extensive test suite(spacy/tests). In order to run the tests, you'll usually want to clone the repository and build spaCy from source. This will also install the required development dependencies and test utilities defined in the requirements.txt(requirements.txt). Alternatively, you can run pytest on the tests from within the installed spacy package. Don't forget to also install the test utilities via spaCy's requirements.txt(requirements.txt): @Code",2
3715,237791077,1,Python,"Diagrams. (/LICENSE) @link @link Diagram as Code. Diagrams lets you draw the cloud system architecture in Python code. It was born for prototyping a new system architecture design without any design tools. You can also describe or visualize the existing system architecture as well. Diagrams currently supports main major providers including: AWS, Azure, GCP, Kubernetes, Alibaba Cloud, Oracle Cloud etc... It also supports On-Premise nodes, SaaS and major Programming frameworks and languages. Diagram as Code also allows you to track the architecture diagram changes in any version control system. NOTE: It does not control any actual cloud resources nor does it generate cloud formation or terraform code. It is just for drawing the cloud system architecture diagrams.",1
3716,237791077,2,Python,"Getting Started. It requires Python 3.7 or higher, check your Python version first. It uses Graphviz @link  to render the diagram, so you need to install Graphviz @link  to use diagrams. After installing graphviz (or already have it), install the diagrams. macOS users can download the Graphviz via brew install graphviz if you're using Homebrew @link . @Code You can start with quick start @link . Check out guides @link  for more details, and you can find all available nodes list in here @link ..",2
3717,237791077,2,Python,Examples. You can find all the examples on the examples @link  page.,2
3718,237791077,2,Python,"Contributing. To contribute to diagram, check out contribution guidelines(CONTRIBUTING.md). Let me know if you are using diagrams! I'll add you in showcase page. (I'm working on it!) :)",3
3719,237791077,2,Python,"Who uses it?. Apache Airflow @link  is the most popular data workflow Orchestrator. Airflow uses Diagrams to generate architecture diagrams in their documentation. Cloudiscovery @link  helps you to analyze resources in your cloud (AWS/GCP/Azure/Alibaba/IBM) account. It allows you to create a diagram of analyzed cloud resource map based on this Diagrams library, so you can draw your existing cloud infrastructure with Cloudiscovery. Airflow Diagrams @link  is an Airflow plugin that aims to easily visualise your Airflow DAGs on service level from providers like AWS, GCP, Azure, etc. via diagrams.",3
3720,237791077,2,Python,"Other languages. - If you are familiar with Go, you can use go-diagrams @link  as well.",1
3721,254832799,3,Python,All in One Hacking tool For Hackers. ! @link ! @link ! @link ! @link ! @link ! @link ! @link @link ! @link,1
3722,254832799,4,Python,Install Kali Linux in WIndows10 Without VirtualBox YOUTUBE @link  or use Docker.,2
3723,254832799,2,Python,Update Available V1.2.0  . -  Installation Bug Fixed - x Added New Tools - x Reverse Engineering - x RAT Tools - x Web Crawling - x Payload Injector - x Multitor Tools update - X Added Tool in wifijamming - X Added Tool in steganography,3
3724,254832799,1,Python,Hackingtool Menu . - Anonymously Hiding Tools(anonymously-hiding-tools). - Information gathering tools(information-gathering-tools). - Wordlist Generator(wordlist-generator). - Wireless attack tools(wireless-attack-tools). - SQL Injection Tools(sql-injection-tools). - Phishing attack tools(phishing-attack-tools). - Web Attack tools(web-attack-tools). - Post exploitation tools(post-exploitation-tools). - Forensic tools(forensic-tools). - Payload creation tools(payload-creation-tools). - Exploit framework(exploit-framework). - Reverse engineering tools(reverse-engineering-tools). - DDOS Attack Tools(ddos-attack-tools). - Remote Administrator Tools (RAT)(remote-administrator-tools--rat-). - XSS Attack Tools(xss-attack-tools). - Steganograhy tools(steganograhy-tools). - Other tools(other-tools). - SocialMedia Bruteforce(socialmedia-bruteforce). - Android Hacking tools(android-hacking-tools). - IDN Homograph Attack(idn-homograph-attack). - Email Verify tools(email-verify-tools). - Hash cracking tools(hash-cracking-tools). - Wifi Deauthenticate(wifi-deauthenticate). - SocialMedia Finder(socialmedia-finder). - Payload Injector(payload-injector). - Web crawling(web-crawling). - Mix tools(mix-tools).,1
3725,254832799,3,Python,Anonymously Hiding Tools. - Anonmously Surf @link - Multitor @link,1
3726,254832799,3,Python,Information gathering tools. - Network Map (nmap) @link - Dracnmap @link - Port scanning - Host to IP - Xerosploit @link - RED HAWK (All In One Scanning) @link - ReconSpider(For All Scanning) @link - IsItDown (Check Website Down/Up) - Infoga - Email OSINT @link - ReconDog @link - Striker @link - SecretFinder (like API & etc) @link - Find Info Using Shodan @link - Port Scanner - rang3r (Python 2.7) @link - Port Scanner - Ranger Reloaded (Python 3) @link - Breacher @link,1
3727,254832799,3,Python,Wordlist Generator. - Cupp @link - WordlistCreator @link - Goblin WordGenerator @link - Password list (1.4 Billion Clear Text Password) @link,1
3728,254832799,3,Python,Wireless attack tools. - WiFi-Pumpkin @link - pixiewps @link - Bluetooth Honeypot GUI Framework @link - Fluxion @link - Wifiphisher @link - Wifite @link - EvilTwin @link - Fastssh @link - Howmanypeople,1
3729,254832799,3,Python,SQL Injection Tools. - Sqlmap tool @link - NoSqlMap @link - Damn Small SQLi Scanner @link - Explo @link - Blisqy - Exploit Time-based blind-SQL injection @link - Leviathan - Wide Range Mass Audit Toolkit @link - SQLScan @link,1
3730,254832799,3,Python,Phishing attack tools. - Setoolkit @link - SocialFish @link - HiddenEye @link - Evilginx2 @link - I-See_You(Get Location using phishing attack) @link - SayCheese (Grab target's Webcam Shots) @link - QR Code Jacking @link - ShellPhish @link - BlackPhish @link,1
3731,254832799,3,Python,Web Attack tools. - Web2Attack @link - Skipfish - SubDomain Finder @link - CheckURL @link - Blazy(Also Find ClickJacking) @link - Sub-Domain TakeOver @link - Dirb @link,1
3732,254832799,3,Python,Post exploitation tools. - Vegile - Ghost In The Shell @link - Chrome Keylogger @link,1
3733,254832799,3,Python,Forensic tools. - Autopsy - Wireshark - Bulk extractor @link - Disk Clone and ISO Image Acquire @link - Toolsley @link - Volatility3 @link,1
3734,254832799,3,Python,Payload creation tools. - The FatRat @link - Brutal @link - Stitch @link - MSFvenom Payload Creator @link - Venom Shellcode Generator @link - Spycam @link - Mob-Droid @link - Enigma @link,1
3735,254832799,3,Python,Exploit framework. - RouterSploit @link - WebSploit @link - Commix @link - Web2Attack @link,1
3736,254832799,3,Python,Reverse engineering tools. - Androguard @link - Apk2Gold @link - JadX @link,1
3737,254832799,3,Python,DDOS Attack Tools. - SlowLoris - Asyncrone - UFOnet @link - GoldenEye @link,1
3738,254832799,3,Python,Remote Administrator Tools (RAT). - Stitch @link - Pyshell @link,1
3739,254832799,3,Python,XSS Attack Tools. - DalFox(Finder of XSS) @link - XSS Payload Generator @link - Extended XSS Searcher and Finder @link - XSS-Freak @link - XSpear @link - XSSCon @link - XanXSS @link - Advanced XSS Detection Suite @link - RVuln @link - Cyclops @link,1
3740,254832799,3,Python,Steganograhy tools. - SteganoHide - StegnoCracker - StegoCracker @link - Whitespace @link,1
3741,254832799,4,Python,SocialMedia Bruteforce. - Instagram Attack @link - AllinOne SocialMedia Attack @link - Facebook Attack @link - Application Checker @link,1
3742,254832799,4,Python,Android Hacking tools. - Keydroid @link - MySMS @link - Lockphish (Grab target LOCK PIN) @link - DroidCam (Capture Image) @link - EvilApp (Hijack Session) @link - HatCloud(Bypass CloudFlare for IP) @link,1
3743,254832799,4,Python,IDN Homograph Attack. - EvilURL @link,1
3744,254832799,4,Python,Email Verify tools. - Knockmail @link,1
3745,254832799,4,Python,Hash cracking tools. - Hash Buster @link,1
3746,254832799,4,Python,Wifi Deauthenticate. - WifiJammer-NG @link - KawaiiDeauther @link,1
3747,254832799,4,Python,SocialMedia Finder. - Find SocialMedia By Facial Recognation System @link - Find SocialMedia By UserName @link - Sherlock @link - SocialScan   Payload Injector. - Debinject @link - Pixload @link,1
3748,254832799,4,Python,Web crawling. - Gospider @link,1
3749,254832799,4,Python,Mix tools. - Terminal Multiplexer ! @link ! @link ! @link ! @link ! @link,1
3751,254832799,3,Python,!! RUN HACKINGTOOL AS ROOT !! .,2
3752,254832799,2,Python,Steps are given below : .,2
3753,254832799,2,Python,Step : 1 Download hackingtool. git clone  @link,2
3754,254832799,2,Python,Step : 2 Give Permission to hackingtool. chmod -R 755 hackingtool,2
3755,254832799,2,Python,Step : 3 Move to hackingtool directory. cd hackingtool,2
3756,254832799,2,Python,Step : 4 Run hackingtool. sudo bash install.sh,2
3757,254832799,2,Python,Step : 5 For installing tools in directory. sudo hackingtool,2
3759,254832799,3,Python,Create Docker Image. - Create the docker image @Code,2
3761,254832799,3,Python,Interact with terminal. - Get into the container @Code OUTPUT: @Code Enter the options and continue. - If need open other ports you can edit the docker-compose.yml file - Volumes are mounted in the container to persist data and can share files between the host and the container,2
3762,254832799,4,Python,Thanks to original Author of the tools used in hackingtool. Please Don't Use for illegal Activity,3
3763,254832799,3,Python,To do . -   Release Tool -   Add Tools for CTF -   Want to do automatic,3
3764,254832799,2,Python,Social Media :mailbox_with_no_mail:. @link @link,3
3765,254832799,5,Python,Your Favourite Tool is not in hackingtool or Suggestions Please CLICK HERE @link .,3
3766,254832799,4,Python,Don't Forgot to share with Your Friends .,3
3767,254832799,3,Python,The new Update get will soon stay updated.,3
3768,265612440,2,Python,Coqui.ai News. -  TTSv2 is here with 16 languages and better performance across the board. -  TTS fine-tuning code is out. Check the example recipes @link . -  TTS can now stream with,1
3769,265612440,2,Python,. TTS is a library for advanced Text-to-Speech generation. Pretrained models in 1100 languages. Tools for training new models and fine-tuning existing models in any language. Utilities for dataset analysis and curation. ______________________________________________________________________ @link @link @link @link @link ______________________________________________________________________,1
3770,265612440,2,Python,Where to ask questions. Please use our dedicated channels for questions and discussion. Help is much more valuable if it's shared publicly so that more people can benefit from it. github issue tracker:  @link github discussions:  @link discord:  @link Tutorials and Examples:  @link,3
3772,265612440,2,Python,TTS Performance. Underlined 'TTS' and 'Judy' are internal TTS models that are not released open-source. They are here to show the potential. Models prefixed with a dot (.Jofish .Abe and .Janice) are real human voices.,1
3773,265612440,2,Python,"Features. - High-performance Deep Learning models for Text2Speech tasks. - Text2Spec models (Tacotron, Tacotron2, Glow-TTS, SpeedySpeech). - Speaker Encoder to compute speaker embeddings efficiently. - Vocoder models (MelGAN, Multiband-MelGAN, GAN-TTS, ParallelWaveGAN, WaveGrad, WaveRNN) - Fast and efficient model training. - Detailed training logs on the terminal and Tensorboard. - Support for Multi-speaker TTS. - Efficient, flexible, lightweight but feature complete Trainer API. - Released and ready-to-use models. - Tools to curate Text2Speech datasets under @Code . - Utilities to use and test your models. - Modular (but not too much) code base enabling easy implementation of new ideas.",1
3774,265612440,3,Python,Spectrogram models. - Tacotron: paper @link - Tacotron2: paper @link - Glow-TTS: paper @link - Speedy-Speech: paper @link - Align-TTS: paper @link - FastPitch: paper @link - FastSpeech: paper @link - FastSpeech2: paper @link - SC-GlowTTS: paper @link - Capacitron: paper @link - OverFlow: paper @link - Neural HMM TTS: paper @link - Delightful TTS: paper @link,2
3775,265612440,3,Python,End-to-End Models. - TTS: blog @link - VITS: paper @link -  YourTTS: paper @link -  Tortoise: orig. repo @link -  Bark: orig. repo @link,2
3776,265612440,3,Python,Attention Methods. - Guided Attention: paper @link - Forward Backward Decoding: paper @link - Graves Attention: paper @link - Double Decoder Consistency: blog @link - Dynamic Convolutional Attention: paper @link - Alignment Network: paper @link,2
3777,265612440,3,Python,Speaker Encoder. - GE2E: paper @link - Angular Loss: paper @link,2
3778,265612440,3,Python,Vocoders. - MelGAN: paper @link - MultiBandMelGAN: paper @link - ParallelWaveGAN: paper @link - GAN-TTS discriminators: paper @link - WaveRNN: origin @link - WaveGrad: paper @link - HiFiGAN: paper @link - UnivNet: paper @link,2
3779,265612440,3,Python,Voice Conversion. - FreeVC: paper @link You can also help us implement more models.,2
3780,265612440,2,Python,"Installation. TTS is tested on Ubuntu 18.04 with python  3.9, /fairseq/vits. You can find the language ISO codes here @link and learn about the Fairseq models here @link . @Code",2
3781,265612440,3,Python,"Command-line tts. Synthesize speech on command line. You can either use your trained model or choose a model from the provided list. If you don't specify any models, then it uses LJSpeech based English model.",2
3782,265612440,4,Python,Single Speaker Models. - List provided models: @Code - Get model info (for both tts_models and vocoder_models): - Query by type/name: The model_info_by_name uses the name as it from the --list_models. @Code For example: @Code - Query by type/idx: The model_query_idx uses the corresponding idx from --list_models. @Code For example: @Code - Query info for model info by full name: @Code - Run TTS with default models: @Code - Run TTS and pipe out the generated TTS wav file data: @Code - Run a TTS model with its default vocoder model: @Code For example: @Code - Run with specific TTS and vocoder models from the list: @Code For example: @Code - Run your own TTS model (Using Griffin-Lim Vocoder): @Code - Run your own TTS and Vocoder models: @Code,2
3783,265612440,4,Python,Multi-speaker Models. - List the available speakers and choose a  among them: @Code - Run the multi-speaker TTS model with the target speaker ID: @Code - Run your own multi-speaker TTS model: @Code,2
3784,265612440,3,Python,Voice Conversion Models. @Code,2
3785,28751632,1,Python,Mailing List. We have a Google Group mailing list for learning the kernel source code. Here are some instructions about how to use it.,3
3786,28751632,4,Python,"Join. Send an email with any subject/content to kernelhackingsubscribe@googlegroups.com. Then you will receive a confirmation email. Reply it with any content and then you are done. If you have Google account, you can also open the archive page @link  and click Apply to join group. You will be approved automatically..",3
3787,28751632,4,Python,Send emails to mailing list. Just send emails to kernelhacking@googlegroups.com. The basic usage is the same as other mailing lists powered by mailman.,3
3788,28751632,4,Python,"Archives. @link On other languages ------------------- Brazilian Portuguese @link Chinese @link Japanese @link Korean @link Russian @link Spanish @link Turkish @link Docker ------ In order to run your own copy of the book with gitbook within a local container: 1. Enable Docker experimental features with vim or another text editor @Code Then add --experimentaltrue to the end of the ExecStart/usr/bin/dockerd -H fd:// line and save. Eg: ExecStart/usr/bin/dockerd -H fd:// --experimentaltrue Then, you need to reload and restart the Docker daemon: @Code 2. Run docker image @Code 3. Open your local copy of linux insides book under this url @link or run make browse Contributions -------------- Feel free to create issues or pull-requests if you have any problems. Please read CONTRIBUTING.md @link  before pushing any changes. Author --------------- @0xAX @link LICENSE ------------- Licensed BY-NC-SA Creative Commons @link .",2
3789,2881789,1,Python,"My Python Eggs  . I do not consider myself as a programmer. I create these little programs as experiments to play with Python, or to solve problems for myself. I would gladly accept pointers from others to improve, simplify, or make the code more efficient. If you would like to make any comments then please feel free to email me: craig@geekcomputers.co.uk. This repository contains a collection of Python scripts that are designed to reduce human workload and serve as educational examples for beginners to get started with Python. The code documentation is aligned correctly for viewing in Notepad @link  :spiral_notepad: Feel free to explore the scripts and use them for your learning and automation needs!",1
3790,2881789,2,Python,"List of Scripts:. 1. batch_file_rename.py @link  - Batch rename a group of files in a specified directory, changing their extensions. 2. create_dir_if_not_there.py @link  - Check if a directory exists in the user's home directory. Create it if it doesn't exist. 3. Fast Youtube Downloader @link  - Download YouTube videos quickly with parallel threads using aria2c. 4. Google Image Downloader @link  - Query a given term and retrieve images from the Google Image database. 5. dir_test.py @link  - Test if the directory testdir exists. If not, create it. 6. env_check.py @link  - Check if all the required environment variables are set. 7. blackjack.py @link  - Casino Blackjack-21 game in Python. 8. fileinfo.py @link  - Show file information for a given file. 9. folder_size.py @link  - Scan the current directory and all subdirectories and display their sizes. 10. logs.py @link  - Search for all .log files in a directory, zip them using the specified program, and date stamp them. 11. move_files_over_x_days.py @link  - Move all files over a specified age (in days) from the source directory to the destination directory. 12. nslookup_check.py @link  - Open the file server_list.txt and perform nslookup for each server to check the DNS entry. 13. osinfo.py @link  - Display information about the operating system on which the script is running. 14. ping_servers.py @link  - Ping the servers associated with the specified application group. 15. ping_subnet.py @link  - Scan the final range of a given IP subnet for available addresses. 16. powerdown_startup.py @link  - Ping machines in the server list. Load the putty session if the machine is up, or notify if it is not. 17. puttylogs.py @link  - Zip all the logs in the given directory. 18. script_count.py @link  - Scan the scripts directory and count the different types of scripts. 19. get_youtube_view.py @link  - Get more views for YouTube videos and repeat songs on YouTube. 20. script_listing.py @link  - List all files in a given directory and its subdirectories. 21. testlines.py @link  - Open a file and print out 100 lines of the set line variable. 22. tweeter.py @link  - Tweet text or a picture from the terminal. 23. serial_scanner.py @link  - List available serial ports in use on Linux and Windows systems. 24. get_youtube_view.py @link  - Get more views for YouTube videos and repeat songs on YouTube. 25. CountMillionCharacter.py @link  and CountMillionCharacter2.0 @link  - Get character count of a text file. 26. xkcd_downloader.py @link  - Download the latest XKCD comic and place them in a new folder called 'comics'. 27. timymodule.py @link  - An alternative to Python's 'timeit' module and easier to use. 28. calculator.py @link  - Implement a calculator using Python's eval() function. 29. Google_News.py @link  - Use BeautifulSoup to provide latest news headlines along with news links. 30. cricket_live_score @link  - Use BeautifulSoup to provide live cricket scores. 31. youtube.py @link  - Take a song name as input and fetch the YouTube URL of the best matching song and play it. 32. site_health.py @link  - Check the health of a remote server. 33. SimpleStopWatch.py @link  - Simple stop watch implementation using Python's time module. 34. Changemac.py @link  - Change your MAC address, generate a random MAC address, or enter input as a new MAC address on Linux (Successfully Tested in Ubuntu 18.04). 35. whatsapp-monitor.py @link  - Use Selenium to give online status updates about your contacts in WhatsApp on the terminal. 36. whatsapp-chat-analyzer.py @link  - WhatsApp group/individual chat analyzer that visualizes chat activity using matplotlib. 37. JARVIS.py @link  - Control Windows programs with your voice. 38. Images Downloader @link  - Download images from webpages on Unix-based systems. 39. space_invader.py.py @link  - Classical 2D space invader game to recall your childhood memories. 40. Test Case Generator @link  - Generate different types of test cases with a clean and friendly UI, used in competitive programming and software testing. 41. Extract Thumbnail From Video @link  - Extract Thumbnail from video files 42. How to begin the journey of open source (first contribution) @link  - First Contribution of open source _Note: The content in this repository belongs to the respective authors and creators. I'm just providing a formatted README.md for better presentation._",2
3791,32689863,2,Python,"Installation. WARNING: These instructions are for ManimGL _only_. Trying to use these instructions to install ManimCommunity/manim @link  or instructions there to install this version will cause problems. You should first decide which version you wish to install, then only follow the instructions for your desired version. Note: To install manim directly through pip, please pay attention to the name of the installed package. This repository is ManimGL of 3b1b. The package name is manimgl instead of manim or manimlib. Please use pip install manimgl to install the version in this repository. Manim runs on Python 3.7 or higher. System requirements are FFmpeg @link , OpenGL @link  and LaTeX @link  (optional, if you want to use LaTeX). For Linux, Pango @link  along with its development headers are required. See instruction here @link ..",2
3792,32689863,3,Python,"Directly. @Code For more options, take a look at the Using manim(using-manim) sections further below.. If you want to hack on manimlib itself, clone this repository and in that directory execute: @Code",2
3793,32689863,3,Python,Directly (Windows). 1. Install FFmpeg @link . 2. Install a LaTeX distribution. MiKTeX @link  is recommended. 3. Install the remaining Python packages. @Code,2
3794,32689863,3,Python,"Mac OSX. 1. Install FFmpeg, LaTeX in terminal using homebrew. @Code 2. Install latest version of manim using these command. @Code",2
3795,32689863,2,Python,Anaconda Install. 1. Install LaTeX as above. 2. Create a conda environment using conda create -n manim python3.8. 3. Activate the environment using conda activate manim. 4. Install manimgl using pip install -e ..,2
3796,32689863,2,Python,"Using manim. Try running the following: @Code This should pop up a window playing a simple scene. Some useful flags include: -w to write the scene to a file -o to write the scene to a file and open the result -s to skip to the end and just show the final frame. -so will save the final frame to an image and show it -n  to skip ahead to the n'th animation of a scene. -f to make the playback window fullscreen Take a look at custom_config.yml for further configuration.  To add your customization, you can either edit this file, or add another file by the same name 'custom_config.yml' to whatever directory you are running manim from.  For example this is the one @link  for 3blue1brown videos.  There you can specify where videos should be output to, where manim should look for image files and sounds you want to read in, and other defaults regarding style and video quality. Look through the example scenes @link  to get a sense of how it is used, and feel free to look through the code behind 3blue1brown videos @link  for a much larger set of example. Note, however, that developments are often made to the library without considering backwards compatibility with those old videos. To run an old project with a guarantee that it will work, you will have to go back to the commit which completed that project.",2
3797,32689863,3,Python,Documentation. Documentation is in progress at 3b1b.github.io/manim @link . And there is also a Chinese version maintained by @manim-kindergarten @link : docs.manim.org.cn @link  (in Chinese). manim-kindergarten @link  wrote and collected some useful extra classes and some codes of videos in manim_sandbox repo @link .,2
3798,32689863,2,Python,"Contributing. Is always welcome.  As mentioned above, the community edition @link  has the most active ecosystem for contributions, with testing and continuous integration, but pull requests are welcome here too.  Please explain the motivation for a given change and examples of its effect.",3
3799,32689863,2,Python,License. This project falls under the MIT license.,3
3800,33015583,1,Python,"Keras 3: Deep Learning for Humans. Keras 3 is a multi-backend deep learning framework, with support for JAX, TensorFlow, and PyTorch. Effortlessly build and train models for computer vision, natural language processing, audio processing, timeseries forecasting, recommender systems, etc. - Accelerated model development: Ship deep learning solutions faster thanks to the high-level UX of Keras and the availability of easy-to-debug runtimes like PyTorch or JAX eager execution. - State-of-the-art performance: By picking the backend that is the fastest for your model architecture (often JAX!), leverage speedups ranging from 20% to 350% compared to other frameworks. Benchmark here @link . - Datacenter-scale training: Scale confidently from your laptop to large clusters of GPUs or TPUs. Join nearly three million developers, from burgeoning startups to global enterprises, in harnessing the power of Keras 3.",1
3801,33015583,3,Python,"Install with pip. Keras 3 is available on PyPI as keras. Note that Keras 2 remains available as the tf-keras package. 1. Install keras: @Code 2. Install backend package(s). To use keras, you should also install the backend of choice: tensorflow, jax, or torch. Note that tensorflow is required for using certain Keras 3 features: certain preprocessing layers as well as tf.data pipelines.",2
3802,33015583,4,Python,"Minimal installation. Keras 3 is compatible with Linux and MacOS systems. For Windows users, we recommend using WSL2 to run Keras. To install a local development version: 1. Install dependencies: @Code 2. Run installation command from the root directory. @Code 3. Run API generation script when creating PRs that update keras_export public APIs: @Code",2
3803,33015583,4,Python,"Adding GPU support. The requirements.txt file will install a CPU-only version of TensorFlow, JAX, and PyTorch. For GPU support, we also provide a separate requirements-{backend}-cuda.txt for TensorFlow, JAX, and PyTorch. These install all CUDA dependencies via pip and expect a NVIDIA driver to be pre-installed. We recommend a clean python environment for each backend to avoid CUDA version mismatches. As an example, here is how to create a Jax GPU environment with conda: @Code",2
3804,33015583,2,Python,"Configuring your backend. You can export the environment variable KERAS_BACKEND or you can edit your local config file at /.keras/keras.json to configure your backend. Available backend options are: 'tensorflow', 'jax', 'torch'. Example: @Code In Colab, you can do: @Code Note: The backend must be configured before importing keras, and the backend cannot be changed after the package has been imported.",2
3805,33015583,2,Python,"Backwards compatibility. Keras 3 is intended to work as a drop-in replacement for tf.keras (when using the TensorFlow backend). Just take your existing tf.keras code, make sure that your calls to model.save() are using the up-to-date .keras format, and you're done. If your tf.keras model does not include custom components, you can start running it on top of JAX or PyTorch immediately. If it does include custom components (e.g. custom layers or a custom train_step()), it is usually possible to convert it to a backend-agnostic implementation in just a few minutes. In addition, Keras models can consume datasets in any format, regardless of the backend you're using: you can train your models with your existing tf.data.Dataset pipelines or PyTorch DataLoaders.",2
3806,33015583,2,Python,"Why use Keras 3?. - Run your high-level Keras workflows on top of any framework -- benefiting at will from the advantages of each framework, e.g. the scalability and performance of JAX or the production ecosystem options of TensorFlow. - Write custom components (e.g. layers, models, metrics) that you can use in low-level workflows in any framework. - You can take a Keras model and train it in a training loop written from scratch in native TF, JAX, or PyTorch. - You can take a Keras model and use it as part of a PyTorch-native Module or as part of a JAX-native model function. - Make your ML code future-proof by avoiding framework lock-in. - As a PyTorch user: get access to power and usability of Keras, at last! - As a JAX user: get access to a fully-featured, battle-tested, well-documented modeling and training library. Read more in the Keras 3 release announcement @link .",1
3807,33614304,1,Python,"The Fuck . The Fuck is a magnificent app, inspired by a @liamosaur @link tweet @link , that corrects errors in previous console commands. Is The Fuck too slow? Try the experimental instant mode!(experimental-instant-mode). !gif with examplesexamples-linkexamples-link More examples: @Code @Code @Code @Code @Code If you're not afraid of blindly running corrected commands, the require_confirmation settings(settings) option can be disabled:. @Code",1
3808,33614304,2,Python,Contents. 1. Requirements(requirements). 2. Installations(installation). 3. Updating(updating). 4. How it works(how-it-works). 5. Creating your own rules(creating-your-own-rules). 6. Settings(settings). 7. Third party packages with rules(third-party-packages-with-rules). 8. Experimental instant mode(experimental-instant-mode). 9. Developing(developing). 10. License(license-mit).,1
3809,33614304,2,Python,Requirements. - python (3.5) - pip - python-dev,2
3810,33614304,2,Python,"Installation. On macOS or Linux, you can install The Fuck via Homebrewhomebrew: @Code On Ubuntu / Mint, install The Fuck with the following commands: @Code On FreeBSD, install The Fuck with the following commands: @Code On ChromeOS, install The Fuck using chromebrew @link  with the following command: @Code On Arch based systems, install The Fuck with the following command: @Code On other systems, install The Fuck  by using pip: @Code Alternatively, you may use an OS package manager (OS X, Ubuntu, Arch). @link . It is recommended that you place this command in your .bash_profile, .bashrc, .zshrc or other startup script: @Code Or in your shell config (Bash, Zsh, Fish, Powershell, tcsh). @link Changes are only available in a new shell session. To make changes immediately available, run source /.bashrc (or your shell config file like .zshrc). To run fixed commands without confirmation, use the --yeah option (or just -y for short, or --hard if you're especially frustrated): @Code To fix commands recursively until succeeding, use the -r option: @Code",2
3811,33614304,2,Python,Updating. @Code Note: Alias functionality was changed in v1.34 of The Fuck,2
3812,33614304,2,Python,"Uninstall. To remove The Fuck, reverse the installation process: - erase or comment thefuck alias line from your Bash, Zsh, Fish, Powershell, tcsh, ... shell config - use your package manager (brew, pip3, pkg, crew, pip) to uninstall the binaries",2
3813,33614304,2,Python,"How it works. The Fuck attempts to match the previous command with a rule. If a match is found, a new command is created using the matched rule and executed. The following rules are enabled by default: adb_unknown_command &ndash; fixes misspelled commands like adb logcta; ag_literal &ndash; adds -Q to ag when suggested; aws_cli &ndash; fixes misspelled commands like aws dynamdb scan; az_cli &ndash; fixes misspelled commands like az providers; cargo &ndash; runs cargo build instead of cargo; cargo_no_command &ndash; fixes wrong commands like cargo buid; cat_dir &ndash; replaces cat with ls when you try to cat a directory; cd_correction &ndash; spellchecks and corrects failed cd commands; cd_cs &ndash; changes cs to cd; cd_mkdir &ndash; creates directories before cd'ing into them; cd_parent &ndash; changes cd.. to cd ..; chmod_x &ndash; adds execution bit; choco_install &ndash; appends common suffixes for chocolatey packages; composer_not_command &ndash; fixes composer command name; conda_mistype &ndash; fixes conda commands; cp_create_destination &ndash; creates a new directory when you attempt to cp or mv to a non-existent one cp_omitting_directory &ndash; adds -a when you cp directory; cpp11 &ndash; adds missing -stdc11 to g or clang; dirty_untar &ndash; fixes tar x command that untarred in the current directory; dirty_unzip &ndash; fixes unzip command that unzipped in the current directory; django_south_ghost &ndash; adds --delete-ghost-migrations to failed because ghosts django south migration; django_south_merge &ndash; adds --merge to inconsistent django south migration; docker_login &ndash; executes a docker login and repeats the previous command; docker_not_command &ndash; fixes wrong docker commands like docker tags; docker_image_being_used_by_container &dash; removes the container that is using the image before removing the image; dry &ndash; fixes repetitions like git git push; fab_command_not_found &ndash; fixes misspelled fabric commands; fix_alt_space &ndash; replaces AltSpace with Space character; fix_file &ndash; opens a file with an error in your $EDITOR; gem_unknown_command &ndash; fixes wrong gem commands; git_add &ndash; fixes 'pathspec 'foo' did not match any file(s) known to git.'; git_add_force &ndash; adds --force to git add ... when paths are .gitignore'd; git_bisect_usage &ndash; fixes git bisect strt, git bisect goood, git bisect rset, etc. when bisecting; git_branch_delete &ndash; changes git branch -d to git branch -D; git_branch_delete_checked_out &ndash; changes git branch -d to git checkout master && git branch -D when trying to delete a checked out branch; git_branch_exists &ndash; offers git branch -d foo, git branch -D foo or git checkout foo when creating a branch that already exists; git_branch_list &ndash; catches git branch list in place of git branch and removes created branch; git_branch_0flag &ndash; fixes commands such as git branch 0v and git branch 0r removing the created branch; git_checkout &ndash; fixes branch name or creates new branch; git_clone_git_clone &ndash; replaces git clone git clone ... with git clone ... git_clone_missing &ndash; adds git clone to URLs that appear to link to a git repository. git_commit_add &ndash; offers git commit -a ... or git commit -p ... after previous commit if it failed because nothing was staged; git_commit_amend &ndash; offers git commit --amend after previous commit; git_commit_reset &ndash; offers git reset HEAD after previous commit; git_diff_no_index &ndash; adds --no-index to previous git diff on untracked files; git_diff_staged &ndash; adds --staged to previous git diff with unexpected output; git_fix_stash &ndash; fixes git stash commands (misspelled subcommand and missing save); git_flag_after_filename &ndash; fixes fatal: bad flag '...' after filename git_help_aliased &ndash; fixes git help  commands replacing  with the aliased command; git_hook_bypass &ndash; adds --no-verify flag previous to git am, git commit, or git push command; git_lfs_mistype &ndash; fixes mistyped git lfs  commands; git_main_master &ndash; fixes incorrect branch name between main and master git_merge &ndash; adds remote to branch names; git_merge_unrelated &ndash; adds --allow-unrelated-histories when required git_not_command &ndash; fixes wrong git commands like git brnch; git_pull &ndash; sets upstream before executing previous git pull; git_pull_clone &ndash; clones instead of pulling when the repo does not exist; git_pull_uncommitted_changes &ndash; stashes changes before pulling and pops them afterwards; git_push &ndash; adds --set-upstream origin $branch to previous failed git push; git_push_different_branch_names &ndash; fixes pushes when local branch name does not match remote branch name; git_push_pull &ndash; runs git pull when push was rejected; git_push_without_commits &ndash; creates an initial commit if you forget and only git add ., when setting up a new project; git_rebase_no_changes &ndash; runs git rebase --skip instead of git rebase --continue when there are no changes; git_remote_delete &ndash; replaces git remote delete remote_name with git remote remove remote_name; git_rm_local_modifications &ndash; adds -f or --cached when you try to rm a locally modified file; git_rm_recursive &ndash; adds -r when you try to rm a directory; git_rm_staged &ndash;  adds -f or --cached when you try to rm a file with staged changes git_rebase_merge_dir &ndash; offers git rebase (--continue  git_remote_seturl_add &ndash; runs git remote add when git remote set_url on nonexistent remote; git_stash &ndash; stashes your local modifications before rebasing or switching branch; git_stash_pop &ndash; adds your local modifications before popping stash, then resets; git_tag_force &ndash; adds --force to git tag  when the tag already exists; git_two_dashes &ndash; adds a missing dash to commands like git commit -amend or git rebase -continue; go_run &ndash; appends .go extension when compiling/running Go programs; go_unknown_command &ndash; fixes wrong go commands, for example go bulid; gradle_no_task &ndash; fixes not found or ambiguous gradle task; gradle_wrapper &ndash; replaces gradle with ./gradlew; grep_arguments_order &ndash; fixes grep arguments order for situations like grep -lir . test; grep_recursive &ndash; adds -r when you try to grep directory; grunt_task_not_found &ndash; fixes misspelled grunt commands; gulp_not_task &ndash; fixes misspelled gulp tasks; has_exists_script &ndash; prepends ./ when script/binary exists; heroku_multiple_apps &ndash; adds --app  to heroku commands like heroku pg; heroku_not_command &ndash; fixes wrong heroku commands like heroku log; history &ndash; tries to replace command with the most similar command from history; hostscli &ndash; tries to fix hostscli usage; ifconfig_device_not_found &ndash; fixes wrong device names like wlan0 to wlp2s0; java &ndash; removes .java extension when running Java programs; javac &ndash; appends missing .java when compiling Java files; lein_not_task &ndash; fixes wrong lein tasks like lein rpl; long_form_help &ndash; changes -h to --help when the short form version is not supported ln_no_hard_link &ndash; catches hard link creation on directories, suggest symbolic link; ln_s_order &ndash; fixes ln -s arguments order; ls_all &ndash; adds -A to ls when output is empty; ls_lah &ndash; adds -lah to ls; man &ndash; changes manual section; man_no_space &ndash; fixes man commands without spaces, for example mandiff; mercurial &ndash; fixes wrong hg commands; missing_space_before_subcommand &ndash; fixes command with missing space like npminstall; mkdir_p &ndash; adds -p when you try to create a directory without a parent; mvn_no_command &ndash; adds clean package to mvn; mvn_unknown_lifecycle_phase &ndash; fixes misspelled life cycle phases with mvn; npm_missing_script &ndash; fixes npm custom script name in npm run-script ; npm_run_script &ndash; adds missing run-script for custom npm scripts; npm_wrong_command &ndash; fixes wrong npm commands like npm urgrade; no_command &ndash; fixes wrong console commands, for example vom/vim; no_such_file &ndash; creates missing directories with mv and cp commands; omnienv_no_such_command &ndash; fixes wrong commands for goenv, nodenv, pyenv and rbenv (eg.: pyenv isntall or goenv list); open &ndash; either prepends  @link to address passed to open or creates a new file or directory and passes it to open; pip_install &ndash; fixes permission issues with pip install commands by adding --user or prepending sudo if necessary; pip_unknown_command &ndash; fixes wrong pip commands, for example pip instatl/pip install; php_s &ndash; replaces -s by -S when trying to run a local php server; port_already_in_use &ndash; kills process that bound port; prove_recursively &ndash; adds -r when called with directory; python_command &ndash; prepends python when you try to run non-executable/without ./ python script; python_execute &ndash; appends missing .py when executing Python files; python_module_error &ndash; fixes ModuleNotFoundError by trying to pip install that module; quotation_marks &ndash; fixes uneven usage of ' and ' when containing args'; path_from_history &ndash; replaces not found path with a similar absolute path from history; rails_migrations_pending &ndash; runs pending migrations; react_native_command_unrecognized &ndash; fixes unrecognized react-native commands; remove_shell_prompt_literal &ndash; removes leading shell prompt symbol $, common when copying commands from documentations; remove_trailing_cedilla &ndash; removes trailing cedillas ç, a common typo for European keyboard layouts; rm_dir &ndash; adds -rf when you try to remove a directory; scm_correction &ndash; corrects wrong scm like hg log to git log; sed_unterminated_s &ndash; adds missing '/' to sed's s commands; sl_ls &ndash; changes sl to ls; ssh_known_hosts &ndash; removes host from known_hosts on warning; sudo &ndash; prepends sudo to the previous command if it failed because of permissions; sudo_command_from_user_path &ndash; runs commands from users $PATH with sudo; switch_lang &ndash; switches command from your local layout to en; systemctl &ndash; correctly orders parameters of confusing systemctl; terraform_init.py &ndash; runs terraform init before plan or apply; terraform_no_command.py &ndash; fixes unrecognized terraform commands; test.py &ndash; runs pytest instead of test.py; touch &ndash; creates missing directories before 'touching'; tsuru_login &ndash; runs tsuru login if not authenticated or session expired; tsuru_not_command &ndash; fixes wrong tsuru commands like tsuru shell; tmux &ndash; fixes tmux commands; unknown_command &ndash; fixes hadoop hdfs-style 'unknown command', for example adds missing '-' to the command on hdfs dfs ls; unsudo &ndash; removes sudo from previous command if a process refuses to run on superuser privilege. vagrant_up &ndash; starts up the vagrant instance; whois &ndash; fixes whois command; workon_doesnt_exists &ndash; fixes virtualenvwrapper env name os suggests to create new. wrong_hyphen_before_subcommand &ndash; removes an improperly placed hyphen (apt-install - apt install, git-log - git log, etc.) yarn_alias &ndash; fixes aliased yarn commands like yarn ls; yarn_command_not_found &ndash; fixes misspelled yarn commands; yarn_command_replaced &ndash; fixes replaced yarn commands; yarn_help &ndash; makes it easier to open yarn documentation;",2
3814,33614304,5,Python,"Back to Contents(contents). The following rules are enabled by default on specific platforms only: apt_get &ndash; installs app from apt if it not installed (requires python-commandnotfound / python3-commandnotfound); apt_get_search &ndash; changes trying to search using apt-get with searching using apt-cache; apt_invalid_operation &ndash; fixes invalid apt and apt-get calls, like apt-get isntall vim; apt_list_upgradable &ndash; helps you run apt list --upgradable after apt update; apt_upgrade &ndash; helps you run apt upgrade after apt list --upgradable; brew_cask_dependency &ndash; installs cask dependencies; brew_install &ndash; fixes formula name for brew install; brew_reinstall &ndash; turns brew install  into brew reinstall ; brew_link &ndash; adds --overwrite --dry-run if linking fails; brew_uninstall &ndash; adds --force to brew uninstall if multiple versions were installed; brew_unknown_command &ndash; fixes wrong brew commands, for example brew docto/brew doctor; brew_update_formula &ndash; turns brew update  into brew upgrade ; dnf_no_such_command &ndash; fixes mistyped DNF commands; nixos_cmd_not_found &ndash; installs apps on NixOS; pacman &ndash; installs app with pacman if it is not installed (uses yay, pikaur or yaourt if available); pacman_invalid_option &ndash; replaces lowercase pacman options with uppercase. pacman_not_found &ndash; fixes package name with pacman, yay, pikaur or yaourt. yum_invalid_operation &ndash; fixes invalid yum calls, like yum isntall vim; The following commands are bundled with The Fuck, but are not enabled by default: git_push_force &ndash; adds --force-with-lease to a git push (may conflict with git_push_pull); rm_root &ndash; adds --no-preserve-root to rm -rf / command.",2
3815,33614304,2,Python,"Creating your own rules. To add your own rule, create a file named your-rule-name.py in /.config/thefuck/rules. The rule file must contain two functions: @Code Additionally, rules can contain optional functions: @Code Rules can also contain the optional variables enabled_by_default, requires_output and priority. Command has three attributes: script, output and script_parts. Your rule should not change Command. Rules api changed in 3.0: To access a rule's settings, import it with from thefuck.conf import settings settings is a special object assembled from /.config/thefuck/settings.py, and values from env (see more below(settings)).. A simple example rule for running a script with sudo: @Code More examples of rules @link , utility functions for rules @link , app/os-specific helpers @link .",2
3816,33614304,2,Python,"Settings. Several The Fuck parameters can be changed in the file $XDG_CONFIG_HOME/thefuck/settings.py ($XDG_CONFIG_HOME defaults to /.config): rules &ndash; list of enabled rules, by default thefuck.const.DEFAULT_RULES; exclude_rules &ndash; list of disabled rules, by default ; require_confirmation &ndash; requires confirmation before running new command, by default True; wait_command &ndash; the max amount of time in seconds for getting previous command output; no_colors &ndash; disable colored output; priority &ndash; dict with rules priorities, rule with lower priority will be matched first; debug &ndash; enables debug output, by default False; history_limit &ndash; the numeric value of how many history commands will be scanned, like 2000; alter_history &ndash; push fixed command to history, by default True; wait_slow_command &ndash; max amount of time in seconds for getting previous command output if it in slow_commands list; slow_commands &ndash; list of slow commands; num_close_matches &ndash; the maximum number of close matches to suggest, by default 3. excluded_search_path_prefixes &ndash; path prefixes to ignore when searching for commands, by default . An example of settings.py: @Code Or via environment variables: THEFUCK_RULES &ndash; list of enabled rules, like DEFAULT_RULES:rm_root or sudo:no_command; THEFUCK_EXCLUDE_RULES &ndash; list of disabled rules, like git_pull:git_push; THEFUCK_REQUIRE_CONFIRMATION &ndash; require confirmation before running new command, true/false; THEFUCK_WAIT_COMMAND &ndash; the max amount of time in seconds for getting previous command output; THEFUCK_NO_COLORS &ndash; disable colored output, true/false; THEFUCK_PRIORITY &ndash; priority of the rules, like no_command9999:apt_get100, rule with lower priority will be matched first; THEFUCK_DEBUG &ndash; enables debug output, true/false; THEFUCK_HISTORY_LIMIT &ndash; how many history commands will be scanned, like 2000; THEFUCK_ALTER_HISTORY &ndash; push fixed command to history true/false; THEFUCK_WAIT_SLOW_COMMAND &ndash; the max amount of time in seconds for getting previous command output if it in slow_commands list; THEFUCK_SLOW_COMMANDS &ndash; list of slow commands, like lein:gradle; THEFUCK_NUM_CLOSE_MATCHES &ndash; the maximum number of close matches to suggest, like 5. THEFUCK_EXCLUDED_SEARCH_PATH_PREFIXES &ndash; path prefixes to ignore when searching for commands, by default . For example: @Code",2
3817,33614304,2,Python,"Third-party packages with rules. If you'd like to make a specific set of non-public rules, but would still like to share them with others, create a package named thefuck_contrib_ with the following structure: @Code The Fuck will find rules located in the rules module.",2
3818,33614304,2,Python,"Experimental instant mode. The default behavior of The Fuck requires time to re-run previous commands. When in instant mode, The Fuck saves time by logging output with script @link ), then reading the log. !gif with instant modeinstant-mode-gif-linkinstant-mode-gif-link Currently, instant mode only supports Python 3 with bash or zsh. zsh's autocorrect function also needs to be disabled in order for thefuck to work properly. To enable instant mode, add --enable-experimental-instant-mode to the alias initialization in .bashrc, .bash_profile or .zshrc. For example: @Code",2
3819,33614304,2,Python,License MIT. Project License can be found here(LICENSE.md). version-badge:    @link version-link:     @link workflow-badge:   @link workflow-link:    @link coverage-badge:   @link coverage-link:    @link license-badge:    @link examples-link:    @link instant-mode-gif-link:    @link homebrew:         @link,3
3820,33884891,1,Python,"Apache Airflow. @link @link @link @link @link @link @link @link @link @link @link @link @link @link Apache Airflow @link  (or simply Airflow) is a platform to programmatically author, schedule, and monitor workflows. When workflows are defined as code, they become more maintainable, versionable, testable, and collaborative. Use Airflow to author workflows as directed acyclic graphs (DAGs) of tasks. The Airflow scheduler executes your tasks on an array of workers while following the specified dependencies. Rich command line utilities make performing complex surgeries on DAGs a snap. The rich user interface makes it easy to visualize pipelines running in production, monitor progress, and troubleshoot issues when needed. Table of contents - Project Focus(project-focus). - Principles(principles). - Requirements(requirements). - Getting started(getting-started). - Installing from PyPI(installing-from-pypi). - Official source code(official-source-code). - Convenience packages(convenience-packages). - User Interface(user-interface). - Semantic versioning(semantic-versioning). - Version Life Cycle(version-life-cycle). - Support for Python and Kubernetes versions(support-for-python-and-kubernetes-versions). - Base OS support for reference Airflow images(base-os-support-for-reference-airflow-images). - Approach to dependencies of Airflow(approach-to-dependencies-of-airflow). - Contributing(contributing). - Voting Policy(voting-policy). - Who uses Apache Airflow?(who-uses-apache-airflow). - Who maintains Apache Airflow?(who-maintains-apache-airflow). - What goes into the next release?(what-goes-into-the-next-release). - Can I use the Apache Airflow logo in my presentation?(can-i-use-the-apache-airflow-logo-in-my-presentation). - Links(links). - Sponsors(sponsors).",1
3821,33884891,2,Python,"Project Focus. Airflow works best with workflows that are mostly static and slowly changing. When the DAG structure is similar from one run to the next, it clarifies the unit of work and continuity. Other similar projects include Luigi @link , Oozie @link  and Azkaban @link . Airflow is commonly used to process data, but has the opinion that tasks should ideally be idempotent (i.e., results of the task will be the same, and will not create duplicated data in a destination system), and should not pass large quantities of data from one task to the next (though tasks can pass metadata using Airflow's XCom feature @link ). For high-volume, data-intensive tasks, a best practice is to delegate to external services specializing in that type of work. Airflow is not a streaming solution, but it is often used to process real-time data, pulling data off streams in batches.",1
3822,33884891,2,Python,"Principles. - Dynamic: Airflow pipelines are configuration as code (Python), allowing for dynamic pipeline generation. This allows for writing code that instantiates pipelines dynamically. - Extensible: Easily define your own operators, executors and extend the library so that it fits the level of abstraction that suits your environment. - Elegant: Airflow pipelines are lean and explicit. Parameterizing your scripts is built into the core of Airflow using the powerful Jinja templating engine. - Scalable: Airflow has a modular architecture and uses a message queue to orchestrate an arbitrary number of workers.",1
3823,33884891,2,Python,"Requirements. Apache Airflow is tested with: \ Experimental Note: MySQL 5.x versions are unable to or have limitations with running multiple schedulers -- please see the Scheduler docs @link . MariaDB is not tested/recommended. Note: SQLite is used in Airflow tests. Do not use it in production. We recommend using the latest stable version of SQLite for local development. Note: Airflow currently can be run on POSIX-compliant Operating Systems. For development, it is regularly tested on fairly modern Linux Distros and recent versions of macOS. On Windows you can run it via WSL2 (Windows Subsystem for Linux 2) or via Linux Containers. The work to add Windows support is tracked via 10388 @link , but. it is not a high priority. You should only use Linux-based distros as 'Production' execution environment as this is the only environment that is supported. The only distro that is used in our CI tests and that is used in the Community managed DockerHub image @link  is Debian Bookworm. We also have support for legacy Debian Bullseye base image if you want to build a custom image but it is deprecated and option to do it will be removed in the Dockerfile that will accompany Airflow 2.9.2 so you are advised to switch to Debian Bookworm for your custom images.",2
3824,33884891,2,Python,"Getting started. Visit the official Airflow website documentation (latest stable release) for help with installing Airflow @link , getting started @link , or walking through a more complete tutorial @link . Note: If you're looking for documentation for the main branch (latest development branch): you can find it on s.apache.org/airflow-docs @link . For more information on Airflow Improvement Proposals (AIPs), visit the Airflow Wiki @link . Documentation for dependent projects like provider packages, Docker image, Helm Chart, you'll find it in the documentation index @link .",2
3825,33884891,2,Python,"Installing from PyPI. We publish Apache Airflow as apache-airflow package in PyPI. Installing it however might be sometimes tricky because Airflow is a bit of both a library and application. Libraries usually keep their dependencies open, and applications usually pin them, but we should do neither and both simultaneously. We decided to keep our dependencies as open as possible (in pyproject.toml) so users can install different versions of libraries if needed. This means that pip install apache-airflow will not work from time to time or will produce unusable Airflow installation. To have repeatable installation, however, we keep a set of 'known-to-be-working' constraint files in the orphan constraints-main and constraints-2-0 branches. We keep those 'known-to-be-working' constraints files separately per major/minor Python version. You can use them as constraint files when installing Airflow from PyPI. Note that you have to specify correct Airflow tag/version/branch and Python versions in the URL. 1. Installing just Airflow: Note: Only pip installation is currently officially supported. While it is possible to install Airflow with tools like Poetry @link  or pip-tools @link , they do not share the same workflow as pip - especially when it comes to constraint vs. requirements management. Installing via Poetry or pip-tools is not currently supported. There are known issues with bazel that might lead to circular dependencies when using it to install Airflow. Please switch to pip if you encounter such problems. Bazel community works on fixing the problem in this PR _ so it might be that newer versions of bazel will handle it. If you wish to install Airflow using those tools, you should use the constraint files and convert them to the appropriate format and workflow that your tool requires. @Code 2. Installing with extras (i.e., postgres, google) @Code For information on installing provider packages, check providers @link .",2
3826,33884891,2,Python,"Official source code. Apache Airflow is an Apache Software Foundation @link  (ASF) project, and our official source code releases: - Follow the ASF Release Policy @link - Can be downloaded from the ASF Distribution Directory @link - Are cryptographically signed by the release manager - Are officially voted on by the PMC members during the Release Approval Process @link . Following the ASF rules, the source packages released must be sufficient for a user to build and test the release provided they have access to the appropriate platform and tools.",2
3827,33884891,2,Python,"Convenience packages. There are other ways of installing and using Airflow. Those are 'convenience' methods - they are not 'official releases' as stated by the ASF Release Policy, but they can be used by the users who do not want to build the software themselves. Those are - in the order of most common ways people install Airflow: - PyPI releases @link  to install Airflow using standard pip tool - Docker Images @link  to install airflow via docker tool, use them in Kubernetes, Helm Charts, docker-compose, docker swarm, etc. You can read more about using, customising, and extending the images in the Latest docs @link , and learn details on the internals in the images @link  document. - Tags in GitHub @link  to retrieve the git project sources that were used to generate official source packages via git All those artifacts are not official releases, but they are prepared using officially released sources. Some of those artifacts are 'development' or 'pre-release' ones, and they are clearly marked as such following the ASF Policy.",2
3828,33884891,2,Python,User Interface. - DAGs: Overview of all DAGs in your environment. - Grid: Grid representation of a DAG that spans across time. - Graph: Visualization of a DAG's dependencies and their current status for a specific run. - Task Duration: Total time spent on different tasks over time. - Gantt: Duration and overlap of a DAG. - Code: Quick way to view source code of a DAG.,2
3829,33884891,2,Python,"Semantic versioning. As of Airflow 2.0.0, we support a strict SemVer @link  approach for all packages released. There are few specific rules that we agreed to that define details of versioning of the different packages: Airflow: SemVer rules apply to core airflow only (excludes any changes to providers). Changing limits for versions of Airflow dependencies is not a breaking change on its own. Airflow Providers: SemVer rules apply to changes in the particular provider's code only. SemVer MAJOR and MINOR versions for the packages are independent of the Airflow version. For example, google 4.1.0 and amazon 3.0.3 providers can happily be installed with Airflow 2.1.2. If there are limits of cross-dependencies between providers and Airflow packages, they are present in providers as install_requires limitations. We aim to keep backwards compatibility of providers with all previously released Airflow 2 versions but there will sometimes be breaking changes that might make some, or all providers, have minimum Airflow version specified. Airflow Helm Chart: SemVer rules apply to changes in the chart only. SemVer MAJOR and MINOR versions for the chart are independent of the Airflow version. We aim to keep backwards compatibility of the Helm Chart with all released Airflow 2 versions, but some new features might only work starting from specific Airflow releases. We might however limit the Helm Chart to depend on minimal Airflow version. Airflow API clients: Their versioning is independent from Airflow versions. They follow their own SemVer rules for breaking changes and new features - which for example allows to change the way we generate the clients.",1
3830,33884891,2,Python,Version Life Cycle. Apache Airflow version life cycle: Limited support versions will be supported with security and critical bug fix only. EOL versions will not get any fixes nor support. We always recommend that all users run the latest available minor release for whatever major version is in use. We highly recommend upgrading to the latest Airflow major release at the earliest convenient time and before the EOL date.,1
3831,33884891,2,Python,"Support for Python and Kubernetes versions. As of Airflow 2.0, we agreed to certain rules we follow for Python and Kubernetes support. They are based on the official release schedule of Python and Kubernetes, nicely summarized in the Python Developer's Guide @link  and. Kubernetes version skew policy @link . 1. We drop support for Python and Kubernetes versions when they reach EOL. Except for Kubernetes, a version stays supported by Airflow if two major cloud providers still provide support for it. We drop support for those EOL versions in main right after EOL date, and it is effectively removed when we release the first new MINOR (Or MAJOR if there is no new MINOR version) of Airflow. For example, for Python 3.8 it means that we will drop support in main right after 27.06.2023, and the first MAJOR or MINOR version of Airflow released after will not have it. 2. We support a new version of Python/Kubernetes in main after they are officially released, as soon as we make them work in our CI pipeline (which might not be immediate due to dependencies catching up with new versions of Python mostly) we release new images/support in Airflow based on the working CI setup. 3. This policy is best-effort which means there may be situations where we might terminate support earlier if circumstances require it.",1
3832,33884891,2,Python,"Base OS support for reference Airflow images. The Airflow Community provides conveniently packaged container images that are published whenever we publish an Apache Airflow release. Those images contain: Base OS with necessary packages to install Airflow (stable Debian OS) Base Python installation in versions supported at the time of release for the MINOR version of Airflow released (so there could be different versions for 2.3 and 2.2 line for example) Libraries required to connect to supported Databases (again the set of databases supported depends on the MINOR version of Airflow) Predefined set of popular providers (for details see the Dockerfile @link ). Possibility of building your own, custom image where the user can choose their own set of providers and libraries (see Building the image @link ) In the future Airflow might also support a 'slim' version without providers nor database clients installed The version of the base OS image is the stable version of Debian. Airflow supports using all currently active stable versions - as soon as all Airflow dependencies support building, and we set up the CI pipeline for building and testing the OS version. Approximately 6 months before the end-of-regular support of a previous stable version of the OS, Airflow switches the images released to use the latest supported version of the OS. For example since Debian Buster end-of-life was August 2022, Airflow switched the images in main branch to use Debian Bullseye in February/March 2022. The version was used in the next MINOR release after the switch happened. In case of the Bullseye switch - 2.3.0 version used Debian Bullseye. The images released  in the previous MINOR version continue to use the version that all other releases for the MINOR version used. Similar switch from Debian Bullseye to Debian Bookworm has been implemented before 2.8.0 release in October 2023 and Debian Bookworm will be the only option supported as of Airflow 2.9.0. Users will continue to be able to build their images using stable Debian releases until the end of regular support and building and verifying of the images happens in our CI but no unit tests were executed using this image in the main branch.",1
3833,33884891,2,Python,"Approach to dependencies of Airflow. Airflow has a lot of dependencies - direct and transitive, also Airflow is both - library and application, therefore our policies to dependencies has to include both - stability of installation of application, but also ability to install newer version of dependencies for those users who develop DAGs. We developed the approach where constraints are used to make sure airflow can be installed in a repeatable way, while we do not limit our users to upgrade most of the dependencies. As a result we decided not to upper-bound version of Airflow dependencies by default, unless we have good reasons to believe upper-bounding them is needed because of importance of the dependency as well as risk it involves to upgrade specific dependency. We also upper-bound the dependencies that we know cause problems. The constraint mechanism of ours takes care about finding and upgrading all the non-upper bound dependencies automatically (providing that all the tests pass). Our main build failures will indicate in case there are versions of dependencies that break our tests - indicating that we should either upper-bind them or that we should fix our code/tests to account for the upstream changes from those dependencies. Whenever we upper-bound such a dependency, we should always comment why we are doing it - i.e. we should have a good reason why dependency is upper-bound. And we should also mention what is the condition to remove the binding.",1
3834,33884891,3,Python,"Approach for dependencies for Airflow Core. Those dependencies are maintained in pyproject.toml. There are few dependencies that we decided are important enough to upper-bound them by default, as they are known to follow predictable versioning scheme, and we know that new versions of those are very likely to bring breaking changes. We commit to regularly review and attempt to upgrade to the newer versions of the dependencies as they are released, but this is manual process. The important dependencies are: SQLAlchemy: upper-bound to specific MINOR version (SQLAlchemy is known to remove deprecations and introduce breaking changes especially that support for different Databases varies and changes at various speed) Alembic: it is important to handle our migrations in predictable and performant way. It is developed together with SQLAlchemy. Our experience with Alembic is that it very stable in MINOR version Flask: We are using Flask as the back-bone of our web UI and API. We know major version of Flask are very likely to introduce breaking changes across those so limiting it to MAJOR version makes sense werkzeug: the library is known to cause problems in new versions. It is tightly coupled with Flask libraries, and we should update them together celery: Celery is a crucial component of Airflow as it used for CeleryExecutor (and similar). Celery follows SemVer @link , so. we should upper-bound it to the next MAJOR version. Also, when we bump the upper version of the library, we should make sure Celery Provider minimum Airflow version is updated. kubernetes: Kubernetes is a crucial component of Airflow as it is used for the KubernetesExecutor (and similar). Kubernetes Python library follows SemVer @link ,. so we should upper-bound it to the next MAJOR version. Also, when we bump the upper version of the library, we should make sure Kubernetes Provider minimum Airflow version is updated.",2
3835,33884891,3,Python,"Approach for dependencies in Airflow Providers and extras. The main part of the Airflow is the Airflow Core, but the power of Airflow also comes from a number of providers that extend the core functionality and are released separately, even if we keep them (for now) in the same monorepo for convenience. You can read more about the providers in the Providers documentation @link . We also have set of policies implemented for maintaining and releasing community-managed providers as well as the approach for community vs. 3rd party providers in the providers @link  document. Those extras and providers dependencies are maintained in provider.yaml of each provider. By default, we should not upper-bound dependencies for providers, however each provider's maintainer might decide to add additional limits (and justify them with comment).",2
3836,33884891,2,Python,Contributing. Want to help build Apache Airflow? Check out our contributing documentation @link . Official Docker (container) images for Apache Airflow are described in images(dev/breeze/doc/ci/02_images.md).,3
3837,33884891,2,Python,"Voting Policy. Commits need a 1 vote from a committer who is not the author When we do AIP voting, both PMC member's and committer's 1s are considered a binding vote.",3
3838,33884891,2,Python,Who uses Apache Airflow?. We know about around 500 organizations that are using Apache Airflow (but there are likely many more) in the wild @link . If you use Airflow - feel free to make a PR to add your organisation to the list.,3
3839,33884891,2,Python,"Who maintains Apache Airflow?. Airflow is the work of the community @link , but the core committers/maintainers @link . are responsible for reviewing and merging PRs as well as steering conversations around new feature requests. If you would like to become a maintainer, please review the Apache Airflow committer requirements @link ..",3
3840,33884891,2,Python,"What goes into the next release?. Often you will see an issue that is assigned to specific milestone with Airflow version, or a PR that gets merged to the main branch and you might wonder which release the merged PR(s) will be released in or which release the fixed issues will be in. The answer to this is as usual - it depends on various scenarios. The answer is different for PRs and Issues. To add a bit of context, we are following the Semver @link  versioning scheme as described in Airflow release process @link . More details are explained in detail in this README under the Semantic versioning(semantic-versioning) chapter, but. in short, we have MAJOR.MINOR.PATCH versions of Airflow. MAJOR version is incremented in case of breaking changes MINOR version is incremented when there are new features added PATCH version is incremented when there are only bug-fixes and doc-only changes Generally we release MINOR versions of Airflow from a branch that is named after the MINOR version. For example 2.7. releases are released from v2-7-stable branch, 2.8. releases are released from v2-8-stable branch, etc. Most of the time in our release cycle, when the branch for next MINOR branch is not yet created, all PRs merged to main (unless they get reverted), will find their way to the next MINOR release. For example if the last release is 2.7.3 and v2-8-stable branch is not created yet, the next MINOR release is 2.8.0 and all PRs merged to main will be released in 2.8.0. There is a brief period of time when we cut a new MINOR release branch and prepare the alpha, beta, RC candidates for the 2.NEXT_MINOR.0 version where PRs merged to main will only be released in the following MINOR release. However, some PRs (bug-fixes and doc-only changes) when merged, can be cherry-picked to current MINOR branch and released in the next PATCHLEVEL release - for example when the last released version from v2-7-stable branch is 2.7.2. Some of the PRs from main can be marked as 2.7.3 milestone by committers, the release manager will try to cherry-pick them into the release branch. If successful, they will be released in 2.7.3. The final decision about cherry-picking is made by the release manager. Marking issues with a milestone is a bit different. Maintainers do not mark issues with a milestone usually, normally they are only marked in PRs. If PR linked to the issue (and 'fixing it') gets merged and released in a specific version following the process described above, the issue will be automatically closed, no milestone will be set for the issue, you need to check the PR that fixed the issue to see which version it was released in. However, sometimes maintainers mark issues with specific milestone, which means that the issue is important to become a candidate to take a look when the release is being prepared. Since this is an Open-Source project, where basically all contributors volunteer their time, there is no guarantee that specific issue will be fixed in specific version. We do not want to hold the release because some issue is not fixed, so in such case release manager will reassign such unfixed issues to the next milestone in case they are not fixed in time for the current release. Therefore, the milestone for issue is more of an intent that it should be looked at, than promise it will be fixed in the version. More context and FAQ about the patchlevel release can be found in the What goes into the next release(dev/WHAT_GOES_INTO_THE_NEXT_RELEASE.md) document in the dev folder of the repository.",3
3841,33884891,2,Python,Can I use the Apache Airflow logo in my presentation?. Yes! Be sure to abide by the Apache Foundation trademark policies @link  and the Apache Airflow Brandbook @link . The most up-to-date logos are found in this repo @link  and on the Apache Software Foundation website @link ..,3
3842,33884891,2,Python,Links. - Documentation @link - Chat @link - Community Information @link,3
3843,33884891,2,Python,Sponsors. The CI infrastructure for Apache Airflow has been sponsored by:,3
3844,3544424,2,Python,"We lost 54k GitHub stars. Please note we recently accidentally made this repo private for a moment, and GitHub deleted our community that took a decade to build. Read the full story here:  @link !(docs/stardust.png)",3
3845,3544424,2,Python,Getting started. - Installation instructions  @link . - Full documentation  @link,2
3846,3544424,2,Python,"Features. - Expressive and intuitive syntax - Formatted and colorized terminal output - Built-in JSON support - Forms and file uploads - HTTPS, proxies, and authentication - Arbitrary request data - Custom headers - Persistent sessions - wget-like downloads See all features  @link",1
3847,3544424,2,Python,"Examples. Hello World: @Code Custom HTTP method @link , HTTP headers @link  and JSON @link  data:. @Code Build and print a request without sending it using offline mode @link : @Code Use GitHub API @link  to post a comment on an Issue @link  with authentication @link :. @Code See more examples  @link .",2
3848,3544424,2,Python,"Community & support. - Visit the HTTPie website @link  for full documentation and useful links. - Join our Discord server @link  is to ask questions, discuss features, and for general API chat. - Tweet at @httpie @link  on Twitter. - Use StackOverflow @link  to ask questions and include a httpie tag. - Create GitHub Issues @link  for bug reports and feature requests. - Subscribe to the HTTPie newsletter @link  for occasional updates.",3
3849,3544424,2,Python,"Contributing. Have a look through existing Issues @link  and Pull Requests @link  that you could help with. If you'd like to request a feature or report a bug, please create a GitHub Issue @link  using one of the templates provided. See contribution guide  @link",3
3850,3638964,1,Python,"Ansible. Ansible is a radically simple IT automation system. It handles configuration management, application deployment, cloud provisioning, ad-hoc task execution, network automation, and multi-node orchestration. Ansible makes complex changes like zero-downtime rolling updates with load balancers easy. More information on the Ansible website @link .",1
3851,3638964,2,Python,"Design Principles. Have an extremely simple setup process with a minimal learning curve. Manage machines quickly and in parallel. Avoid custom-agents and additional open ports, be agentless by leveraging the existing SSH daemon. Describe infrastructure in a language that is both machine and human friendly. Focus on security and easy auditability/review/rewriting of content. Manage new remote machines instantly, without bootstrapping any software. Allow module development in any dynamic language, not just Python. Be usable as non-root. Be the easiest IT automation system to use, ever.",1
3852,3638964,2,Python,"Use Ansible. You can install a released version of Ansible with pip or a package manager. See our installation guide @link  for details on installing Ansible on a variety of platforms. Power users and developers can run the devel branch, which has the latest features and fixes, directly. Although it is reasonably stable, you are more likely to encounter breaking changes when running the devel branch. We recommend getting involved in the Ansible community if you want to run the devel branch.",2
3853,3638964,2,Python,"Get Involved. Read Community Information @link  for all kinds of ways to contribute to and interact with the project, including mailing list information and how to submit bug reports and code to Ansible. Join a Working Group @link ,. an organized community devoted to a specific technology domain or platform. Submit a proposed code update through a pull request to the devel branch. Talk to us before making larger changes to avoid duplicate efforts. This not only helps everyone know what is going on, but it also helps save time and effort if we decide some changes are needed. For a list of email lists, IRC channels and Working Groups, see the Communication page @link",3
3854,3638964,2,Python,"Coding Guidelines. We document our Coding Guidelines in the Developer Guide @link . We particularly suggest you review: Contributing your module to Ansible @link Conventions, tips, and pitfalls @link",3
3855,3638964,2,Python,Branch Info. The devel branch corresponds to the release actively under development. The stable-2.X branches correspond to stable releases. Create a branch based on devel and set up a dev environment @link  if you want to open a PR.. See the Ansible release and maintenance @link  page for information about active branches.,3
3856,3638964,2,Python,"Roadmap. Based on team and community feedback, an initial roadmap will be published for a major or minor version (ex: 2.7, 2.8). The Ansible Roadmap page @link  details what is planned and how to influence the roadmap.",3
3857,3638964,2,Python,"Authors. Ansible was created by Michael DeHaan @link and has contributions from over 5000 users (and growing). Thanks everyone! Ansible @link  is sponsored by Red Hat, Inc. @link",3
3858,3638964,2,Python,License. GNU General Public License v3.0 or later See COPYING(COPYING) to see the full text.,3
3859,393571599,2,Python,"Features. Chinese supported mandarin and tested with multiple datasets: aidatatang_200zh, magicdata, aishell3, data_aishell, and etc. PyTorch worked for pytorch, tested in version of 1.9.0(latest in August 2021), with GPU Tesla T4 and GTX 2060 Windows  Linux run in both Windows OS and linux OS (even in M1 MACOS) Easy & Awesome effect with only newly-trained synthesizer, by reusing the pretrained encoder/vocoder Webserver Ready to serve your result with remote calling",1
3862,393571599,4,Python,"1.1 General Setup. Follow the original repo to test if you got all environment ready. Python 3.7 or higher  is needed to run the toolbox. Install PyTorch @link . If you get an ERROR: Could not find a version that satisfies the requirement torch1.9.0cu102 (from versions: 0.1.2, 0.1.2.post1, 0.1.2.post2 ) This error is probably due to a low version of python, try using 3.9 and it will install successfully Install ffmpeg @link .. Run pip install -r requirements.txt to install the remaining necessary packages. Install webrtcvad pip install webrtcvad-wheels(If you need) or - install dependencies with conda or mamba @Code @Code will create a virtual environment where necessary dependencies are installed. Switch to the new environment by conda activate env_name and enjoy it. env.yml only includes the necessary dependencies to run the project，temporarily without monotonic-align. You can check the official website to install the GPU version of pytorch.",2
3863,393571599,4,Python,"1.2 Setup with a M1 Mac. The following steps are a workaround to directly use the original demo_toolbox.pywithout the changing of codes. Since the major issue comes with the PyQt5 packages used in demo_toolbox.py not compatible with M1 chips, were one to attempt on training models with the M1 chip, either that person can forgo demo_toolbox.py, or one can try the web.py in the project.",2
3864,393571599,5,Python,"1.2.1 Install PyQt5, with ref @link  here.. Create and open a Rosetta Terminal, with ref @link  here. Use system Python to create a virtual environment for the project @Code Upgrade pip and install PyQt5 @Code",2
3865,393571599,5,Python,"1.2.2 Install pyworld and ctc-segmentation. Both packages seem to be unique to this project and are not seen in the original Real-Time Voice Cloning @link  project. When installing with pip install, both packages lack wheels so the program tries to directly compile from c code and could not find Python.h. Install pyworld brew install python Python.h can come with Python installed by brew export CPLUS_INCLUDE_PATH/opt/homebrew/Frameworks/Python.framework/Headers The filepath of brew-installed Python.h is unique to M1 MacOS and listed above. One needs to manually add the path to the environment variables. pip install pyworld that should do. Installctc-segmentation Same method does not apply to ctc-segmentation, and one needs to compile it from the source code on github @link . git clone  @link      cd ctc-segmentation source /PathToMockingBird/venv/bin/activate If the virtual environment hasn't been deployed, activate it. cythonize -3 ctc_segmentation/ctc_segmentation_dyn.pyx /usr/bin/arch -x86_64 python setup.py build Build with x86 architecture. /usr/bin/arch -x86_64 python setup.py install --optimize1 --skip-buildInstall with x86 architecture.",2
3866,393571599,5,Python,"1.2.3 Other dependencies. /usr/bin/arch -x86_64 pip install torch torchvision torchaudio Pip installing PyTorch as an example, articulate that it's installed with x86 architecture pip install ffmpeg  Install ffmpeg pip install -r requirements.txt Install other requirements.",2
3867,393571599,5,Python,"1.2.4 Run the Inference Time (with Toolbox). To run the project on x86 architecture. ref @link . vim /PathToMockingBird/venv/bin/pythonM1 Create an executable file pythonM1 to condition python interpreter at /PathToMockingBird/venv/bin. Write in the following content: @Code chmod x pythonM1 Set the file as executable. If using PyCharm IDE, configure project interpreter to pythonM1(steps here @link ), if using command line python, run /PathToMockingBird/venv/bin/pythonM1 demo_toolbox.py.",2
3868,393571599,3,Python,"2. Prepare your models. Note that we are using the pretrained encoder/vocoder but not synthesizer, since the original model is incompatible with the Chinese symbols. It means the demo_cli is not working at this moment, so additional synthesizer models are required. You can either train your models or use existing ones:",2
3869,393571599,4,Python,"2.1 Train encoder with your dataset (Optional). Preprocess with the audios and the mel spectrograms: python encoder_preprocess.py  Allowing parameter --dataset {dataset} to support the datasets you want to preprocess. Only the train set of these datasets will be used. Possible names: librispeech_other, voxceleb1, voxceleb2. Use comma to sperate multiple datasets. Train the encoder: python encoder_train.py my_run /SV2TTS/encoder For training, the encoder uses visdom. You can disable it with --no_visdom, but it's nice to have. Run 'visdom' in a separate CLI/process to start your visdom server.",2
3870,393571599,4,Python,"2.2 Train synthesizer with your dataset. Download dataset and unzip: make sure you can access all .wav in folder Preprocess with the audios and the mel spectrograms: python pre.py Allowing parameter --dataset {dataset} to support aidatatang_200zh, magicdata, aishell3, data_aishell, etc.If this parameter is not passed, the default dataset will be aidatatang_200zh. Train the synthesizer: python train.py --typesynth mandarin /SV2TTS/synthesizer Go to next step when you see attention line show and loss meet your need in training folder synthesizer/saved_models/.",2
3871,393571599,4,Python,"2.3 Use pretrained model of synthesizer. Thanks to the community, some models will be shared:",2
3872,393571599,4,Python,"2.4 Train vocoder (Optional). note: vocoder has little difference in effect, so you may not need to train a new one. Preprocess the data: python vocoder_preprocess.py  -m replace with your dataset root，replace with directory of your best trained models of sythensizer, e.g. sythensizer\saved_mode\xxx Train the wavernn vocoder: python vocoder_train.py mandarin Train the hifigan vocoder python vocoder_train.py mandarin  hifigan",2
3873,393571599,4,Python,"3.1 Using the web server. You can then try to run:python web.py and open it in browser, default as  @link",2
3874,393571599,4,Python,3.2 Using the Toolbox. You can then try the toolbox: python demo_toolbox.py -d,2
3875,393571599,4,Python,3.3 Using the command line. You can then try the command: python gen_voice.py  your_wav_file.wav you may need to install cn2an by 'pip install cn2an' for better digital number result.,2
3876,393571599,2,Python,Reference. This repository is forked from Real-Time-Voice-Cloning @link  which only support English.,3
3877,393571599,4,Python,"1.Where can I download the dataset?. After unzip aidatatang_200zh, you need to unzip all the files under aidatatang_200zh\corpus\train",2
3878,393571599,4,Python,"2.What is?. If the dataset path is D:\data\aidatatang_200zh,then  isD:\data",2
3879,393571599,4,Python,3.Not enough VRAM. Train the synthesizer：adjust the batch_size in synthesizer/hparams.py @Code Train Vocoder-Preprocess the data：adjust the batch_size in synthesizer/hparams.py @Code Train Vocoder-Train the vocoder：adjust the batch_size in vocoder/wavernn/hparams.py @Code,2
3880,393571599,4,Python,"4.If it happens RuntimeError: Error(s) in loading state_dict for Tacotron: size mismatch for encoder.embedding.weight: copying a param with shape torch.Size(70, 512) from checkpoint, the shape in current model is torch.Size(75, 512).. Please refer to issue 37 @link .",2
3881,393571599,4,Python,5. How to improve CPU and GPU occupancy rate?. Adjust the batch_size as appropriate to improve,2
3882,393571599,4,Python,"6. What if it happens the page file is too small to complete the operation. Please refer to this video @link  and change the virtual memory to 100G (102400), for example : When the file is placed in the D disk, the virtual memory of the D disk is changed.",2
3883,393571599,4,Python,"7. When should I stop during training?. FYI, my attention came after 18k steps and loss became lower than 0.4 after 50k steps.",2
3884,40416236,1,Python,Big List of Naughty Strings. The Big List of Naughty Strings is an evolving list of strings which have a high probability of causing issues when used as user-input data. This is intended for use in helping both automated and manual QA testing; useful for whenever your QA engineer walks into a bar @link .,1
3885,40416236,2,Python,"Why Test Naughty Strings?. Even multi-billion dollar companies with huge amounts of automated testing can't find every bad input. For example, look at what happens when you try to Tweet a zero-width space @link  (U200B) on Twitter: ! @link Although this is not a malicious error, and typical users aren't Tweeting weird unicode, an 'internal server error' for unexpected input is never a positive experience for the user, and may in fact be a symptom of deeper string-validation issues. The Big List of Naughty Strings is intended to help reveal such issues.",1
3886,40416236,2,Python,Usage. blns.txt consists of newline-delimited strings and comments which are preceded with blns.json file is provided containing an array with all the comments stripped out (the scripts folder contains a Python script used to generate the blns.json)..,2
3887,40416236,2,Python,"Contributions. Feel free to send a pull request to add more strings, or additional sections. However, please do not send pull requests with very-long strings (255 characters), as that makes the list much more difficult to view. Likewise, please do not send pull requests which compromise manual usability of the file. This includes the EICAR test string @link , which can cause the file to be flagged by antivirus scanners, and files which alter the encoding of blns.txt. Also, do not send a null character (U0000) string, as it changes the file format on GitHub to binary @link  and renders it unreadable in pull requests. Finally, when adding or removing a string please update all files when you perform a pull request.",3
3888,40416236,2,Python,"Disclaimer. The Big List of Naughty Strings is intended to be used for software you own and manage. Some of the Naughty Strings can indicate security vulnerabilities, and as a result using such strings with third-party software may be a crime. The maintainer is not responsible for any negative actions that result from the use of the list. Additionally, the Big List of Naughty Strings is not a fully-comprehensive substitute for formal security/penetration testing for your service.",3
3889,40416236,2,Python,"Library / Packages. Various implementations of the Big List of Naughty Strings have made it to various package managers.  Those are maintained by outside parties, but can be found here: Please open a PR to list others.",3
3890,40416236,2,Python,Maintainer/Creator. Max Woolf (@minimaxir @link ),3
3891,40416236,2,Python,"Social Media Discussions. June 10, 2015 Hacker News: Show HN: Big List of Naughty Strings for testing user-input data @link August 17, 2015 Reddit: Big list of naughty strings. @link February 9, 2016 Reddit: Big List of Naughty Strings @link January 15, 2017 Hacker News: Naughty Strings: A list of strings likely to cause issues as user-input data @link January 16, 2017 Reddit: Naughty Strings: A list of strings likely to cause issues as user-input data @link November 16, 2018 Hacker News: Big List of Naughty Strings @link November 16, 2018 Reddit: Naughty Strings - A list of strings which have a high probability of causing issues when used as user-input data @link",3
3892,4086616,3,Python,"Install. Debian / Ubuntu: apt-get install python-pip pip install git @link CentOS: yum install python-setuptools && easy_install pip pip install git @link For CentOS 7, if you need AEAD ciphers, you need install libsodium @Code Linux distributions with snap @link : snap install shadowsocks Windows: See Install Shadowsocks Server on Windows @link .",2
3893,4086616,3,Python,"Usage. ssserver -p 443 -k password -m aes-256-cfb To run in the background: sudo ssserver -p 443 -k password -m aes-256-cfb --user nobody -d start To stop: sudo ssserver -d stop To check the log: sudo less /var/log/shadowsocks.log Check all the options via -h. You can also use a Configuration file instead. If you installed the snap @link  package, you have to prefix the commands with shadowsocks., like this: shadowsocks.ssserver -p 443 -k password -m aes-256-cfb",2
3894,4086616,3,Python,Usage with Config File. Create configuration file and run @link To start: ssserver -c /etc/shadowsocks.json Documentation ------------- You can find all the documentation in the Wiki @link . License ------- Apache License Build Status:       @link PyPI:               @link PyPI version:       @link Travis CI:          @link,2
3895,4578002,5,Python,"Docstrings. Add module level description in form of a docstring with links to corresponding references or other useful information. Add 'Examples in Python ecosystem' section if you know some. It shows how patterns could be applied to real-world problems. facade.py(patterns/structural/facade.py) has a good example of detailed description, but sometimes the shorter one as in template.py(patterns/behavioral/template.py) would suffice.",3
3896,4578002,5,Python,Python 2 compatibility. To see Python 2 compatible versions of some patterns please check-out the legacy @link  tag.,1
3897,4578002,5,Python,Update README. When everything else is done - update corresponding part of README.,3
3898,4578002,5,Python,Travis CI. Please run the following before submitting a patch - black . This lints your code. Then either: - tox or tox -e ci37 This runs unit tests. see tox.ini for further details. - If you have a bash compatible shell use ./lint.sh This script will lint and test your code. This script mirrors the CI pipeline actions. You can also run flake8 or pytest commands manually. Examples can be found in tox.ini.,3
3899,4578002,2,Python,"Contributing via issue triage  @link . You can triage issues and pull requests which may include reproducing bug reports or asking for vital information, such as version numbers or reproduction instructions. If you would like to start triaging issues, one easy way to get started is to subscribe to python-patterns on CodeTriage @link .",3
3900,4793392,1,Python,"sqlmap ! @link . @link   @link   @link   @link sqlmap is an open source penetration testing tool that automates the process of detecting and exploiting SQL injection flaws and taking over of database servers. It comes with a powerful detection engine, many niche features for the ultimate penetration tester, and a broad range of switches including database fingerprinting, over data fetching from the database, accessing the underlying file system, and executing commands on the operating system via out-of-band connections. Screenshots ---- You can visit the collection of screenshots @link  demonstrating some of the features on the wiki. Installation ---- You can download the latest tarball by clicking here @link  or latest zipball by clicking here @link . Preferably, you can download sqlmap by cloning the Git @link  repository: git clone --depth 1  @link sqlmap-dev sqlmap works out of the box with Python @link  version 2.6, 2.7 and 3.x on any platform. Usage ---- To get a list of basic options and switches use: python sqlmap.py -h To get a list of all options and switches use: python sqlmap.py -hh You can find a sample run here @link . To get an overview of sqlmap capabilities, a list of supported features, and a description of all options and switches, along with examples, you are advised to consult the user's manual @link . Links ---- Homepage:  @link  Download: .tar.gz @link  or .zip @link Commits RSS feed:  @link  Issue tracker:  @link  User's manual:  @link  Frequently Asked Questions (FAQ):  @link  X: @sqlmap @link Demos:  @link @link Screenshots:  @link Translations ---- Bulgarian @link Chinese @link Croatian @link Dutch @link French @link Georgian @link German @link Greek @link Hindi @link Indonesian @link Italian @link Japanese @link Korean @link Persian @link Polish @link Portuguese @link Russian @link Serbian @link Slovak @link Spanish @link Turkish @link Ukrainian @link Vietnamese @link",1
3901,51117837,1,Python,"Welcome to the Model Garden for TensorFlow. The TensorFlow Model Garden is a repository with a number of different implementations of state-of-the-art (SOTA) models and modeling solutions for TensorFlow users. We aim to demonstrate the best practices for modeling so that TensorFlow users can take full advantage of TensorFlow for their research and product development. To improve the transparency and reproducibility of our models, training logs on TensorBoard.dev @link  are also provided for models to the extent possible though not all models are suitable.",1
3902,51117837,2,Python,"Installation. To install the current release of tensorflow-models, please follow any one of the methods described below.",2
3903,51117837,4,Python,"Method 1: Install the TensorFlow Model Garden pip package. tf-models-official is the stable Model Garden package. Please check out the releases @link  to see what are available modules. pip3 will install all models and dependencies automatically. @Code Please check out our examples: - basic library import @link - nlp model building @link to learn how to use a PIP package. Note that tf-models-official may not include the latest changes in the master branch of this github repo. To include latest changes, you may install tf-models-nightly, which is the nightly Model Garden package created daily automatically. @Code",2
3904,51117837,4,Python,"Method 2: Clone the source. 1. Clone the GitHub repository: @Code 2. Add the top-level /models folder to the Python path. @Code If you are using in a Windows environment, you may need to use the following command with PowerShell: @Code If you are using a Colab notebook, please set the Python path with os.environ. @Code 3. Install other dependencies @Code Finally, if you are using nlp packages, please also install tensorflow-text-nightly: @Code",2
3905,51117837,2,Python,Announcements. Please check this page @link  for recent announcements.,3
3906,51117837,2,Python,"Contributions. @link If you want to contribute, please review the contribution guidelines @link .",3
3907,51117837,2,Python,License. Apache License 2.0(LICENSE),3
3908,51117837,2,Python,"Citing TensorFlow Model Garden. If you use TensorFlow Model Garden in your research, please cite this repository. @Code",3
3909,519832,1,Python,"mitmproxy. @link @link @link @link @link @link mitmproxy is an interactive, SSL/TLS-capable intercepting proxy with a console interface for HTTP/1, HTTP/2, and WebSockets. mitmdump is the command-line version of mitmproxy. Think tcpdump for HTTP. mitmweb is a web-based interface for mitmproxy.",1
3910,519832,2,Python,"Installation. The installation instructions are here @link . If you want to install from source, see CONTRIBUTING.md @link .",2
3911,519832,2,Python,"Documentation & Help. General information, tutorials, and precompiled binaries can be found on the mitmproxy website. @link The documentation for mitmproxy is available on our website: @link @link If you have questions on how to use mitmproxy, please use GitHub Discussions! @link",3
3912,519832,2,Python,"Contributing. As an open source project, mitmproxy welcomes contributions of all forms. @link",3
3913,527591471,1,Python,"Stable Diffusion web UI . A web interface for Stable Diffusion, implemented using Gradio library. !(screenshot.png)",1
3914,527591471,2,Python,"Features . Detailed feature showcase with images @link : - Original txt2img and img2img modes - One click install and run script (but you still must install python and git) - Outpainting - Inpainting - Color Sketch - Prompt Matrix - Stable Diffusion Upscale - Attention, specify parts of text that the model should pay more attention to - a man in a ((tuxedo)) - will pay more attention to tuxedo - a man in a (tuxedo:1.21) - alternative syntax - select text and press CtrlUp or CtrlDown (or CommandUp or CommandDown if you're on a MacOS) to automatically adjust attention to selected text (code contributed by anonymous user) - Loopback, run img2img processing multiple times - X/Y/Z plot, a way to draw a 3 dimensional plot of images with different parameters - Textual Inversion - have as many embeddings as you want and use any names you like for them - use multiple embeddings with different numbers of vectors per token - works with half precision floating point numbers - train embeddings on 8GB (also reports of 6GB working) - Extras tab with: - GFPGAN, neural network that fixes faces - CodeFormer, face restoration tool as an alternative to GFPGAN - RealESRGAN, neural network upscaler - ESRGAN, neural network upscaler with a lot of third party models - SwinIR and Swin2SR (see here @link ), neural network upscalers - LDSR, Latent diffusion super resolution upscaling - Resizing aspect ratio options - Sampling method selection - Adjust sampler eta values (noise multiplier) - More advanced noise setting options - Interrupt processing at any time - 4GB video card support (also reports of 2GB working) - Correct seeds for batches - Live prompt token length validation - Generation parameters - parameters you used to generate images are saved with that image - in PNG chunks for PNG, in EXIF for JPEG - can drag the image to PNG info tab to restore generation parameters and automatically copy them into UI - can be disabled in settings - drag and drop an image/text-parameters to promptbox - Read Generation Parameters Button, loads parameters in promptbox to UI - Settings page - Running arbitrary python code from UI (must run with --allow-code to enable) - Mouseover hints for most UI elements - Possible to change defaults/mix/max/step values for UI elements via text config - Tiling support, a checkbox to create images that can be tiled like textures - Progress bar and live image generation preview - Can use a separate neural network to produce previews with almost none VRAM or compute requirement - Negative prompt, an extra text field that allows you to list what you don't want to see in generated image - Styles, a way to save part of prompt and easily apply them via dropdown later - Variations, a way to generate same image but with tiny differences - Seed resizing, a way to generate same image but at slightly different resolution - CLIP interrogator, a button that tries to guess prompt from an image - Prompt Editing, a way to change prompt mid-generation, say to start making a watermelon and switch to anime girl midway - Batch Processing, process a group of files using img2img - Img2img Alternative, reverse Euler method of cross attention control - Highres Fix, a convenience option to produce high resolution pictures in one click without usual distortions - Reloading checkpoints on the fly - Checkpoint Merger, a tab that allows you to merge up to 3 checkpoints into one - Custom scripts @link  with many extensions from community - Composable-Diffusion @link , a way to use multiple prompts at once - separate prompts using uppercase AND - also supports weights for prompts: a cat :1.2 AND a dog AND a penguin :2.2 - No token limit for prompts (original stable diffusion lets you use up to 75 tokens) - DeepDanbooru integration, creates danbooru style tags for anime prompts - xformers @link , major speed increase for select cards: (add --xformers to commandline args) - via extension: History tab @link : view, direct and delete images conveniently within the UI - Generate forever option - Training tab - hypernetworks and embeddings options - Preprocessing images: cropping, mirroring, autotagging using BLIP or deepdanbooru (for anime) - Clip skip - Hypernetworks - Loras (same as Hypernetworks but more pretty) - A separate UI where you can choose, with preview, which embeddings, hypernetworks or Loras to add to your prompt - Can select to load a different VAE from settings screen - Estimated completion time in progress bar - API - Support for dedicated inpainting model @link  by RunwayML . - via extension: Aesthetic Gradients @link , a way to generate images with a specific aesthetic by using clip images embeds (implementation of  @link @link ) - Stable Diffusion 2.0 @link  support - see wiki @link  for instructions . - Alt-Diffusion @link  support - see wiki @link  for instructions . - Now without any bad letters! - Load checkpoints in safetensors format - Eased resolution restriction: generated image's dimensions must be a multiple of 8 rather than 64 - Now with a license! - Reorder elements in the UI from settings screen - Segmind Stable Diffusion @link  support",1
3915,527591471,2,Python,"Installation and Running . Make sure the required dependencies @link  are met and follow the instructions available for: - NVidia @link  (recommended) - AMD @link  GPUs. - Intel CPUs, Intel GPUs (both integrated and discrete) @link  (external wiki page) - Ascend NPUs @link  (external wiki page) Alternatively, use online services (like Google Colab): - List of Online Services @link",2
3916,527591471,3,Python,Installation on Windows 10/11 with NVidia-GPUs using release package . 1. Download sd.webui.zip from v1.0.0-pre @link  and extract its contents. 2. Run update.bat. 3. Run run.bat. For more details see Install-and-Run-on-NVidia-GPUs @link,2
3917,527591471,3,Python,"Automatic Installation on Windows . 1. Install Python 3.10.6 @link  (Newer version of Python does not support torch), checking 'Add Python to PATH'. 2. Install git @link . 3. Download the stable-diffusion-webui repository, for example by running git clone  @link 4. Run webui-user.bat from Windows Explorer as normal, non-administrator, user.",2
3918,527591471,3,Python,Automatic Installation on Linux . 1. Install the dependencies: @Code 2. Navigate to the directory you would like the webui to be installed and execute the following command: @Code 3. Run webui.sh. 4. Check webui-user.sh for options.,2
3919,527591471,3,Python,Installation on Apple Silicon . Find the instructions here @link .,2
3920,527591471,2,Python,Contributing . Here's how to add code to this repo: Contributing @link,3
3921,527591471,2,Python,"Documentation . The documentation was moved from this README over to the project's wiki @link . For the purposes of getting Google and other search engines to crawl the wiki, here's a link to the (not for humans) crawlable wiki @link .",3
3922,527591471,2,Python,"Credits . Licenses for borrowed code can be found in Settings - Licenses screen, and also in html/licenses.html file. - Stable Diffusion -  @link  @link - k-diffusion -  @link - Spandrel -  @link implementing - GFPGAN -  @link - CodeFormer -  @link - ESRGAN -  @link - SwinIR -  @link - Swin2SR -  @link - LDSR -  @link - MiDaS -  @link - Ideas for optimizations -  @link - Cross Attention layer optimization - Doggettx -  @link original idea for prompt editing. - Cross Attention layer optimization - InvokeAI, lstein -  @link (originally  @link - Sub-quadratic Cross Attention layer optimization - Alex Birch  @link , Amin Rezaei  @link - Textual Inversion - Rinon Gal -  @link (we're not using his code, but we are using his ideas). - Idea for SD upscale -  @link - Noise generation for outpainting mk2 -  @link - CLIP interrogator idea and borrowing some code -  @link - Idea for Composable Diffusion -  @link - xformers -  @link - DeepDanbooru - interrogator for anime diffusers  @link - Sampling in float32 precision from a float16 UNet - marunine for the idea, Birch-san for the example Diffusers implementation  @link - Instruct pix2pix - Tim Brooks (star), Aleksander Holynski (star), Alexei A. Efros (no star) -  @link - Security advice - RyotaK - UniPC sampler - Wenliang Zhao -  @link - TAESD - Ollin Boer Bohan -  @link - LyCORIS - KohakuBlueleaf - Restart sampling - lambertae -  @link - Hypertile - tfernd -  @link - Initial Gradio script - posted on 4chan by an Anonymous user. Thank you Anonymous user. - (You)",3
3923,537603333,1,Python,"Whisper. Blog @link Paper @link Model card @link Colab example @link Whisper is a general-purpose speech recognition model. It is trained on a large dataset of diverse audio and is also a multitasking model that can perform multilingual speech recognition, speech translation, and language identification.",1
3924,537603333,2,Python,"Approach. A Transformer sequence-to-sequence model is trained on various speech processing tasks, including multilingual speech recognition, speech translation, spoken language identification, and voice activity detection. These tasks are jointly represented as a sequence of tokens to be predicted by the decoder, allowing a single model to replace many stages of a traditional speech-processing pipeline. The multitask training format uses a set of special tokens that serve as task specifiers or classification targets.",1
3925,537603333,2,Python,"Setup. We used Python 3.9.9 and PyTorch @link  1.10.1 to train and test our models, but the codebase is expected to be compatible with Python 3.8-3.11 and recent PyTorch versions. The codebase also depends on a few Python packages, most notably OpenAI's tiktoken @link  for their fast tokenizer implementation. You can download and install (or update to) the latest release of Whisper with the following command: pip install -U openai-whisper Alternatively, the following command will pull and install the latest commit from this repository, along with its Python dependencies: pip install git @link To update the package to the latest version of this repository, please run: pip install --upgrade --no-deps --force-reinstall git @link It also requires the command-line tool ffmpeg @link  to be installed on your system, which is available from most package managers: @Code You may need rust @link  installed as well, in case tiktoken @link  does not provide a pre-built wheel for your platform. If you see installation errors during the pip install command above, please follow the Getting started page @link  to install Rust development environment. Additionally, you may need to configure the PATH environment variable, e.g. export PATH'$HOME/.cargo/bin:$PATH'. If the installation fails with No module named 'setuptools_rust', you need to install setuptools_rust, e.g. by running: @Code",2
3926,537603333,2,Python,"Available models and languages. There are five model sizes, four with English-only versions, offering speed and accuracy tradeoffs. Below are the names of the available models and their approximate memory requirements and inference speed relative to the large model; actual speed may vary depending on many factors including the available hardware. The .en models for English-only applications tend to perform better, especially for the tiny.en and base.en models. We observed that the difference becomes less significant for the small.en and medium.en models. Whisper's performance varies widely depending on the language. The figure below shows a performance breakdown of large-v3 and large-v2 models by language, using WERs (word error rates) or CER (character error rates, shown in Italic) evaluated on the Common Voice 15 and Fleurs datasets. Additional WER/CER metrics corresponding to the other models and datasets can be found in Appendix D.1, D.2, and D.4 of the paper @link , as well as the BLEU (Bilingual Evaluation Understudy) scores for translation in Appendix D.3.",1
3927,537603333,2,Python,"Command-line usage. The following command will transcribe speech in audio files, using the medium model: whisper audio.flac audio.mp3 audio.wav --model medium The default setting (which selects the small model) works well for transcribing English. To transcribe an audio file containing non-English speech, you can specify the language using the --language option: whisper japanese.wav --language Japanese Adding --task translate will translate the speech into English: whisper japanese.wav --language Japanese --task translate Run the following to view all available options: whisper --help See tokenizer.py @link  for the list of all available languages.",2
3928,537603333,2,Python,"Python usage. Transcription can also be performed within Python: @Code Internally, the transcribe() method reads the entire file and processes the audio with a sliding 30-second window, performing autoregressive sequence-to-sequence predictions on each window. Below is an example usage of whisper.detect_language() and whisper.decode() which provide lower-level access to the model. @Code",2
3929,537603333,2,Python,"More examples. Please use the  Show and tell @link  category in Discussions for sharing more example usages of Whisper and third-party extensions such as web demos, integrations with other tools, ports for different platforms, etc.",3
3930,537603333,2,Python,License. Whisper's code and model weights are released under the MIT License. See LICENSE @link  for further details.,3
3931,54346799,1,Python,Try Public APIs for free. Explore popular APIs and see them work in Postman. APILayer @link  is the fastest way to integrate APIs into any product. They created this repository to support the community in easily finding public APIs. Explore their collections on the Postman API Network @link .,2
3932,54346799,2,Python,Popular categories. Animals(animals). Anime(anime). Art & Design(art--design). Authentication & Authorization(authentication--authorization). Blockchain(blockchain). Business(business). Currency Exchange(currency-exchange). Social(social).,1
3933,54346799,2,Python,Learn more about Public APIs. Get Involved Contributing Guide(CONTRIBUTING.md) API for this project @link Issues @link Pull Requests @link LICENSE(LICENSE) Own an API? Publish your own Run in Postman @link  button. More Resources Postman API Network @link Free APIs @link Dev Resources @link Apihouse @link Collective APIs @link,3
3934,54346799,2,Python,Index. Animals(animals). Anime(anime). Anti-Malware(anti-malware). Art & Design(art--design). Authentication & Authorization(authentication--authorization). Blockchain(blockchain). Books(books). Business(business). Calendar(calendar). Cloud Storage & File Sharing(cloud-storage--file-sharing). Continuous Integration(continuous-integration). Cryptocurrency(cryptocurrency). Currency Exchange(currency-exchange). Data Validation(data-validation). Development(development). Dictionaries(dictionaries). Documents & Productivity(documents--productivity). Email(email). Entertainment(entertainment). Environment(environment). Events(events). Finance(finance). Food & Drink(food--drink). Games & Comics(games--comics). Geocoding(geocoding). Government(government). Health(health). Jobs(jobs). Machine Learning(machine-learning). Music(music). News(news). Open Data(open-data). Open Source Projects(open-source-projects). Patent(patent). Personality(personality). Phone(phone). Photography(photography). Programming(programming). Science & Math(science--math). Security(security). Shopping(shopping). Social(social). Sports & Fitness(sports--fitness). Test Data(test-data). Text Analysis(text-analysis). Tracking(tracking). Transportation(transportation). URL Shorteners(url-shorteners). Vehicle(vehicle). Video(video). Weather(weather).,1
3935,54346799,3,Python,Animals. API Back to Index(index).,1
3936,54346799,3,Python,Anime. API Back to Index(index).,1
3937,54346799,3,Python,Anti-Malware. API Back to Index(index).,1
3938,54346799,3,Python,Art & Design. API Back to Index(index).,1
3939,54346799,3,Python,Authentication & Authorization. API Back to Index(index).,1
3940,54346799,3,Python,Blockchain. Back to Index(index).,1
3941,54346799,3,Python,Books. API Back to Index(index).,1
3942,54346799,3,Python,Business. API Back to Index(index).,1
3943,54346799,3,Python,Calendar. API Back to Index(index).,1
3944,54346799,3,Python,Cloud Storage & File Sharing. API Back to Index(index).,1
3945,54346799,3,Python,Continuous Integration. API Back to Index(index).,1
3946,54346799,3,Python,Cryptocurrency. API Back to Index(index).,1
3947,54346799,3,Python,Currency Exchange. API Back to Index(index).,1
3948,54346799,3,Python,Data Validation. API Back to Index(index).,1
3949,54346799,3,Python,Development. API Back to Index(index).,1
3950,54346799,3,Python,Dictionaries. API Back to Index(index).,1
3951,54346799,3,Python,Documents & Productivity. API Back to Index(index).,1
3952,54346799,3,Python,Email. API Back to Index(index).,1
3953,54346799,3,Python,Entertainment. API Back to Index(index).,1
3954,54346799,3,Python,Environment. API Back to Index(index).,1
3955,54346799,3,Python,Events. API Back to Index(index).,1
3956,54346799,3,Python,Finance. API Back to Index(index).,1
3957,54346799,3,Python,Food & Drink. API Back to Index(index).,1
3958,54346799,3,Python,Games & Comics. API Back to Index(index).,1
3959,54346799,3,Python,Geocoding. API Back to Index(index).,1
3960,54346799,3,Python,Government. API Back to Index(index).,1
3961,54346799,3,Python,Health. API Back to Index(index).,1
3962,54346799,3,Python,Jobs. API Back to Index(index).,1
3963,54346799,3,Python,Machine Learning. API Back to Index(index).,1
3964,54346799,3,Python,Music. API Back to Index(index).,1
3965,54346799,3,Python,News. API Back to Index(index).,1
3966,54346799,3,Python,Open Data. API Back to Index(index).,1
3967,54346799,3,Python,Open Source Projects. API Back to Index(index).,1
3968,54346799,3,Python,Patent. API Back to Index(index).,1
3969,54346799,3,Python,Personality. API Back to Index(index).,1
3970,54346799,3,Python,Phone. API Back to Index(index).,1
3971,54346799,3,Python,Photography. API Back to Index(index).,1
3972,54346799,3,Python,Programming. API Back to Index(index).,1
3973,54346799,3,Python,Science & Math. API Back to Index(index).,1
3974,54346799,3,Python,Security. API Back to Index(index).,1
3975,54346799,3,Python,Shopping. API Back to Index(index).,1
3976,54346799,3,Python,Social. API Back to Index(index).,1
3977,54346799,3,Python,Sports & Fitness. API Back to Index(index).,1
3978,54346799,3,Python,Test Data. API Back to Index(index).,1
3979,54346799,3,Python,Text Analysis. API Back to Index(index).,1
3980,54346799,3,Python,Tracking. API Back to Index(index).,1
3981,54346799,3,Python,Transportation. API Back to Index(index).,1
3982,54346799,3,Python,URL Shorteners. API Back to Index(index).,1
3983,54346799,3,Python,Vehicle. API Back to Index(index).,1
3984,54346799,3,Python,Video. API Back to Index(index).,1
3985,54346799,3,Python,Weather. API Back to Index(index).,1
3986,54346799,2,Python,License. MIT(LICENSE) (c) 2022 public-apis,3
3987,5483330,1,Python,"You-Get. @link @link @link NOTICE (30 May 2022): Support for Python 3.5, 3.6 and 3.7 will eventually be dropped. (see details here @link )) NOTICE (8 Mar 2019): Read this @link  if you are looking for the conventional 'Issues' tab. --- You-Get @link  is a tiny command-line utility to download media contents (videos, audios, images) from the Web, in case there is no other handy way to do it. Here's how you use you-get to download a video from YouTube @link : @Code And here's why you might want to use it: You enjoyed something on the Internet, and just want to download them for your own pleasure. You watch your favorite videos online from your computer, but you are prohibited from saving them. You feel that you have no control over your own computer. (And it's not how an open Web is supposed to work.) You want to get rid of any closed-source technology or proprietary JavaScript code, and disallow things like Flash running on your computer. You are an adherent of hacker culture and free software. What you-get can do for you: Download videos / audios from popular websites such as YouTube, Youku, Niconico, and a bunch more. (See the full list of supported sites(supported-sites)). Stream an online video in your media player. No web browser, no more ads. Download images (of interest) by scraping a web page. Download arbitrary non-HTML contents, i.e., binary files. Interested? Install it(installation) now and get started by examples(getting-started).. Are you a Python programmer? Then check out the source @link  and fork it! ! @link",1
3988,5483330,3,Python,Prerequisites. The following dependencies are recommended: Python @link   3.7.4 or above FFmpeg @link  1.0 or above (Optional) RTMPDump @link,2
3989,5483330,3,Python,"Option 1: Install via pip. The official release of you-get is distributed on PyPI @link , and can be installed easily from a PyPI mirror via the pip @link ) package manager: (Note that you must use the Python 3 version of pip) $ pip install you-get",2
3990,5483330,3,Python,Option 2: Install via Antigen @link  (for Zsh users). Add the following line to your .zshrc: antigen bundle soimort/you-get,2
3991,5483330,3,Python,"Option 3: Download from GitHub. You may either download the stable @link  (identical with the latest release on PyPI) or the develop @link  (more hotfixes, unstable features) branch of you-get. Unzip it, and put the directory containing the you-get script into your PATH. Alternatively, run @Code Or @Code to install you-get to a permanent path. (And don't omit the dot . representing the current directory) You can also use the pipenv @link  to install the you-get in the Python virtual environment. @Code",2
3992,5483330,3,Python,"Option 4: Git clone. This is the recommended way for all developers, even if you don't often code in Python. @Code Then put the cloned directory into your PATH, or run python -m pip install path/to/you-get to install you-get to a permanent path.",2
3993,5483330,3,Python,Option 5: Homebrew (Mac only). You can install you-get easily via: @Code,2
3994,5483330,3,Python,Option 6: pkg (FreeBSD only). You can install you-get easily via: @Code,2
3995,5483330,3,Python,"Shell completion. Completion definitions for Bash, Fish and Zsh can be found in contrib/completion @link . Please consult your shell's manual for how to take advantage of them.",2
3996,5483330,2,Python,"Upgrading. Based on which option you chose to install you-get, you may upgrade it via: @Code or download the latest release via: @Code In order to get the latest  @Code  branch without messing up the PIP, you can try: @Code",2
3997,5483330,3,Python,"Download a video. When you get a video of interest, you might want to use the --info/-i option to see all available quality and formats: @Code By default, the one on the top is the one you will get. If that looks cool to you, download it: @Code (If a YouTube video has any closed captions, they will be downloaded together with the video file, in SubRip subtitle format.) Or, if you prefer another format (mp4), just use whatever the option you-get shows to you: @Code Note: At this point, format selection has not been generally implemented for most of our supported sites; in that case, the default format to download is the one with the highest quality. ffmpeg is a required dependency, for downloading and joining videos streamed in multiple parts (e.g. on some sites like Youku), and for YouTube videos of 1080p or high resolution. If you don't want you-get to join video parts after downloading them, use the --no-merge/-n option.",2
3998,5483330,3,Python,"Download anything else. If you already have the URL of the exact resource you want, you can download it directly with: @Code Otherwise, you-get will scrape the web page and try to figure out if there's anything interesting to you: @Code Note: This feature is an experimental one and far from perfect. It works best on scraping large-sized images from popular websites like Tumblr and Blogger, but there is really no universal pattern that can apply to any site on the Internet.",2
3999,5483330,3,Python,"Search on Google Videos and download. You can pass literally anything to you-get. If it isn't a valid URL, you-get will do a Google search and download the most relevant video for you. (It might not be exactly the thing you wish to see, but still very likely.) @Code",2
4000,5483330,3,Python,"Pause and resume a download. You may use CtrlC to interrupt a download. A temporary .download file is kept in the output directory. Next time you run you-get with the same arguments, the download progress will resume from the last session. In case the file is completely downloaded (the temporary .download extension is gone), you-get will just skip the download. To enforce re-downloading, use the --force/-f option. (Warning: doing so will overwrite any existing file or temporary file with the same name!)",2
4001,5483330,3,Python,"Set the path and name of downloaded file. Use the --output-dir/-o option to set the path, and --output-filename/-O to set the name of the downloaded file: @Code Tips: These options are helpful if you encounter problems with the default video titles, which may contain special characters that do not play well with your current shell / operating system / filesystem. These options are also helpful if you write a script to batch download files and put them into designated folders with designated names.",2
4002,5483330,3,Python,"Proxy settings. You may specify an HTTP proxy for you-get to use, via the --http-proxy/-x option: @Code However, the system proxy setting (i.e. the environment variable http_proxy) is applied by default. To disable any proxy, use the --no-proxy option. Tips: If you need to use proxies a lot (in case your network is blocking certain sites), you might want to use you-get with proxychains @link  and set alias you-get'proxychains -q you-get' (in Bash). For some websites (e.g. Youku), if you need access to some videos that are only available in mainland China, there is an option of using a specific proxy to extract video information from the site: --extractor-proxy/-y.",2
4003,5483330,3,Python,"Watch a video. Use the --player/-p option to feed the video into your media player of choice, e.g. mpv or vlc, instead of downloading it: @Code Or, if you prefer to watch the video in a browser, just without ads or comment section: @Code Tips: It is possible to use the -p option to start another download manager, e.g., you-get -p uget-gtk ' @link though they may not play together very well.",2
4004,5483330,3,Python,"Load cookies. Not all videos are publicly available to anyone. If you need to log in your account to access something (e.g., a private video), it would be unavoidable to feed the browser cookies to you-get via the --cookies/-c option. Note: As of now, we are supporting two formats of browser cookies: Mozilla cookies.sqlite and Netscape cookies.txt.",2
4005,5483330,3,Python,"Reuse extracted data. Use --url/-u to get a list of downloadable resource URLs extracted from the page. Use --json to get an abstract of extracted data in the JSON format. Warning: For the time being, this feature has NOT been stabilized and the JSON schema may have breaking changes in the future.",2
4006,5483330,2,Python,"Supported Sites. For all other sites not on the list, the universal extractor will take care of finding and downloading interesting resources from the page.",2
4007,5483330,3,Python,"Known bugs. If something is broken and you-get can't get you things you want, don't panic. (Yes, this happens all the time!) Check if it's already a known problem on . If not, follow the guidelines on how to report an issue @link .",3
4008,5483330,2,Python,"Getting Involved. You can reach us on the Gitter channel soimort/you-get @link  (here's how you set up your IRC client @link  for Gitter). If you have a quick question regarding you-get, ask it there.. If you are seeking to report an issue or contribute, please make sure to read the guidelines @link  first.",3
4009,5483330,2,Python,"Legal Issues. This software is distributed under the MIT license @link . In particular, please be aware that THE SOFTWARE IS PROVIDED 'AS IS', WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE. Translated to human words: In case your use of the software forms the basis of copyright infringement, or you use the software for any other illegal purposes, the authors cannot take any responsibility for you. We only ship the code here, and how you are going to use it is left to your own discretion.",3
4010,5483330,2,Python,"Authors. Made by @soimort @link , who is in turn powered by :coffee:, :beer: and :ramen:. You can find the list of all contributors @link  here.",3
4011,552661142,1,Python,"LangChain. Build context-aware reasoning applications @link @link @link @link @link . @link @link @link @link @link Looking for the JS/TS library? Check out LangChain.js @link . To help you ship LangChain apps to production faster, check out LangSmith @link . LangSmith @link  is a unified developer platform for building, testing, and monitoring LLM applications. Fill out this form @link  to speak with our sales team.",1
4012,552661142,2,Python,Quick Install. With pip: @Code With conda: @Code,2
4013,552661142,2,Python,"What is LangChain?. LangChain is a framework for developing applications powered by large language models (LLMs). For these applications, LangChain simplifies the entire application lifecycle: - Open-source libraries: Build your applications using LangChain's modular building blocks @link  and components @link . Integrate with hundreds of third-party providers @link .. - Productionization: Inspect, monitor, and evaluate your apps with LangSmith @link  so that you can constantly optimize and deploy with confidence. - Deployment: Turn any chain into a REST API with LangServe @link .",1
4014,552661142,3,Python,"Open-source libraries. - langchain-core: Base abstractions and LangChain Expression Language. - langchain-community: Third party integrations. - Some integrations have been further split into partner packages that only rely on langchain-core. Examples include langchain_openai and langchain_anthropic. - langchain: Chains, agents, and retrieval strategies that make up an application's cognitive architecture. - LangGraph @link : A library for building robust and stateful multi-actor applications with LLMs by modeling steps as edges and nodes in a graph.",1
4015,552661142,3,Python,"Productionization:. - LangSmith @link : A developer platform that lets you debug, test, evaluate, and monitor chains built on any LLM framework and seamlessly integrates with LangChain.",1
4016,552661142,3,Python,Deployment:. - LangServe @link : A library for deploying LangChain chains as REST APIs.,1
4017,552661142,2,Python,What can you build with LangChain?. Question answering with RAG - Documentation @link - End-to-end Example: Chat LangChain @link  and repo @link Extracting structured output - Documentation @link - End-to-end Example: SQL Llama2 Template @link Chatbots - Documentation @link - End-to-end Example: Web LangChain (web researcher chatbot) @link  and repo @link And much more! Head to the Tutorials @link  section of the docs for more.,1
4018,552661142,2,Python,"How does LangChain help?. The main value props of the LangChain libraries are: 1. Components: composable building blocks, tools and integrations for working with language models. Components are modular and easy-to-use, whether you are using the rest of the LangChain framework or not 2. Off-the-shelf chains: built-in assemblages of components for accomplishing higher-level tasks Off-the-shelf chains make it easy to get started. Components make it easy to customize existing chains and build new ones.",1
4019,552661142,2,Python,"LangChain Expression Language (LCEL). LCEL is the foundation of many of LangChain's components, and is a declarative way to compose chains. LCEL was designed from day 1 to support putting prototypes in production, with no code changes, from the simplest “prompt  LLM” chain to the most complex chains. - Overview @link : LCEL and its benefits. - Interface @link : The standard Runnable interface for LCEL objects. - Primitives @link : More on the primitives LCEL includes. - Cheatsheet @link : Quick overview of the most common usage patterns",1
4020,552661142,2,Python,"Components. Components fall into the following modules: Model I/O This includes prompt management @link , prompt optimization @link , a generic interface for chat models @link  and LLMs @link , and common utilities for working with model outputs @link .. Retrieval Retrieval Augmented Generation involves loading data @link  from a variety of sources, preparing it @link , then searching over (a.k.a. retrieving from) @link  it for use in the generation step.. Agents Agents allow an LLM autonomy over how a task is accomplished. Agents make decisions about which Actions to take, then take that Action, observe the result, and repeat until the task is complete. LangChain provides a standard interface for agents @link  along with the LangGraph @link  extension for building custom agents..",1
4021,552661142,2,Python,"Documentation. Please see here @link  for full documentation, which includes: - Introduction @link : Overview of the framework and the structure of the docs. - Tutorials @link : If you're looking to build something specific or are more of a hands-on learner, check out our tutorials. This is the best place to get started. - How-to guides @link : Answers to “How do I….?” type questions. These guides are goal-oriented and concrete; they're meant to help you complete a specific task. - Conceptual guide @link : Conceptual explanations of the key parts of the framework. - API Reference @link : Thorough documentation of every class and method.",2
4022,552661142,2,Python,"Ecosystem. -  LangSmith @link : Tracing and evaluating your language model applications and intelligent agents to help you move from prototype to production. -  LangGraph @link : Creating stateful, multi-actor applications with LLMs, built on top of (and intended to be used with) LangChain primitives. -  LangServe @link : Deploying LangChain runnables and chains as REST APIs. - LangChain Templates @link : Example applications hosted with LangServe.",1
4023,552661142,2,Python,"Contributing. As an open-source project in a rapidly developing field, we are extremely open to contributions, whether it be in the form of a new feature, improved infrastructure, or better documentation. For detailed information on how to contribute, see here @link .",3
4024,560704231,1,Python,"LlamaIndex . @link @link @link @link LlamaIndex (GPT Index) is a data framework for your LLM application. Building with LlamaIndex typically involves working with LlamaIndex core and a chosen set of integrations (or plugins). There are two ways to start building with LlamaIndex in Python: 1. Starter: llama-index  @link . A starter Python package that includes core LlamaIndex as well as a selection of integrations. 2. Customized: llama-index-core  @link . Install core LlamaIndex and add your chosen LlamaIndex integration packages on LlamaHub @link that are required for your application. There are over 300 LlamaIndex integration packages that work seamlessly with core, allowing you to build with your preferred LLM, embedding, and vector store providers. The LlamaIndex Python library is namespaced such that import statements which include core imply that the core package is being used. In contrast, those statements without core imply that an integration package is being used. @Code",1
4025,560704231,3,Python,Important Links. LlamaIndex.TS (Typescript/Javascript):  @link Documentation:  @link Twitter:  @link Discord:  @link,2
4026,560704231,3,Python,Ecosystem. - LlamaHub (community library of data loaders):  @link - LlamaLab (cutting-edge AGI projects using LlamaIndex):  @link,3
4027,560704231,2,Python,Overview. NOTE: This README is not updated as frequently as the documentation. Please check out the documentation above for the latest updates!,2
4028,560704231,3,Python,Context. - LLMs are a phenomenal piece of technology for knowledge generation and reasoning. They are pre-trained on large amounts of publicly available data. - How do we best augment LLMs with our own private data? We need a comprehensive toolkit to help perform this data augmentation for LLMs.,1
4029,560704231,3,Python,"Proposed Solution. That's where LlamaIndex comes in. LlamaIndex is a 'data framework' to help you build LLM apps. It provides the following tools: - Offers data connectors to ingest your existing data sources and data formats (APIs, PDFs, docs, SQL, etc.). - Provides ways to structure your data (indices, graphs) so that this data can be easily used with LLMs. - Provides an advanced retrieval/query interface over your data: Feed in any LLM input prompt, get back retrieved context and knowledge-augmented output. - Allows easy integrations with your outer application framework (e.g. with LangChain, Flask, Docker, ChatGPT, anything else). LlamaIndex provides tools for both beginner users and advanced users. Our high-level API allows beginner users to use LlamaIndex to ingest and query their data in 5 lines of code. Our lower-level APIs allow advanced users to customize and extend any module (data connectors, indices, retrievers, query engines, reranking modules), to fit their needs.",1
4030,560704231,2,Python,Contributing. Interested in contributing? Contributions to LlamaIndex core as well as contributing integrations that build on the core are both accepted and highly encouraged! See our Contribution Guide(CONTRIBUTING.md) for more details.,3
4031,560704231,2,Python,"Documentation. Full documentation can be found here:  @link Please check it out for the most up-to-date tutorials, how-to guides, references, and other resources!",2
4032,560704231,2,Python,"Example Usage. @Code Examples are in the docs/examples folder. Indices are in the indices folder (see list of indices below). To build a simple vector store index using OpenAI: @Code To build a simple vector store index using non-OpenAI LLMs, e.g. Llama 2 hosted on Replicate @link , where you can easily create a free trial API token: @Code To query: @Code By default, data is stored in-memory. To persist to disk (under ./storage): @Code To reload from disk: @Code",2
4033,560704231,2,Python,"Dependencies. We use poetry as the package manager for all Python packages. As a result, the dependencies of each Python package can be found by referencing the pyproject.toml file in each of the package's folders. @Code",2
4034,560704231,2,Python,Citation. Reference to cite if you use LlamaIndex in a paper: @Code,3
4035,569927055,1,Python,Stable Diffusion Version 2. This repository contains Stable Diffusion @link  models trained from scratch and will be continuously updated with new checkpoints. The following list provides an overview of all currently available models. More coming soon.,1
4036,569927055,2,Python,"News. March 24, 2023 Stable UnCLIP 2.1 - New stable diffusion finetune (_Stable unCLIP 2.1_, Hugging Face @link ) at 768x768 resolution,  based on SD2.1-768. This model allows for image variations and mixing operations as described in Hierarchical Text-Conditional Image Generation with CLIP Latents @link , and, thanks to its modularity, can be combined with other models such as KARLO @link . Comes in two variants: Stable unCLIP-L @link  and Stable unCLIP-H @link , which are conditioned on CLIP ViT-L and ViT-H image embeddings, respectively. Instructions are available here(doc/UNCLIP.MD). - A public demo of SD-unCLIP is already available at clipdrop.co/stable-diffusion-reimagine @link December 7, 2022 Version 2.1 - New stable diffusion model (_Stable Diffusion 2.1-v_, Hugging Face @link ) at 768x768 resolution and (_Stable Diffusion 2.1-base_, HuggingFace @link ) at 512x512 resolution, both based on the same number of parameters and architecture as 2.0 and fine-tuned on 2.0, on a less restrictive NSFW filtering of the LAION-5B @link  dataset. Per default, the attention operation of the model is evaluated at full precision when xformers is not installed. To enable fp16 (which can cause numerical instabilities with the vanilla attention module on the v2.1 model) , run your script with ATTN_PRECISIONfp16 python November 24, 2022 Version 2.0 - New stable diffusion model (_Stable Diffusion 2.0-v_) at 768x768 resolution. Same number of parameters in the U-Net as 1.5, but uses OpenCLIP-ViT/H @link  as the text encoder and is trained from scratch. _SD 2.0-v_ is a so-called v-prediction @link  model. - The above model is finetuned from _SD 2.0-base_, which was trained as a standard noise-prediction model on 512x512 images and is also made available. - Added a x4 upscaling latent text-guided diffusion model(image-upscaling-with-stable-diffusion).. - New depth-guided stable diffusion model(depth-conditional-stable-diffusion), finetuned from _SD 2.0-base_. The model is conditioned on monocular depth estimates inferred via MiDaS @link  and can be used for structure-preserving img2img and shape-conditional synthesis.. - A text-guided inpainting model(image-inpainting-with-stable-diffusion), finetuned from SD _2.0-base_.. We follow the original repository @link  and provide basic inference scripts to sample from the models. ________________ The original Stable Diffusion model was created in a collaboration with CompVis @link  and RunwayML @link  and builds upon the work: High-Resolution Image Synthesis with Latent Diffusion Models @link Robin Rombach @link \, Andreas Blattmann @link \, Dominik Lorenz @link \, Patrick Esser @link , Björn Ommer @link _CVPR '22 Oral @link  GitHub @link and many others(shout-outs).. Stable Diffusion is a latent text-to-image diffusion model. ________________________________",1
4037,569927055,2,Python,Requirements. You can update an existing latent diffusion @link  environment by running @Code,2
4038,569927055,4,Python,"xformers efficient attention. For more efficiency and speed on GPUs, we highly recommended installing the xformers @link library. Tested on A100 with CUDA 11.4. Installation needs a somewhat recent version of nvcc and gcc/g, obtain those, e.g., via @Code Then, run the following (compiling takes up to 30 min). @Code Upon successful installation, the code will automatically default to memory efficient attention @link for the self- and cross-attention layers in the U-Net and autoencoder.",2
4039,569927055,2,Python,"General Disclaimer. Stable Diffusion models are general text-to-image diffusion models and therefore mirror biases and (mis-)conceptions that are present in their training data. Although efforts were made to reduce the inclusion of explicit pornographic material, we do not recommend using the provided weights for services or products without additional safety mechanisms and considerations. The weights are research artifacts and should be treated as such. Details on the training procedure and data, as well as the intended use of the model can be found in the corresponding model card @link . The weights are available via the StabilityAI organization at Hugging Face @link  under the CreativeML Open RAIL-M License(LICENSE-MODEL).",3
4040,569927055,2,Python,"Stable Diffusion v2. Stable Diffusion v2 refers to a specific configuration of the model architecture that uses a downsampling-factor 8 autoencoder with an 865M UNet and OpenCLIP ViT-H/14 text encoder for the diffusion model. The _SD 2-v_ model produces 768x768 px outputs. Evaluations with different classifier-free guidance scales (1.5, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0) and 50 DDIM sampling steps show the relative improvements of the checkpoints:",1
4041,569927055,3,Python,Text-to-Image. Stable Diffusion 2 is a latent diffusion model conditioned on the penultimate text embeddings of a CLIP ViT-H/14 text encoder. We provide a reference script for sampling(reference-sampling-script)..,2
4042,569927055,4,Python,"Reference Sampling Script. This script incorporates an invisible watermarking @link  of the outputs, to help viewers identify the images as machine-generated(scripts/tests/test_watermark.py). We provide the configs for the _SD2-v_ (768px) and _SD2-base_ (512px) model. First, download the weights for _SD2.1-v_ @link  and _SD2.1-base_ @link . To sample from the _SD2.1-v_ model, run the following: @Code or try out the Web Demo:  @link . To sample from the base model, use @Code By default, this uses the DDIM sampler @link , and renders images of size 768x768 (which it was trained on) in 50 steps. Empirically, the v-models can be sampled with higher guidance scales. Note: The inference config for all model versions is designed to be used with EMA-only checkpoints. For this reason use_emaFalse is set in the configuration, otherwise the code will try to switch from non-EMA to EMA weights.",2
4043,569927055,4,Python,"Enable Intel Extension for PyTorch optimizations in Text-to-Image script. If you're planning on running Text-to-Image on Intel CPU, try to sample an image with TorchScript and Intel Extension for PyTorch optimizations. Intel Extension for PyTorch extends PyTorch by enabling up-to-date features optimizations for an extra performance boost on Intel hardware. It can optimize memory layout of the operators to Channel Last memory format, which is generally beneficial for Intel CPUs, take advantage of the most advanced instruction set available on a machine, optimize operators and many more. Prerequisites Before running the script, make sure you have all needed libraries installed. (the optimization was checked on Ubuntu 20.04). Install jemalloc @link , numactl @link , Intel OpenMP and Intel Extension for PyTorch. @Code To sample from the _SD2.1-v_ model with TorchScriptIPEX optimizations, run the following. Remember to specify desired number of instances you want to run the program on (more @link ).. @Code To sample from the base model with IPEX optimizations, use @Code If you're using a CPU that supports bfloat16, consider sample from the model with bfloat16 enabled for a performance boost, like so @Code",2
4044,569927055,3,Python,Image Modification with Stable Diffusion.,2
4045,569927055,4,Python,"Depth-Conditional Stable Diffusion. To augment the well-established img2img @link  functionality of Stable Diffusion, we provide a _shape-preserving_ stable diffusion model.. Note that the original method for image modification introduces significant semantic changes w.r.t. the initial image. If that is not desired, download our depth-conditional stable diffusion @link  model and the dpt_hybrid MiDaS model weights @link , place the latter in a folder midas_models and sample via @Code or @Code This method can be used on the samples of the base model itself. For example, take this sample(assets/stable-samples/depth2img/old_man.png) generated by an anonymous discord user. Using the gradio @link  or streamlit @link  script depth2img.py, the MiDaS model first infers a monocular depth estimate given this input, and the diffusion model is then conditioned on the (relative) depth output. depth2image This model is particularly useful for a photorealistic style; see the examples(assets/stable-samples/depth2img). For a maximum strength of 1.0, the model removes all pixel-based information and only relies on the text prompt and the inferred monocular depth estimate.",2
4046,569927055,4,Python,"Classic Img2Img. For running the 'classic' img2img, use @Code and adapt the checkpoint and config paths accordingly.",2
4047,569927055,3,Python,"Image Upscaling with Stable Diffusion. After downloading the weights @link , run @Code or @Code for a Gradio or Streamlit demo of the text-guided x4 superresolution model. This model can be used both on real inputs and on synthesized examples. For the latter, we recommend setting a higher noise_level, e.g. noise_level100.",2
4048,569927055,3,Python,"Image Inpainting with Stable Diffusion. Download the SD 2.0-inpainting checkpoint @link  and run @Code or @Code for a Gradio or Streamlit demo of the inpainting model. This scripts adds invisible watermarking to the demo in the RunwayML @link  repository, but both should work interchangeably with the checkpoints/configs.",2
4049,569927055,2,Python,"Shout-Outs. - Thanks to Hugging Face @link  and in particular Apolinário @link   for support with our model releases! - Stable Diffusion would not be possible without LAION @link  and their efforts to create open, large-scale datasets. - The DeepFloyd team @link  at Stability AI, for creating the subset of LAION-5B @link  dataset used to train the model. - Stable Diffusion 2.0 uses OpenCLIP @link , trained by Romain Beaumont @link . - Our codebase for the diffusion models builds heavily on OpenAI's ADM codebase @link and  @link @link . Thanks for open-sourcing! - CompVis @link  initial stable diffusion release - Patrick @link 's implementation @link  of the streamlit demo for inpainting. - img2img is an application of SDEdit @link  by Chenlin Meng @link  from the Stanford AI Lab @link . - Kat's implementation( @link ) of the PLMS @link  sampler, and more @link . - DPMSolver @link  integration @link  by Cheng Lu @link . - Facebook's xformers @link  for efficient attention computation. - MiDaS @link  for monocular depth estimation.",3
4050,569927055,2,Python,"License. The code in this repository is released under the MIT License. The weights are available via the StabilityAI organization at Hugging Face @link , and released under the CreativeML Open RAIL-M License(LICENSE-MODEL) License.",3
4051,57222302,3,Python,"The team that has been maintaining Gym since 2021 has moved all future development to Gymnasium @link , a drop in replacement for Gym (import gymnasium as gym), and Gym will not be receiving any future updates. Please switch over to Gymnasium as soon as you're able to do so. If you'd like to read more about the story behind this switch, please check out this blog post @link ..",3
4052,57222302,2,Python,"Gym. Gym is an open source Python library for developing and comparing reinforcement learning algorithms by providing a standard API to communicate between learning algorithms and environments, as well as a standard set of environments compliant with that API. Since its release, Gym's API has become the field standard for doing this. Gym documentation website is at  @link @link , and you can propose fixes and changes to it here @link . Gym also has a discord server for development purposes that you can join here:  @link",1
4053,57222302,2,Python,"Installation. To install the base Gym library, use pip install gym. This does not include dependencies for all families of environments (there's a massive number, and some can be problematic to install on certain systems). You can install these dependencies for one family like pip install gymatari or use pip install gymall to install all dependencies. We support Python 3.7, 3.8, 3.9 and 3.10 on Linux and macOS. We will accept PRs related to Windows, but do not officially support it.",2
4054,57222302,2,Python,API. The Gym API's API models environments as simple Python env classes. Creating environment instances and interacting with them is very simple- here's an example using the 'CartPole-v1' environment: @Code,2
4055,57222302,2,Python,"Notable Related Libraries. Please note that this is an incomplete list, and just includes libraries that the maintainers most commonly point newcommers to when asked for recommendations. CleanRL @link  is a learning library based on the Gym API. It is designed to cater to newer people in the field and provides very good reference implementations. Tianshou @link  is a learning library that's geared towards very experienced users and is design to allow for ease in complex algorithm modifications. RLlib @link  is a learning library that allows for distributed training and inferencing and supports an extraordinarily large number of features throughout the reinforcement learning space. PettingZoo @link  is like Gym, but for environments with multiple agents.",3
4056,57222302,2,Python,"Environment Versioning. Gym keeps strict versioning for reproducibility reasons. All environments end in a suffix like '\_v0'.  When changes are made to environments that might impact learning results, the number is increased by one to prevent potential confusion.",2
4057,57222302,2,Python,MuJoCo Environments. The latest '\_v4' and future versions of the MuJoCo environments will no longer depend on mujoco-py. Instead mujoco will be the required dependency for future gym MuJoCo environment versions. Old gym MuJoCo environment versions that depend on mujoco-py will still be kept but unmaintained. To install the dependencies for the latest gym MuJoCo environments use pip install gymmujoco. Dependencies for old MuJoCo environments can still be installed by pip install gymmujoco_py.,2
4058,57222302,2,Python,Citation. A whitepaper from when Gym just came out is available  @link and can be cited with the following bibtex entry: @Code,3
4059,57222302,2,Python,"Release Notes. There used to be release notes for all the new Gym versions here. New release notes are being moved to releases page @link  on GitHub, like most other libraries do. Old notes can be viewed here @link .",3
4060,577603990,1,Python,Table of Contents. - What is Open Assistant?(what-is-open-assistant). - Useful Links(useful-links). - How To Try It Out(how-to-try-it-out). - The Vision(the-vision). - The Plan(the-plan). - How You Can Help(how-you-can-help). ---,1
4061,577603990,2,Python,What is Open Assistant?. Open Assistant is a project meant to give everyone access to a great chat based large language model. We believe that by doing this we will create a revolution in innovation in language. In the same way that stable-diffusion helped the world make art and images in new ways we hope Open Assistant can help improve the world by improving language itself.,1
4062,577603990,1,Python,Useful Links. - Data Collection @link - Chat @link - Project Documentation @link,2
4064,577603990,3,Python,Chatting with the AI. The chat frontend is now live here @link . Log in and start chatting! Please try to react with a thumbs up or down for the assistant's responses when chatting.,2
4065,577603990,3,Python,"Contributing to Data Collection. The data collection frontend is now live here @link . Log in and start taking on tasks! We want to collect a high volume of quality data. By submitting, ranking, and labelling model prompts and responses you will be directly helping to improve the capabilities of Open Assistant.",3
4066,577603990,3,Python,"Running the Development Setup Locally (without chat). You do not need to run the project locally unless you are contributing to the development process. The website link above will take you to the public website where you can use the data collection app and the chat. If you would like to run the data collection app locally for development, you can set up an entire stack needed to run Open-Assistant, including the website, backend, and associated dependent services, with Docker. To start the demo, run this in the root directory of the repository (check this FAQ @link . if you have problems): @Code Note: when running on MacOS with an M1 chip you have to use: DB_PLATFORMlinux/x86_64 docker compose ... Then, navigate to  @link (It may take some time to boot up) and interact with the website. Note: If an issue occurs with the build, please head to the FAQ @link  and check out the entries about Docker. Note: When logging in via email, navigate to  @link to get the magic email login link. Note: If you would like to run this in a standardized development environment (a 'devcontainer' @link ) using vscode locally @link . or in a web browser using GitHub Codespaces @link , you can use the provided .devcontainer(.devcontainer/) folder.",2
4067,577603990,3,Python,"Running the Development Setup Locally for Chat. You do not need to run the project locally unless you are contributing to the development process. The website link above will take you to the public website where you can use the data collection app and the chat. Also note that the local setup is only for development and is not meant to be used as a local chatbot, unless you know what you are doing. If you _do_ know what you are doing, then see the inference folder for getting the inference system up and running, or have a look at --profile inference in addition to --profile ci in the above command.",2
4068,577603990,2,Python,"The Vision. We are not going to stop at replicating ChatGPT. We want to build the assistant of the future, able to not only write email and cover letters, but do meaningful work, use APIs, dynamically research information, and much more, with the ability to be personalized and extended by anyone. And we want to do this in a way that is open and accessible, which means we must not only build a great assistant, but also make it small and efficient enough to run on consumer hardware.",1
4069,577603990,5,Python,"We want to get to an initial MVP as fast as possible, by following the 3-steps outlined in the InstructGPT paper @link . 1. Collect high-quality human generated Instruction-Fulfillment samples (prompt  response), goal 50k. We design a crowdsourced process to collect and reviewed prompts. We do not want to train on flooding/toxic/spam/junk/personal information data. We will have a leaderboard to motivate the community that shows progress and the most active users. Swag will be given to the top-contributors. 2. For each of the collected prompts we will sample multiple completions. Completions of one prompt will then be shown randomly to users to rank them from best to worst. Again this should happen crowd-sourced, e.g. we need to deal with unreliable potentially malicious users. At least multiple votes by independent users have to be collected to measure the overall agreement. The gathered ranking-data will be used to train a reward model. 3. Now follows the RLHF training phase based on the prompts and the reward model. We can then take the resulting model and continue with completion sampling step 2 for a next iteration.",3
4070,577603990,3,Python,Slide Decks. Vision & Roadmap @link Important Data Structures @link,1
4071,577603990,2,Python,How You Can Help. All open source projects begin with people like you. Open source is the belief that if we collaborate we can together gift our knowledge and technology to the world for the benefit of humanity. Check out our contributing guide(CONTRIBUTING.md) to get started.,3
4072,580642043,1,Python,Text generation web UI. A Gradio web UI for Large Language Models. Its goal is to become the AUTOMATIC1111/stable-diffusion-webui @link  of text generation.,1
4073,580642043,2,Python,"Features. 3 interface modes: default (two columns), notebook, and chat. Multiple model backends: Transformers @link , llama.cpp @link  (through llama-cpp-python @link ), ExLlamaV2 @link , AutoGPTQ @link , AutoAWQ @link . Dropdown menu for quickly switching between different models. Large number of extensions (built-in and user-contributed), including Coqui TTS for realistic voice outputs, Whisper STT for voice inputs, translation, multimodal pipelines @link , vector databases, Stable Diffusion integration, and a lot more. See the wiki @link  and the extensions directory @link  for details. Chat with custom characters @link .. Precise chat templates for instruction-following models, including Llama-2-chat, Alpaca, Vicuna, Mistral. LoRA: train new LoRAs with your own data, load/unload LoRAs on the fly for generation. Transformers library integration: load models in 4-bit or 8-bit precision through bitsandbytes, use llama.cpp with transformers samplers (llamacpp_HF loader), CPU inference in 32-bit precision using PyTorch. OpenAI-compatible API server with Chat and Completions endpoints -- see the examples @link ..",1
4074,580642043,2,Python,"How to install. 1) Clone or download @link  the repository. 2) Run the start_linux.sh, start_windows.bat, start_macos.sh, or start_wsl.bat script depending on your OS. 3) Select your GPU vendor when asked. 4) Once the installation ends, browse to  @link 5) Have fun! To restart the web UI in the future, just run the start_ script again. This script creates an installer_files folder where it sets up the project's requirements. In case you need to reinstall the requirements, you can simply delete that folder and start the web UI again. The script accepts command-line flags. Alternatively, you can edit the CMD_FLAGS.txt file with a text editor and add your flags there. To get updates in the future, run update_wizard_linux.sh, update_wizard_windows.bat, update_wizard_macos.sh, or update_wizard_wsl.bat. Setup details and information about installing manually",2
4075,580642043,3,Python,"One-click-installer. The script uses Miniconda to set up a Conda environment in the installer_files folder. If you ever need to install something manually in the installer_files environment, you can launch an interactive shell using the cmd script: cmd_linux.sh, cmd_windows.bat, cmd_macos.sh, or cmd_wsl.bat. There is no need to run any of those scripts (start_, update_wizard_, or cmd_) as admin/root. To install the requirements for extensions, you can use the extensions_reqs script for your OS. At the end, this script will install the main requirements for the project to make sure that they take precedence in case of version conflicts. For additional instructions about AMD and WSL setup, consult the documentation @link . For automated installation, you can use the GPU_CHOICE, USE_CUDA118, LAUNCH_AFTER_INSTALL, and INSTALL_EXTENSIONS environment variables. For instance: GPU_CHOICEA USE_CUDA118FALSE LAUNCH_AFTER_INSTALLFALSE INSTALL_EXTENSIONSTRUE ./start_linux.sh.",2
4076,580642043,3,Python,Manual installation using Conda. Recommended if you have some experience with the command-line.,2
4077,580642043,4,Python,"0. Install Conda. @link On Linux or WSL, it can be automatically installed with these two commands (source @link ): @Code",2
4078,580642043,4,Python,1. Create a new conda environment. @Code,2
4079,580642043,4,Python,"2. Install Pytorch. The up-to-date commands can be found here:  @link For NVIDIA, you also need to install the CUDA runtime libraries: @Code If you need nvcc to compile some library manually, replace the command above with @Code",2
4080,580642043,4,Python,3. Install the web UI. @Code Requirements file to use:,2
4081,580642043,3,Python,Start the web UI. @Code Then browse to @link,2
4082,580642043,5,Python,AMD GPU on Windows. 1) Use requirements_cpu_only.txt or requirements_cpu_only_noavx2.txt in the command above. 2) Manually install llama-cpp-python using the appropriate command for your hardware: Installation from PyPI @link .. Use the LLAMA_HIPBLASon toggle. Note the Windows remarks @link .. 3) Manually install AutoGPTQ: Installation @link .. Perform the from-source installation - there are no prebuilt ROCm packages for Windows.,2
4083,580642043,5,Python,"Older NVIDIA GPUs. 1) For Kepler GPUs and older, you will need to install CUDA 11.8 instead of 12: @Code 2) bitsandbytes  0.39 may not work. In that case, to use --load-in-8bit, you may have to downgrade like this: Linux: pip install bitsandbytes0.38.1 Windows: pip install  @link",2
4084,580642043,5,Python,"Manual install. The requirements.txt above contain various wheels precompiled through GitHub Actions. If you wish to compile things manually, or if you need to because no suitable wheels are available for your hardware, you can use requirements_nowheels.txt and then install your desired loaders manually.",2
4085,580642043,3,Python,"Alternative: Docker. @Code You need to have Docker Compose v2.17 or higher installed. See this guide @link  for instructions. For additional docker files, check out this repository @link .",2
4086,580642043,3,Python,"Updating the requirements. From time to time, the requirements.txt change. To update, use these commands: @Code List of command-line flags @Code",2
4087,580642043,2,Python,"Downloading models. Models should be placed in the folder text-generation-webui/models. They are usually downloaded from Hugging Face @link . GGUF models are a single file and should be placed directly into models. Example: @Code The remaining model types (like 16-bit transformers models and GPTQ models) are made of several files and must be placed in a subfolder. Example: @Code In both cases, you can use the 'Model' tab of the UI to download the model from Hugging Face automatically. It is also possible to download it via the command-line with @Code Run python download-model.py --help to see all the options.",2
4088,580642043,2,Python,Google Colab notebook. @link,2
4089,580642043,2,Python,"Contributing. If you would like to contribute to the project, check out the Contributing guidelines @link .",3
4090,580642043,2,Python,Community. Subreddit:  @link  Discord:  @link,3
4091,580642043,2,Python,"Acknowledgment. In August 2023, Andreessen Horowitz @link  (a16z) provided a generous grant to encourage and support my independent work on this project. I am extremely grateful for their trust and recognition.",3
4093,589831718,2,Python,"Features. - Nodes/graph/flowchart interface to experiment and create complex Stable Diffusion workflows without needing to code anything. - Fully supports SD1.x, SD2.x, SDXL @link , Stable Video Diffusion @link , Stable Cascade @link  and SD3 @link - Asynchronous Queue system - Many optimizations: Only re-executes the parts of the workflow that changes between executions. - Command line option:  @Code  to make it work on GPUs with less than 3GB vram (enabled automatically on GPUs with low vram) - Works even if you don't have a GPU with:  @Code  (slow) - Can load ckpt, safetensors and diffusers models/checkpoints. Standalone VAEs and CLIP models. - Embeddings/Textual inversion - Loras (regular, locon and loha) @link - Hypernetworks @link - Loading full workflows (with seeds) from generated PNG files. - Saving/Loading workflows as Json files. - Nodes interface can be used to create complex workflows like one for Hires fix @link  or much more advanced ones. - Area Composition @link - Inpainting @link  with both regular and inpainting models. - ControlNet and T2I-Adapter @link - Upscale Models (ESRGAN, ESRGAN variants, SwinIR, Swin2SR, etc...) @link - unCLIP Models @link - GLIGEN @link - Model Merging @link - LCM models and Loras @link - SDXL Turbo @link - Latent previews with TAESD(how-to-show-high-quality-previews). - Starts up very fast. - Works fully offline: will never download anything. - Config file(extra_model_paths.yaml.example) to set the search paths for models. Workflow examples can be found on the Examples page @link",1
4094,589831718,2,Python,Shortcuts. Ctrl can also be replaced with Cmd instead for macOS users,2
4095,589831718,2,Python,Windows. There is a portable standalone build for Windows that should work for running on Nvidia GPUs or for running on your CPU only on the releases page @link .,2
4096,589831718,3,Python,"Direct link to download @link . Simply download, extract with 7-Zip @link  and run. Make sure you put your Stable Diffusion checkpoints/models (the huge ckpt/safetensors files) in: ComfyUI\models\checkpoints If you have trouble extracting it, right click the file - properties - unblock",2
4097,589831718,4,Python,How do I share models between another UI and ComfyUI?. See the Config file(extra_model_paths.yaml.example) to set the search paths for models. In the standalone windows build you can find this file in the ComfyUI directory. Rename this file to extra_model_paths.yaml and edit it with your favorite text editor.,2
4098,589831718,2,Python,"Jupyter Notebook. To run it on services like paperspace, kaggle or colab you can use my Jupyter Notebook(notebooks/comfyui_colab.ipynb)",2
4099,589831718,2,Python,"Manual Install (Windows, Linux). Git clone this repo. Put your SD checkpoints (the huge ckpt/safetensors files) in: models/checkpoints Put your VAE in: models/vae",2
4100,589831718,3,Python,"AMD GPUs (Linux only). AMD users can install rocm and pytorch with pip if you don't have it already installed, this is the command to install the stable version: @Code This is the command to install the nightly with ROCm 6.0 which might have some performance improvements: @Code",2
4101,589831718,3,Python,NVIDIA. Nvidia users should install stable pytorch using this command: @Code This is the command to install pytorch nightly instead which might have performance improvements: @Code,2
4102,589831718,4,Python,"Troubleshooting. If you get the 'Torch not compiled with CUDA enabled' error, uninstall torch with: @Code And install it again with the command above.",2
4103,589831718,3,Python,Dependencies. Install the dependencies by opening your terminal inside the ComfyUI folder and: @Code After this you should have everything installed and can proceed to running ComfyUI.,2
4104,589831718,4,Python,Intel GPUs. Intel GPU support is available for all Intel GPUs supported by Intel's Extension for Pytorch (IPEX) with the support requirements listed in the Installation @link  page. Choose your platform and method of install and follow the instructions. The steps are as follows:. 1. Start by installing the drivers or kernel listed or newer in the Installation page of IPEX linked above for Windows and Linux if needed. 1. Follow the instructions to install Intel's oneAPI Basekit @link  for your platform. 1. Install the packages for IPEX using the instructions provided in the Installation page for your platform. 1. Follow the ComfyUI manual installation(manual-install-windows-linux) instructions for Windows and Linux and run ComfyUI normally as described above after everything is installed.. Additional discussion and help can be found here @link .,2
4105,589831718,4,Python,"Apple Mac silicon. You can install ComfyUI in Apple Mac silicon (M1 or M2) with any recent macOS version. 1. Install pytorch nightly. For instructions, read the Accelerated PyTorch training on Mac @link  Apple Developer guide (make sure to install the latest pytorch nightly). 1. Follow the ComfyUI manual installation(manual-install-windows-linux) instructions for Windows and Linux.. 1. Install the ComfyUI dependencies(dependencies). If you have another Stable Diffusion UI you might be able to reuse the dependencies(i-already-have-another-ui-for-stable-diffusion-installed-do-i-really-have-to-install-all-of-these-dependencies).. 1. Launch ComfyUI by running python main.py Note: Remember to add your models, VAE, LoRAs etc. to the corresponding Comfy folders, as discussed in ComfyUI manual installation(manual-install-windows-linux)..",2
4106,589831718,4,Python,DirectML (AMD Cards on Windows). @Code  Then you can launch ComfyUI with:  @Code,2
4107,589831718,3,Python,I already have another UI for Stable Diffusion installed do I really have to install all of these dependencies?. You don't. If you have another UI installed and working with its own python venv you can use that venv to run ComfyUI. You can open up your favorite terminal and activate it: @Code or on Windows: With Powershell:  @Code With cmd.exe:  @Code And then you can use that terminal to run ComfyUI without installing any dependencies. Note that the venv folder might be called something else depending on the SD UI.,2
4108,589831718,3,Python,"For AMD cards not officially supported by ROCm. Try running it with this command if you have issues: For 6700, 6600 and maybe other RDNA2 or older:  @Code For AMD 7600 and maybe other RDNA3 cards:  @Code",2
4109,589831718,1,Python,"Notes. Only parts of the graph that have an output with all the correct inputs will be executed. Only parts of the graph that change from each execution to the next will be executed, if you submit the same graph twice only the first will be executed. If you change the last part of the graph only the part you changed and the part that depends on it will be executed. Dragging a generated png on the webpage or loading one will give you the full workflow including seeds that were used to create it. You can use () to change emphasis of a word or phrase like: (good code:1.2) or (bad code:0.8). The default emphasis for () is 1.1. To use () characters in your actual prompt escape them like \\( or \\). You can use {day Dynamic prompts also support C-style comments, like // comment or / comment /. To use a textual inversion concepts/embeddings in a text prompt put them in the models/embeddings directory and use them in the CLIPTextEncode node like this (you can omit the .pt extension): @Code",2
4110,589831718,2,Python,"How to show high-quality previews?. Use  @Code  to enable previews. The default installation includes a fast latent preview method that's low-resolution. To enable higher-quality previews with TAESD @link , download the taesd_decoder.pth @link  (for SD1.x and SD2.x) and taesdxl_decoder.pth @link  (for SDXL) models and place them in the models/vae_approx folder. Once they're installed, restart ComfyUI to enable high-quality previews.",2
4111,589831718,2,Python,"How to use TLS/SSL?. Generate a self-signed certificate (not appropriate for shared/production use) and key by running the command: openssl req -x509 -newkey rsa:4096 -keyout key.pem -out cert.pem -sha256 -days 3650 -nodes -subj '/CXX/STStateName/LCityName/OCompanyName/OUCompanySectionName/CNCommonNameOrHostname' Use --tls-keyfile key.pem --tls-certfile cert.pem to enable TLS/SSL, the app will now be accessible with  @link instead of  @link Note: Windows users can use alexisrolland/docker-openssl @link  or one of the 3rd party binary distributions @link  to run the command example above. If you use a container, note that the volume mount -v can be a relative path so ... -v '.\:/openssl-certs' ... would create the key & cert files in the current directory of your command prompt or powershell terminal.",2
4112,589831718,2,Python,Support and dev channel. Matrix space: comfyui_space:matrix.org @link  (it's like discord but open source)..,3
4113,589831718,3,Python,Why did you make this?. I wanted to learn how Stable Diffusion worked in detail. I also wanted something clean and powerful that would let me experiment with SD without restrictions.,1
4114,589831718,3,Python,Who is this for?. This is for anyone that wants to make complex workflows with SD or that wants to learn more how SD works. The interface follows closely how SD works and the code should be much more simple to understand than other SD UIs.,1
4115,596892,1,Python,"Flask. Flask is a lightweight WSGI web application framework. It is designed to make getting started quick and easy, with the ability to scale up to complex applications. It began as a simple wrapper around Werkzeug and Jinja, and has become one of the most popular Python web application frameworks. Flask offers suggestions, but doesn't enforce any dependencies or project layout. It is up to the developer to choose the tools and libraries they want to use. There are many extensions provided by the community that make adding new functionality easy. WSGI:  @link Werkzeug:  @link Jinja:  @link",1
4116,596892,2,Python,A Simple Example. @Code @Code,2
4117,596892,2,Python,"Donate. The Pallets organization develops and supports Flask and the libraries it uses. In order to grow the community of contributors and users, and allow the maintainers to devote more time to the projects, please donate today. please donate today:  @link",3
4118,601538369,1,Python,"Llama 2. We are unlocking the power of large language models. Llama 2 is now accessible to individuals, creators, researchers, and businesses of all sizes so that they can experiment, innovate, and scale their ideas responsibly. This release includes model weights and starting code for pre-trained and fine-tuned Llama language models — ranging from 7B to 70B parameters. This repository is intended as a minimal example to load Llama 2 @link  models and run inference. For more detailed examples leveraging Hugging Face, see llama-recipes @link .",1
4119,601538369,2,Python,"Updates post-launch. See UPDATES.md(UPDATES.md). Also for a running list of frequently asked questions, see here @link .",3
4120,601538369,2,Python,"Download. In order to download the model weights and tokenizer, please visit the Meta website @link  and accept our License. Once your request is approved, you will receive a signed URL over email. Then run the download.sh script, passing the URL provided when prompted to start the download. Pre-requisites: Make sure you have wget and md5sum installed. Then run the script: ./download.sh. Keep in mind that the links expire after 24 hours and a certain amount of downloads. If you start seeing errors such as 403: Forbidden, you can always re-request a link.",2
4121,601538369,3,Python,"Access to Hugging Face. We are also providing downloads on Hugging Face @link . You can request access to the models by acknowledging the license and filling the form in the model card of a repo. After doing so, you should get access to all the Llama models of a version (Code Llama, Llama 2, or Llama Guard) within 1 hour.",2
4122,601538369,2,Python,"Quick Start. You can follow the steps below to quickly get up and running with Llama 2 models. These steps will let you run quick inference locally. For more examples, see the Llama 2 recipes repository @link . 1. In a conda env with PyTorch / CUDA available clone and download this repository. 2. In the top-level directory run: @Code 3. Visit the Meta website @link  and register to download the model/s. 4. Once registered, you will get an email with a URL to download the models. You will need this URL when you run the download.sh script. 5. Once you get the email, navigate to your downloaded llama repository and run the download.sh script. - Make sure to grant execution permissions to the download.sh script - During this process, you will be prompted to enter the URL from the email. - Do not use the “Copy Link” option but rather make sure to manually copy the link from the email. 6. Once the model/s you want have been downloaded, you can run the model locally using the command below: @Code Note - Replace  llama-2-7b-chat/ with the path to your checkpoint directory and tokenizer.model with the path to your tokenizer model. - The –nproc_per_node should be set to the MP(inference) value for the model you are using.. - Adjust the max_seq_len and max_batch_size parameters as needed. - This example runs the example_chat_completion.py(example_chat_completion.py) found in this repository but you can change that to a different .py file.",2
4123,601538369,2,Python,"Inference. Different models require different model-parallel (MP) values: All models support sequence length up to 4096 tokens, but we pre-allocate the cache according to max_seq_len and max_batch_size values. So set those according to your hardware.",2
4124,601538369,3,Python,"Pretrained Models. These models are not finetuned for chat or Q&A. They should be prompted so that the expected answer is the natural continuation of the prompt. See example_text_completion.py for some examples. To illustrate, see the command below to run it with the llama-2-7b model (nproc_per_node needs to be set to the MP value): @Code",2
4125,601538369,3,Python,"Fine-tuned Chat Models. The fine-tuned models were trained for dialogue applications. To get the expected features and performance for them, a specific formatting defined in chat_completion @link . needs to be followed, including the INST and  tags, BOS and EOS tokens, and the whitespaces and breaklines in between (we recommend calling strip() on inputs to avoid double-spaces). You can also deploy additional classifiers for filtering out inputs and outputs that are deemed unsafe. See the llama-recipes repo for an example @link  of how to add a safety checker to the inputs and outputs of your inference code. Examples using llama-2-7b-chat: @Code Llama 2 is a new technology that carries potential risks with use. Testing conducted to date has not — and could not — cover all scenarios. In order to help developers address these risks, we have created the Responsible Use Guide(Responsible-Use-Guide.pdf). More details can be found in our research paper as well.",2
4126,601538369,2,Python,"Issues. Please report any software “bug”, or other problems with the models through one of the following means: - Reporting issues with the model: github.com/facebookresearch/llama @link - Reporting risky content generated by the model: developers.facebook.com/llama_output_feedback @link - Reporting bugs and security concerns: facebook.com/whitehat/info @link",3
4127,601538369,2,Python,Model Card. See MODEL_CARD.md(MODEL_CARD.md).,1
4128,601538369,2,Python,"License. Our model and weights are licensed for both researchers and commercial entities, upholding the principles of openness. Our mission is to empower individuals, and industry through this opportunity, while fostering an environment of discovery and ethical AI advancements. See the LICENSE(LICENSE) file, as well as our accompanying Acceptable Use Policy(USE_POLICY.md)",3
4129,601538369,2,Python,"References. 1. Research Paper @link 2. Llama 2 technical overview @link 3. Open Innovation AI Research Community @link For common questions, the FAQ can be found here @link  which will be kept up to date over time as new questions arise.",3
4130,601538369,2,Python,Original Llama. The repo for the original llama release is in the llama_v1 @link  branch.,3
4131,608555244,1,Python,"TaskMatrix. TaskMatrix connects ChatGPT and a series of Visual Foundation Models to enable sending and receiving images during chatting. See our paper: Visual ChatGPT: Talking, Drawing and Editing with Visual Foundation Models @link",1
4132,608555244,2,Python,"Updates:. - Now TaskMatrix supports GroundingDINO @link  and segment-anything @link ! Thanks @jordddan for his efforts. For the image editing case, GroundingDINO is first used to locate bounding boxes guided by given text, then segment-anything is used to generate the related mask, and finally stable diffusion inpainting is used to edit image based on the mask. - Firstly, run python visual_chatgpt.py --load 'Text2Box_cuda:0,Segmenting_cuda:0,Inpainting_cuda:0,ImageCaptioning_cuda:0' - Then, say find xxx in the image or segment xxx in the image. xxx is an object. TaskMatrix will return the detection or segmentation result! - Now TaskMatrix can support Chinese! Thanks to @Wang-Xiaodong1899 for his efforts. - We propose the template idea in TaskMatrix! - A template is a pre-defined execution flow that assists ChatGPT in assembling complex tasks involving multiple foundation models. - A template contains the experiential solution to complex tasks as determined by humans. - A template can invoke multiple foundation models or even establish a new ChatGPT session - To define a template, simply adding a class with attributes template_model  True - Thanks to @ShengmingYin and @thebestannie for providing a template example in InfinityOutPainting class (see the following gif) - Firstly, run python visual_chatgpt.py --load 'Inpainting_cuda:0,ImageCaptioning_cuda:0,VisualQuestionAnswering_cuda:0' - Secondly, say extend the image to 2048x1024 to TaskMatrix! - By simply creating an InfinityOutPainting template, TaskMatrix can seamlessly extend images to any size through collaboration with existing ImageCaptioning, Inpainting, and VisualQuestionAnswering foundation models, without the need for additional training. - TaskMatrix needs the effort of the community! We crave your contribution to add new and interesting features!",1
4133,608555244,2,Python,"Insight & Goal:. On the one hand, ChatGPT (or LLMs) serves as a general interface that provides a broad and diverse understanding of a wide range of topics. On the other hand, Foundation Models serve as domain experts by providing deep knowledge in specific domains. By leveraging both general and deep knowledge, we aim at building an AI that is capable of handling various tasks.",1
4135,608555244,2,Python,"GPU memory usage. Here we list the GPU memory usage of each visual foundation model, you can specify which one you like:",1
4136,608555244,2,Python,Acknowledgement. We appreciate the open source of the following projects: Hugging Face @link  &8194;. LangChain @link  &8194;. Stable Diffusion @link  &8194; . ControlNet @link  &8194; . InstructPix2Pix @link  &8194; . CLIPSeg @link  &8194;. BLIP @link  &8194;.,3
4137,608555244,2,Python,"Contact Information. For help or issues using the TaskMatrix, please submit a GitHub issue. For other communications, please contact Chenfei WU (chewu@microsoft.com) or Nan DUAN (nanduan@microsoft.com).",3
4138,608555244,2,Python,"Trademark Notice. Trademarks This project may contain trademarks or logos for projects, products, or services. Authorized use of Microsoft trademarks or logos is subject to and must follow Microsoft’s Trademark & Brand Guidelines @link . Use of Microsoft trademarks or logos in modified versions of this project must not cause confusion or imply Microsoft sponsorship. Any use of third-party trademarks or logos are subject to those third-party’s policies.",3
4139,608555244,2,Python,"Disclaimer. The recommended models in this Repo are just examples, used for scientific research exploring the concept of task automation and benchmarking with the paper published at Visual ChatGPT: Talking, Drawing and Editing with Visual Foundation Models @link . Users can replace the models in this Repo according to their research needs. When using the recommended models in this Repo, you need to comply with the licenses of these models respectively. Microsoft shall not be held liable for any infringement of third-party rights resulting from your usage of this repo. Users agree to defend, indemnify and hold Microsoft harmless from and against all damages, costs, and attorneys' fees in connection with any claims arising from this Repo. If anyone believes that this Repo infringes on your rights, please notify the project owner email(chewu@microsoft.com).",3
4140,612427246,1,Python,"Stanford Alpaca: An Instruction-following LLaMA Model. @link @link @link @link @link This is the repo for the Stanford Alpaca project, which aims to build and share an instruction-following LLaMA model. The repo contains: - The 52K data(data-release) used for fine-tuning the model.. - The code for generating the data(data-generation-process).. - The code for fine-tuning the model(fine-tuning).. - The code for recovering Alpaca-7B weights from our released weight diff(recovering-alpaca-weights).. Note: We thank the community for feedback on Stanford-Alpaca and supporting our research. Our live demo is suspended until further notice. Usage and License Notices: Alpaca is intended and licensed for research use only. The dataset is CC BY NC 4.0 (allowing only non-commercial use) and models trained using the dataset should not be used outside of research purposes. The weight diff is also CC BY NC 4.0 (allowing only non-commercial use).",1
4141,612427246,2,Python,"Overview. The current Alpaca model is fine-tuned from a 7B LLaMA model 1 on 52K instruction-following data generated by the techniques in the Self-Instruct 2 paper, with some modifications that we discuss in the next section. In a preliminary human evaluation, we found that the Alpaca 7B model behaves similarly to the text-davinci-003 model on the Self-Instruct instruction-following evaluation suite 2. Alpaca is still under development, and there are many limitations that have to be addressed. Importantly, we have not yet fine-tuned the Alpaca model to be safe and harmless. We thus encourage users to be cautious when interacting with Alpaca, and to report any concerning behavior to help improve the safety and ethical considerations of the model. Our initial release contains the data generation procedure, dataset, and training recipe. We intend to release the model weights if we are given permission to do so by the creators of LLaMA. For now, we have chosen to host a live demo to help readers better understand the capabilities and limits of Alpaca, as well as a way to help us better evaluate Alpaca's performance on a broader audience. Please read our release blog post @link  for more details about the model, our discussion of the potential harm and limitations of Alpaca models, and our thought process for releasing a reproducible model. 1: LLaMA: Open and Efficient Foundation Language Models. Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, Guillaume Lample.  @link 2: Self-Instruct: Aligning Language Model with Self Generated Instructions. Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A. Smith, Daniel Khashabi, Hannaneh Hajishirzi.  @link",1
4142,612427246,2,Python,"Data Release. alpaca_data.json @link  contains 52K instruction-following data we used for fine-tuning the Alpaca model. This JSON file is a list of dictionaries, each dictionary contains the following fields: - instruction: str, describes the task the model should perform. Each of the 52K instructions is unique. - input: str, optional context or input for the task. For example, when the instruction is 'Summarize the following article', the input is the article. Around 40% of the examples have an input. - output: str, the answer to the instruction as generated by text-davinci-003. We used the following prompts for fine-tuning the Alpaca model: - for examples with a non-empty input field: @Code - for examples with an empty input field: @Code During inference (eg for the web demo), we use the user instruction with an empty input field (second option).",2
4143,612427246,2,Python,"Data Generation Process. Running the code 1. Set environment variables OPENAI_API_KEY to your OpenAI API key. 2. Install the dependencies with pip install -r requirements.txt. 3. Run python -m generate_instruction generate_instruction_following_data to generate the data. We built on the data generation pipeline from self-instruct @link  and made the following modifications: - We used text-davinci-003 to generate the instruction data instead of davinci. - We wrote a new prompt (prompt.txt) that explicitly gave the requirement of instruction generation to text-davinci-003. Note: there is a slight error in the prompt we used, and future users should incorporate the edit in - We adopted much more aggressive batch decoding, i.e., generating 20 instructions at once, which significantly reduced the cost of data generation. - We simplified the data generation pipeline by discarding the difference between classification and non-classification instructions. - We only generated a single instance for each instruction, instead of 2 to 3 instances as in 1. This produced an instruction-following dataset with 52K examples obtained at a much lower cost (less than $500). In a preliminary study, we also find our 52K generated data to be much more diverse than the data released by self-instruct @link . We plot the below figure (in the style of Figure 2 in the self-instruct paper @link  to demonstrate the diversity of our data. The inner circle of the plot represents the root verb of the instructions, and the outer circle represents the direct objects. //:  (!parse_analysis&40;assert/parse_analysis.png  @link",2
4144,612427246,2,Python,"Fine-tuning. We fine-tune our models using standard Hugging Face training code. We fine-tune LLaMA-7B and LLaMA-13B with the following hyperparameters: To reproduce our fine-tuning runs for LLaMA, first install the requirements @Code Below is a command that fine-tunes LLaMA-7B with our dataset on a machine with 4 A100 80G GPUs in FSDP full_shard mode. We were able to reproduce a model of similar quality as the one we hosted in our demo with the following command using Python 3.10. Replace  with a port of your own,  with the path to your converted checkpoint and tokenizer (following instructions in the PR), and  with where you want to store your outputs. @Code The same script also works for OPT fine-tuning. Here's an example for fine-tuning OPT-6.7B @Code Note the given training script is meant to be simple and easy to use, and is not particularly optimized. To run on more gpus, you may prefer to turn down gradient_accumulation_steps to keep a global batch size of 128. Global batch size has not been tested for optimality.",2
4145,612427246,3,Python,"Addressing OOM. Naively, fine-tuning a 7B model requires about 7 x 4 x 4  112 GB of VRAM. Commands given above enable parameter sharding, so no redundant model copy is stored on any GPU. If you'd like to further reduce the memory footprint, here are some options: - Turn on CPU offload for FSDP with --fsdp 'full_shard auto_wrap offload'. This saves VRAM at the cost of longer runtime. - In our experience, DeepSpeed stage-3 (with offload) can at times be more memory efficient than FSDP with offload. Here's an example to use DeepSpeed stage-3 with 4 GPUs with both parameter and optimizer offload: @Code - The DeepSpeed library also provides some helpful functions @link  to estimate memory usage. - LoRA @link  fine-tunes low-rank slices of the query, key, and value embedding heads. This can reduce the total memory footprint from 112GB to about 7x428GB. We may release our re-implemention of this in the future, but for now the peft @link  codebase can be a useful resource.",2
4146,612427246,2,Python,"Recovering Alpaca Weights. The weight diff between Alpaca-7B and LLaMA-7B is located here @link . To recover the original Alpaca-7B weights, follow these steps: @Code Once step 3 completes, you should have a directory with the recovered weights, from which you can load the model like the following @Code",2
4147,612427246,3,Python,Authors. All grad students below contributed equally and the order is determined by random draw. - Rohan Taori @link - Ishaan Gulrajani @link - Tianyi Zhang @link - Yann Dubois @link - Xuechen Li @link All advised by Tatsunori B. Hashimoto @link . Yann is also advised by Percy Liang @link  and Xuechen is also advised by Carlos Guestrin @link .,3
4148,612427246,3,Python,"Citation. Please cite the repo if you use the data or code in this repo. @Code Naturally, you should also cite the original LLaMA paper 1 and the Self-Instruct paper 2.",3
4149,612427246,3,Python,"Acknowledgements. We thank Yizhong Wang for his help in explaining the data generation pipeline in Self-Instruct and providing the code for the parse analysis plot. We thank Yifan Mai for helpful support, and members of the Stanford NLP Group as well as the Center for Research on Foundation Models (CRFM) for their helpful feedback.",3
4150,614765452,1,Python,"AutoGPT: build & use AI agents. @link  &ensp; @link  &ensp; @link AutoGPT is a generalist LLM based AI agent that can autonomously accomplish minor tasks. Examples: - Look up and summarize this research paper - Write a marketing for food supplements - Write a blog post detailing the news in AI Our mission is to provide the tools, so that you can focus on what matters: -  Building - Lay the foundation for something amazing. -  Testing - Fine-tune your agent to perfection. -  Delegating - Let AI work for you, and have your ideas come to life. Be part of the revolution! AutoGPT is here to stay, at the forefront of AI innovation. Documentation @link &ensp; Contributing(CONTRIBUTING.md) &ensp; Build your own Agent - Quickstart(QUICKSTART.md)",1
4152,614765452,3,Python,"Forge. Forge your own agent! &ndash; Forge is a ready-to-go template for your agent application. All the boilerplate code is already handled, letting you channel all your creativity into the things that set your agent apart. All tutorials are located here @link . Components from the forge.sdk(/forge/forge/sdk) can also be used individually to speed up development and reduce boilerplate in your agent project. Getting Started with Forge @link  &ndash; This guide will walk you through the process of creating your own agent and using the benchmark and user interface. Learn More @link  about Forge",2
4153,614765452,3,Python,"Benchmark. Measure your agent's performance! The agbenchmark can be used with any agent that supports the agent protocol, and the integration with the project's CLI makes it even easier to use with AutoGPT and forge-based agents. The benchmark offers a stringent testing environment. Our framework allows for autonomous, objective performance evaluations, ensuring your agents are primed for real-world action. agbenchmark @link  on Pypi &ensp; Learn More @link  about the Benchmark",2
4154,614765452,3,Python,"UI. Makes agents easy to use! The frontend gives you a user-friendly interface to control and monitor your agents. It connects to agents through the agent protocol(-agent-protocol), ensuring compatibility with many agents from both inside and outside of our ecosystem.. The frontend works out-of-the-box with all agents in the repo. Just use the CLI to run your agent of choice! Learn More @link  about the Frontend",2
4155,614765452,3,Python,"CLI. CLI: -cli. To make it as easy as possible to use all of the tools offered by the repository, a CLI is included at the root of the repo: @Code Just clone the repo, install dependencies with ./run setup, and you should be good to go!",2
4156,614765452,2,Python,Questions? Problems? Suggestions?.,3
4157,614765452,3,Python,"Get help - Discord  @link . @link To report a bug or request a feature, create a GitHub Issue @link . Please ensure someone else hasn’t created an issue for the same topic.",3
4159,614765452,3,Python,"Agent Protocol. To maintain a uniform standard and ensure seamless compatibility with many current and future applications, AutoGPT employs the agent protocol @link  standard by the AI Engineer Foundation. This standardizes the communication pathways from your agent to the frontend and benchmark. --- .",1
4160,615882673,1,Python,"FastChat. FastChat is an open platform for training, serving, and evaluating large language model based chatbots. - FastChat powers Chatbot Arena  @link , serving over 10 million chat requests for 70 LLMs. - Chatbot Arena has collected over 500K human votes from side-by-side LLM battles to compile an online LLM Elo leaderboard @link . FastChat's core features include: - The training and evaluation code for state-of-the-art models (e.g., Vicuna, MT-Bench). - A distributed multi-model serving system with web UI and OpenAI-compatible RESTful APIs.",1
4161,615882673,2,Python,"News. - 2024/03  We released Chatbot Arena technical report @link . - 2023/09 We released LMSYS-Chat-1M, a large-scale real-world LLM conversation dataset. Read the report @link . - 2023/08 We released Vicuna v1.5 based on Llama 2 with 4K and 16K context lengths. Download weights(vicuna-weights).. - 2023/07 We released Chatbot Arena Conversations, a dataset containing 33k conversations with human preferences. Download it here @link . More - 2023/08 We released LongChat v1.5 based on Llama 2 with 32K context lengths. Download weights(longchat).. - 2023/06 We introduced MT-bench, a challenging multi-turn question set for evaluating chatbots. Check out the blog post @link . - 2023/06 We introduced LongChat, our long-context chatbots and evaluation tools. Check out the blog post @link . - 2023/05 We introduced Chatbot Arena for battles among LLMs. Check out the blog post @link . - 2023/03 We released Vicuna: An Open-Source Chatbot Impressing GPT-4 with 90% ChatGPT Quality. Check out the blog post @link .",1
4162,615882673,2,Python,Contents. - Install(install). - Model Weights(model-weights). - Inference with Command Line Interface(inference-with-command-line-interface). - Serving with Web GUI(serving-with-web-gui). - API(api). - Evaluation(evaluation). - Fine-tuning(fine-tuning). - Citation(citation).,1
4163,615882673,3,Python,Method 1: With pip. @Code,2
4164,615882673,3,Python,Method 2: From source. 1. Clone this repository and navigate to the FastChat folder @Code If you are running on Mac: @Code 2. Install Package @Code,2
4165,615882673,3,Python,"Vicuna Weights. Vicuna @link  is based on Llama 2 and should be used under Llama's model license @link . You can use the commands below to start chatting. It will automatically download the weights from Hugging Face repos. Downloaded weights are stored in a .cache folder in the user's home folder (e.g., /.cache/huggingface/hub/). See more command options and how to handle out-of-memory in the 'Inference with Command Line Interface' section below. NOTE: transformers4.31 is required for 16K versions. Old weights: see docs/vicuna_weights_version.md(docs/vicuna_weights_version.md) for all versions of weights and their differences.",2
4166,615882673,3,Python,"Other Models. Besides Vicuna, we also released two additional models: LongChat @link  and FastChat-T5. You can use the commands below to chat with them. They will automatically download the weights from Hugging Face repos.",2
4167,615882673,2,Python,Inference with Command Line Interface. (Experimental Feature: You can specify --style rich to enable rich text output and better text streaming quality for some non-ASCII content. This may not work properly on certain terminals.),2
4168,615882673,4,Python,"Supported Models. FastChat supports a wide range of models, including LLama 2, Vicuna, Alpaca, Baize, ChatGLM, Dolly, Falcon, FastChat-T5, GPT4ALL, Guanaco, MTP, OpenAssistant, OpenChat, RedPajama, StableLM, WizardLM, xDAN-AI and more. See a complete list of supported models and instructions to add a new model here(docs/model_support.md).",1
4169,615882673,4,Python,Single GPU. The command below requires around 14GB of GPU memory for Vicuna-7B and 28GB of GPU memory for Vicuna-13B. See the 'Not Enough Memory' section(not-enough-memory) below if you do not have enough memory.. --model-path can be a local folder or a Hugging Face repo name. @Code,2
4170,615882673,4,Python,"Multiple GPUs. You can use model parallelism to aggregate GPU memory from multiple GPUs on the same machine. @Code Tips: Sometimes the 'auto' device mapping strategy in huggingface/transformers does not perfectly balance the memory allocation across multiple GPUs. You can use --max-gpu-memory to specify the maximum memory per GPU for storing model weights. This allows it to allocate more memory for activations, so you can use longer context lengths or larger batch sizes. For example, @Code",2
4171,615882673,4,Python,CPU Only. This runs on the CPU only and does not require GPU. It requires around 30GB of CPU memory for Vicuna-7B and around 60GB of CPU memory for Vicuna-13B. @Code Use Intel AI Accelerator AVX512_BF16/AMX to accelerate CPU inference. @Code,2
4172,615882673,4,Python,Metal Backend (Mac Computers with Apple Silicon or AMD GPUs). Use --device mps to enable GPU acceleration on Mac computers (requires torch  2.0). Use --load-8bit to turn on 8-bit compression. @Code Vicuna-7B can run on a 32GB M1 Macbook with 1 - 2 words / second.,2
4173,615882673,4,Python,Intel XPU (Intel Data Center and Arc A-Series GPUs). Install the Intel Extension for PyTorch @link . Set the OneAPI environment variables: @Code Use --device xpu to enable XPU/GPU acceleration. @Code Vicuna-7B can run on an Intel Arc A770 16GB.,2
4174,615882673,4,Python,Ascend NPU. Install the Ascend PyTorch Adapter @link . Set the CANN environment variables: @Code Use --device npu to enable NPU acceleration. @Code Vicuna-7B/13B can run on an Ascend NPU.,2
4175,615882673,4,Python,"Not Enough Memory. If you do not have enough memory, you can enable 8-bit compression by adding --load-8bit to commands above. This can reduce memory usage by around half with slightly degraded model quality. It is compatible with the CPU, GPU, and Metal backend. Vicuna-13B with 8-bit compression can run on a single GPU with 16 GB of VRAM, like an Nvidia RTX 3090, RTX 4080, T4, V100 (16GB), or an AMD RX 6800 XT. @Code In addition to that, you can add --cpu-offloading to commands above to offload weights that don't fit on your GPU onto the CPU memory. This requires 8-bit compression to be enabled and the bitsandbytes package to be installed, which is only available on linux operating systems.",2
4176,615882673,4,Python,"More Platforms and Quantization. - For AMD GPU users, please install ROCm and the ROCm version of PyTorch @link  before you install FastChat. See also this post @link .. - FastChat supports ExLlama V2. See docs/exllama_v2.md(/docs/exllama_v2.md). - FastChat supports GPTQ 4bit inference with GPTQ-for-LLaMa @link . See docs/gptq.md(/docs/gptq.md). - FastChat supports AWQ 4bit inference with mit-han-lab/llm-awq @link . See docs/awq.md(/docs/awq.md). - MLC LLM @link , backed by TVM Unity @link  compiler, deploys Vicuna natively on phones, consumer-class GPUs and web browsers via Vulkan, Metal, CUDA and WebGPU.",2
4177,615882673,4,Python,"Use models from modelscope. For Chinese users, you can use models from www.modelscope.cn via specify the following environment variables. @Code",2
4178,615882673,2,Python,"Serving with Web GUI. To serve using the web UI, you need three main components: web servers that interface with users, model workers that host one or more models, and a controller to coordinate the webserver and model workers. You can learn more about the architecture here(docs/server_arch.md). Here are the commands to follow in your terminal:",2
4179,615882673,4,Python,Launch the controller. @Code This controller manages the distributed workers.,2
4180,615882673,4,Python,"Launch the model worker(s). @Code Wait until the process finishes loading the model and you see 'Uvicorn running on ...'. The model worker will register itself to the controller . To ensure that your model worker is connected to your controller properly, send a test message using the following command: @Code You will see a short output.",2
4181,615882673,4,Python,"Launch the Gradio web server. @Code This is the user interface that users will interact with. By following these steps, you will be able to serve your models using the web UI. You can open your browser and chat with a model now. If the models do not show up, try to reboot the gradio web server.",2
4182,615882673,4,Python,"(Optional): Advanced Features, Scalability, Third Party UI. - You can register multiple model workers to a single controller, which can be used for serving a single model with higher throughput or serving multiple models at the same time. When doing so, please allocate different GPUs and ports for different model workers. @Code - You can also launch a multi-tab gradio server, which includes the Chatbot Arena tabs. @Code - The default model worker based on huggingface/transformers has great compatibility but can be slow. If you want high-throughput batched serving, you can try vLLM integration(docs/vllm_integration.md). - If you want to host it on your own UI or third party UI, see Third Party UI(docs/third_party_ui.md).",2
4183,615882673,3,Python,"OpenAI-Compatible RESTful APIs & SDK. FastChat provides OpenAI-compatible APIs for its supported models, so you can use FastChat as a local drop-in replacement for OpenAI APIs. The FastChat server is compatible with both openai-python @link  library and cURL commands. The REST API is capable of being executed from Google Colab free tier, as demonstrated in the FastChat_API_GoogleColab.ipynb @link  notebook, available in our repository. See docs/openai_api.md(docs/openai_api.md).",1
4184,615882673,3,Python,Hugging Face Generation APIs. See fastchat/serve/huggingface_api.py(fastchat/serve/huggingface_api.py).,1
4185,615882673,3,Python,LangChain Integration. See docs/langchain_integration(docs/langchain_integration.md).,1
4186,615882673,2,Python,"Evaluation. We use MT-bench, a set of challenging multi-turn open-ended questions to evaluate models. To automate the evaluation process, we prompt strong LLMs like GPT-4 to act as judges and assess the quality of the models' responses. See instructions for running MT-bench at fastchat/llm_judge(fastchat/llm_judge). MT-bench is the new recommended way to benchmark your models. If you are still looking for the old 80 questions used in the vicuna blog post, please go to vicuna-blog-eval @link .",2
4187,615882673,3,Python,"Data. Vicuna is created by fine-tuning a Llama base model using approximately 125K user-shared conversations gathered from ShareGPT.com with public APIs. To ensure data quality, we convert the HTML back to markdown and filter out some inappropriate or low-quality samples. Additionally, we divide lengthy conversations into smaller segments that fit the model's maximum context length. For detailed instructions to clean the ShareGPT data, check out here(docs/commands/data_cleaning.md). We will not release the ShareGPT dataset. If you would like to try the fine-tuning code, you can run it with some dummy conversations in dummy_conversation.json(data/dummy_conversation.json). You can follow the same format and plug in your own data.",2
4188,615882673,3,Python,Code and Hyperparameters. Our code is based on Stanford Alpaca @link  with additional support for multi-turn conversations. We use similar hyperparameters as the Stanford Alpaca.,2
4189,615882673,3,Python,"Fine-tuning Vicuna-7B with Local GPUs. - Install dependency @Code - You can use the following command to train Vicuna-7B with 4 x A100 (40GB). Update --model_name_or_path with the actual path to Llama weights and --data_path with the actual path to data. @Code Tips: - If you are using V100 which is not supported by FlashAttention, you can use the memory-efficient attention @link  implemented in xFormers @link . Install xformers and replace fastchat/train/train_mem.py above with fastchat/train/train_xformers.py(fastchat/train/train_xformers.py). - If you meet out-of-memory due to 'FSDP Warning: When using FSDP, it is efficient and recommended... ', see solutions here @link .. - If you meet out-of-memory during model saving, see solutions here @link . - To turn on logging to popular experiment tracking tools such as Tensorboard, MLFlow or Weights & Biases, use the report_to argument, e.g. pass --report_to wandb to turn on logging to Weights & Biases.",2
4190,615882673,3,Python,"Other models, platforms and LoRA support. More instructions to train other models (e.g., FastChat-T5) and use LoRA are in docs/training.md(docs/training.md).",2
4191,615882673,3,Python,"Fine-tuning on Any Cloud with SkyPilot. SkyPilot @link  is a framework built by UC Berkeley for easily and cost effectively running ML workloads on any cloud (AWS, GCP, Azure, Lambda, etc.). Find SkyPilot documentation here @link  on using managed spot instances to train Vicuna and save on your cloud costs.",2
4192,615882673,2,Python,"Citation. The code (training, serving, and evaluation) in this repository is mostly developed for or derived from the paper below. Please cite it if you find the repository helpful. @Code We are also planning to add more of our research to this repository.",3
4193,620936652,2,Python,"What's New. - Added gpt-4o, simply use gpt-4o in chat.completion.create. - Installation Guide for Windows (.exe):  installation-guide-for-windows(installation-guide-for-windows-exe). - Join our Telegram Channel:  telegram.me/g4f_channel @link - Join our Discord Group:  discord.gg/XfybzPXPH5 @link - g4f now supports 100% local inference:  local-docs @link",1
4194,620936652,2,Python,Site Takedown. Is your site on this repository and you want to take it down? Send an email to takedown@g4f.ai with proof it is yours and it will be removed as fast as possible. To prevent reproduction please secure your API.,3
4195,620936652,2,Python,"Feedback and Todo. You can always leave some feedback here:  @link As per the survey, here is a list of improvements to come - x Update the repository to include the new openai library syntax (ex: Openai() class) -   Golang implementation -    Improve Documentation (in /docs & Guides, Howtos, & Do video tutorials) - x Improve the provider status list & updates -   Tutorials on how to reverse sites to write your own wrapper (PoC only ofc) - x Improve the Bing wrapper. (Wait and Retry or reuse conversation) -    Write a standard provider performance test to improve the stability -   Potential support and development of local models -    Improve compatibility and error handling",3
4196,620936652,2,Python,Table of Contents. -  What's New(-whats-new). -  Table of Contents(-table-of-contents). -  Getting Started(-getting-started). Docker Container Guide(docker-container-guide). Installation Guide for Windows (.exe)(installation-guide-for-windows-exe). Use python(use-python). - Prerequisites(prerequisites). - Install using PyPI package:(install-using-pypi-package). - Install from source:(install-from-source). - Install using Docker:(install-using-docker). -  Usage(-usage). Text Generation(text-generation). Image Generation(image-generation). Web UI(web-ui). Interference API(interference-api). Configuration(configuration). -  Providers and Models(-providers-and-models). GPT-4(gpt-4). GPT-3.5(gpt-35). Other(other). Models(models). -  Powered by gpt4free(-powered-by-gpt4free). -  Contribute(-contribute). How do i create a new Provider?(guide-how-do-i-create-a-new-provider). How can AI help me with writing code?(guide-how-can-ai-help-me-with-writing-code). -  Contributors(-contributors). -  Copyright(-copyright). -  Star History(-star-history). -  License(-license).,1
4199,620936652,5,Python,"Getting Started Quickly:. 1. Install Docker: Begin by downloading and installing Docker @link . 2. Set Up the Container: Use the following commands to pull the latest image and start the container: @Code 3. Access the Client: - To use the included client, navigate to:  @link @link - Or set the API base for your client to:  @link @link 4. (Optional) Provider Login: If required, you can access the container's desktop here:  @link for provider login purposes.",2
4200,620936652,4,Python,"Installation Guide for Windows (.exe). To ensure the seamless operation of our application, please follow the instructions below. These steps are designed to guide you through the installation process on Windows operating systems.",2
4201,620936652,3,Python,"Installation Steps. 1. Download the Application: Visit our releases page @link  and download the most recent version of the application, named g4f.exe.zip. 2. File Placement: After downloading, locate the .zip file in your Downloads folder. Unpack it to a directory of your choice on your system, then execute the g4f.exe file to run the app. 3. Open GUI: The app starts a web server with the GUI. Open your favorite browser and navigate to  @link to access the application interface. 4. Firewall Configuration (Hotfix): Upon installation, it may be necessary to adjust your Windows Firewall settings to allow the application to operate correctly. To do this, access your Windows Firewall settings and allow the application. By following these steps, you should be able to successfully install and run the application on your Windows system. If you encounter any issues during the installation process, please refer to our Issue Tracker or try to get contact over Discord for assistance. Run the Webview UI on other Platfroms: - /docs/guides/webview @link",2
4202,620936652,5,Python,Use your smartphone:. Run the Web UI on Your Smartphone: - /docs/guides/phone @link,2
4203,620936652,5,Python,Prerequisites:. 1. Download and install Python @link  (Version 3.10 is recommended). 2. Install Google Chrome @link  for providers with webdriver,2
4204,620936652,5,Python,Install using PyPI package:. @Code How do I install only parts or do disable parts? Use partial requirements: /docs/requirements @link,2
4205,620936652,5,Python,Install from source:. How do I load the project using git and installing the project requirements? Read this tutorial and follow it step by step: /docs/git @link,2
4206,620936652,5,Python,Install using Docker:. How do I build and run composer image from source? Use docker-compose: /docs/docker @link,2
4207,620936652,4,Python,Image Generation. @Code @link Full Documentation for Python API - New AsyncClient API from G4F: /docs/async_client @link - Client API like the OpenAI Python library: /docs/client @link - Legacy API with python modules: /docs/legacy @link,2
4208,620936652,4,Python,"Web UI. To start the web interface, type the following codes in python: @Code or execute the following command: @Code",2
4209,620936652,4,Python,Interference API. You can use the Interference API to serve other OpenAI integrations with G4F. See docs: /docs/interference @link Access with:  @link,2
4210,620936652,4,Python,"Cookies. Cookies are essential for using Meta AI and Microsoft Designer to create images. Additionally, cookies are required for the Google Gemini and WhiteRabbitNeo Provider. From Bing, ensure you have the '_U' cookie, and from Google, all cookies starting with '__Secure-1PSID' are needed. You can pass these cookies directly to the create function or set them using the set_cookies method before running G4F: @Code",2
4211,620936652,4,Python,"Using .har and Cookie Files. You can place .har and cookie files in the default ./har_and_cookies directory. To export a cookie file, use the EditThisCookie Extension @link  available on the Chrome Web Store.",2
4212,620936652,4,Python,"Creating .har Files to Capture Cookies. To capture cookies, you can also create .har files. For more details, refer to the next section.",2
4213,620936652,4,Python,"Changing the Cookies Directory and Loading Cookie Files in Python. You can change the cookies directory and load cookie files in your Python environment. To set the cookies directory relative to your Python file, use the following code: @Code",2
4214,620936652,3,Python,"Debug Mode. If you enable debug mode, you will see logs similar to the following: @Code",2
4215,620936652,4,Python,.HAR File for OpenaiChat Provider.,2
4216,620936652,5,Python,"Generating a .HAR File. To utilize the OpenaiChat provider, a .har file is required from  @link Follow the steps below to create a valid .har file: 1. Navigate to  @link using your preferred web browser and log in with your credentials. 2. Access the Developer Tools in your browser. This can typically be done by right-clicking the page and selecting 'Inspect,' or by pressing F12 or CtrlShiftI (CmdOptionI on a Mac). 3. With the Developer Tools open, switch to the 'Network' tab. 4. Reload the website to capture the loading process within the Network tab. 5. Initiate an action in the chat which can be captured in the .har file. 6. Right-click any of the network activities listed and select 'Save all as HAR with content' to export the .har file.",2
4217,620936652,5,Python,"Storing the .HAR File. - Place the exported .har file in the ./har_and_cookies directory if you are using Docker. Alternatively, you can store it in any preferred location within your current working directory. Note: Ensure that your .har file is stored securely, as it may contain sensitive information.",2
4218,620936652,4,Python,"Using Proxy. If you want to hide or change your IP address for the providers, you can set a proxy globally via an environment variable: - On macOS and Linux: @Code - On Windows: @Code",2
4220,620936652,2,Python,"Best OpenSource Models. While we wait for gpt-5, here is a list of new models that are at least better than gpt-3.5-turbo. Some are better than gpt-4. Expect this list to grow.",1
4222,620936652,2,Python,Powered by gpt4free. Projects Stars Forks Issues Pull requests gpt4free gpt4free-ts Free AI API's & Potential Providers List ChatGPT-Clone Ai agent ChatGpt Discord Bot chatGPT-discord-bot Nyx-Bot (Discord) LangChain gpt4free ChatGpt Telegram Bot ChatGpt Line Bot Action Translate Readme Langchain Document GPT python-tgpt,3
4223,620936652,2,Python,"Contribute. We welcome contributions from the community. Whether you're adding new providers or features, or simply fixing typos and making small improvements, your input is valued. Creating a pull request is all it takes – our co-pilot will handle the code review process. Once all changes have been addressed, we'll merge the pull request into the main branch and release the updates at a later time.",3
4224,620936652,6,Python,Guide: How do i create a new Provider?. - Read: /docs/guides/create_provider @link,2
4225,620936652,6,Python,Guide: How can AI help me with writing code?. - Read: /docs/guides/help_me @link,2
4226,620936652,2,Python,Contributors. A list of all contributors is available here @link - The Vercel.py @link  file contains code from vercel-llm-api @link  by @ading2210 @link - The har_file.py @link  has input from xqdoo00o/ChatGPT-to-API @link - The PerplexityLabs.py @link  has input from nathanrchn/perplexityai @link - The Gemini.py @link  has input from dsdanielpark/Gemini-API @link - The MetaAI.py @link  file contains code from meta-ai-api @link  by @Strvm @link - The proofofwork.py @link  has input from missuo/FreeGPT35 @link Having input implies that the AI's code generation utilized it as one of many sources.,3
4227,620936652,2,Python,Copyright. This program is licensed under the GNU GPL v3 @link @Code,3
4229,620936652,2,Python,License. This project is licensed under GNU_GPL_v3.0. ( Back to top).,3
4230,634224458,1,Python,gpt-engineer. @link @link @link @link @link gpt-engineer lets you: - Specify software in natural language - Sit back and watch as an AI writes and executes the code - Ask the AI to implement improvements,1
4231,634224458,3,Python,Install gpt-engineer. For stable release: - python -m pip install gpt-engineer For development: - git clone  @link - cd gpt-engineer - poetry install - poetry shell to activate the virtual environment We actively support Python 3.10 - 3.12. The last version to support Python 3.8 - 3.9 was 0.2.6 @link .,2
4232,634224458,3,Python,"Setup API key. Choose one of: - Export env variable (you can add this to .bashrc so that you don't have to do it each time you start the terminal) - export OPENAI_API_KEYyour api key - .env file: - Create a copy of .env.template named .env - Add your OPENAI_API_KEY in .env - Custom model: - See docs @link , supports local model, azure, etc. Check the Windows README @link  for Windows usage. Other ways to run: - Use Docker (instructions(docker/README.md)) - Do everything in your browser: @link",2
4233,634224458,3,Python,Create new code (default usage). - Create an empty folder for your project anywhere on your computer - Create a file called prompt (no extension) inside your new folder and fill it with instructions - Run gpte  with a relative path to your folder - For example: gpte projects/my-new-project from the gpt-engineer directory root with your new folder in projects/,2
4234,634224458,3,Python,Improve existing code. - Locate a folder with code which you want to improve anywhere on your computer - Create a file called prompt (no extension) inside your new folder and fill it with instructions for how you want to improve the code - Run gpte  -i with a relative path to your folder - For example: gpte projects/my-old-project -i from the gpt-engineer directory root with your folder in projects/,2
4235,634224458,3,Python,"Benchmark custom agents. - gpt-engineer installs the binary 'bench', which gives you a simple interface for benchmarking your own agent implementations against popular public datasets. - The easiest way to get started with benchmarking is by checking out the template @link  repo, which contains detailed instructions and an agent template. - Currently supported benchmark: - APPS @link - MBPP @link By running gpt-engineer, you agree to our terms @link .",2
4236,634224458,2,Python,Relation to gptengineer.app (GPT Engineer). gptengineer.app @link  is a commercial project for the automatic generation of web apps. It features a UI for non-technical users connected to a git-controlled codebase. The gptengineer.app team is actively supporting the open source community.,1
4237,634224458,3,Python,Pre Prompts. You can specify the 'identity' of the AI agent by overriding the preprompts folder with your own version of the preprompts. You can do so via the --use-custom-preprompts argument. Editing the preprompts is how you make the agent remember things between projects.,1
4238,634224458,3,Python,"Vision. By default, gpt-engineer expects text input via a prompt file. It can also accept image inputs for vision-capable models. This can be useful for adding UX or architecture diagrams as additional context for GPT Engineer. You can do this by specifying an image directory with the —-image_directory flag and setting a vision-capable model in the second CLI argument. E.g. gpte projects/example-vision gpt-4-vision-preview --prompt_file prompt/text --image_directory prompt/images -i",1
4239,634224458,3,Python,"Open source, local and alternative models. By default, gpt-engineer supports OpenAI Models via the OpenAI API or Azure OpenAI API, as well as Anthropic models. With a little extra setup, you can also run with open source models like WizardCoder. See the documentation @link  for example instructions.",1
4240,634224458,2,Python,"Mission. The gpt-engineer community mission is to maintain tools that coding agent builders can use and facilitate collaboration in the open source community. If you are interested in contributing to this, we are interested in having you. If you want to see our broader ambitions, check out the roadmap @link , and join discord @link to learn how you can contribute(.github/CONTRIBUTING.md) to it. gpt-engineer is governed @link  by a board of long-term contributors. If you contribute routinely and have an interest in shaping the future of gpt-engineer, you will be considered for the board.",3
4241,63476337,2,Python,Getting Started. Read through our Contribution Guidelines(CONTRIBUTING.md) before you contribute.,3
4242,63476337,2,Python,Community Channels. We are on Discord @link  and Gitter @link ! Community channels are a great way for you to ask questions and get help. Please join us!,3
4243,63476337,2,Python,List of Algorithms. See our directory(DIRECTORY.md) for easier navigation and a better overview of the project.,2
4244,635240594,1,Python,"PrivateGPT . @link @link @link @link Install & usage docs:  @link Join the community: Twitter @link  & Discord @link PrivateGPT is a production-ready AI project that allows you to ask questions about your documents using the power of Large Language Models (LLMs), even in scenarios without an Internet connection. 100% private, no data leaves your execution environment at any point. The project provides an API offering all the primitives required to build private, context-aware AI applications. It follows and extends the OpenAI API standard @link , and supports both normal and streaming responses. The API is divided into two logical blocks: High-level API, which abstracts all the complexity of a RAG (Retrieval Augmented Generation) pipeline implementation: - Ingestion of documents: internally managing document parsing, splitting, metadata extraction, embedding generation and storage. - Chat & Completions using context from ingested documents: abstracting the retrieval of context, the prompt engineering and the response generation. Low-level API, which allows advanced users to implement their own complex pipelines: - Embeddings generation: based on a piece of text. - Contextual chunks retrieval: given a query, returns the most relevant chunks of text from the ingested documents. In addition to this, a working Gradio UI @link client is provided to test the API, together with a set of useful tools such as bulk model download script, ingestion script, documents folder watch, etc. Need help applying PrivateGPT to your specific use case? Let us know more about it @link and we'll try to help! We are refining PrivateGPT through your feedback.",1
4245,635240594,2,Python,Overview. DISCLAIMER: This README is not updated as frequently as the documentation @link . Please check it out for the latest updates!,1
4246,635240594,3,Python,"Motivation behind PrivateGPT. Generative AI is a game changer for our society, but adoption in companies of all sizes and data-sensitive domains like healthcare or legal is limited by a clear concern: privacy. Not being able to ensure that your data is fully under your control when using third-party AI tools is a risk those industries cannot take.",1
4247,635240594,3,Python,"Primordial version. The first version of PrivateGPT was launched in May 2023 as a novel approach to address the privacy concerns by using LLMs in a complete offline way. That version, which rapidly became a go-to project for privacy-sensitive setups and served as the seed for thousands of local-focused generative AI projects, was the foundation of what PrivateGPT is becoming nowadays; thus a simpler and more educational implementation to understand the basic concepts required to build a fully local -and therefore, private- chatGPT-like tool. If you want to keep experimenting with it, we have saved it in the primordial branch @link  of the project. It is strongly recommended to do a clean clone and install of this new version of PrivateGPT if you come from the previous, primordial version.",1
4248,635240594,3,Python,"Present and Future of PrivateGPT. PrivateGPT is now evolving towards becoming a gateway to generative AI models and primitives, including completions, document ingestion, RAG pipelines and other low-level building blocks. We want to make it easier for any developer to build AI applications and experiences, as well as provide a suitable extensive architecture for the community to keep contributing. Stay tuned to our releases @link  to check out all the new features and changes included.",1
4249,635240594,2,Python,"Documentation. Full documentation on installation, dependencies, configuration, running the server, deployment options, ingesting local documents, API details and UI features can be found here:  @link",2
4250,635240594,2,Python,"Architecture. Conceptually, PrivateGPT is an API that wraps a RAG pipeline and exposes its primitives. The API is built using FastAPI @link  and follows OpenAI's API scheme @link . The RAG pipeline is based on LlamaIndex @link . The design of PrivateGPT allows to easily extend and adapt both the API and the RAG implementation. Some key architectural decisions are: Dependency Injection, decoupling the different components and layers. Usage of LlamaIndex abstractions such as LLM, BaseEmbedding or VectorStore, making it immediate to change the actual implementations of those abstractions. Simplicity, adding as few layers and new abstractions as possible. Ready to use, providing a full implementation of the API and RAG pipeline. Main building blocks: APIs are defined in private_gpt:server:. Each package contains an _router.py (FastAPI layer) and an _service.py (the service implementation). Each Service uses LlamaIndex base abstractions instead of specific implementations, decoupling the actual implementation from its usage. Components are placed in private_gpt:components:. Each Component is in charge of providing actual implementations to the base abstractions used in the Services - for example LLMComponent is in charge of providing an actual implementation of an LLM (for example LlamaCPP or OpenAI).",1
4251,635240594,2,Python,"Contributing. Contributions are welcomed! To ensure code quality we have enabled several format and typing checks, just run make check before committing to make sure your code is ok. Remember to test your code! You'll find a tests folder with helpers, and you can run tests using make test command. Don't know what to contribute? Here is the public Project Board @link  with several ideas. Head over to Discord contributors channel and ask for write permissions on that GitHub project..",3
4252,635240594,2,Python,Community. Join the conversation around PrivateGPT on our: - Twitter (aka X) @link - Discord @link,3
4253,635240594,2,Python,"Citation. If you use PrivateGPT in a paper, check out the Citation file(CITATION.cff) for the correct citation. You can also use the 'Cite this repository' button in this repo to get the citation in different formats. Here are a couple of examples:",3
4254,635240594,2,Python,"Partners & Supporters. PrivateGPT is actively supported by the teams behind: Qdrant @link , providing the default vector database Fern @link , providing Documentation and SDKs LlamaIndex @link , providing the base RAG framework and abstractions This project has been strongly influenced and supported by other amazing projects like LangChain @link , GPT4All @link , LlamaCpp @link , Chroma @link and SentenceTransformers @link .",3
4255,638629097,1,Python,"Aider is AI pair programming in your terminal . Aider lets you pair program with LLMs, to edit code in your local git repository. Start a new project or work with an existing git repo. Aider works best with GPT-4o & Claude 3.5 Sonnet and can connect to almost any LLM @link .",1
4256,638629097,2,Python,Getting started . You can get started quickly like this: @Code See the installation instructions @link and other documentation @link for more details.,2
4257,638629097,2,Python,"Features . - Run aider with the files you want to edit: aider   ... - Ask for changes: - Add new features or test cases. - Describe a bug. - Paste in an error message or or GitHub issue URL. - Refactor code. - Update docs. - Aider will edit your files to complete your request. - Aider automatically git commits @link  changes with a sensible commit message. - Aider works with most popular languages @link : python, javascript, typescript, php, html, css, and more... - Aider works best with GPT-4o & Claude 3.5 Sonnet and can connect to almost any LLM @link . - Aider can edit multiple files at once for complex requests. - Aider uses a map of your entire git repo @link , which helps it work well in larger codebases. - Edit files in your editor while chatting with aider, and it will always use the latest version. Pair program with AI. - Add images to the chat @link  (GPT-4o, Claude 3.5 Sonnet, etc). - Add URLs to the chat @link  and aider will read their content. - Code with your voice @link .",1
4258,638629097,2,Python,"Top tier performance . Aider has the one of the top scores on SWE Bench @link . SWE Bench is a challenging software engineering benchmark where aider solved real GitHub issues from popular open source projects like django, scikitlearn, matplotlib, etc.",2
4259,638629097,2,Python,More info . - Documentation @link - Installation @link - Usage @link - Tutorial videos @link - Connecting to LLMs @link - Configuration @link - Troubleshooting @link - LLM Leaderboards @link - GitHub @link - Discord @link - Blog @link,3
4260,638629097,2,Python,"Kind words from users . - The best free open source AI coding assistant. -- IndyDevDan @link - The best AI coding assistant so far. -- Matthew Berman @link - Aider ... has easily quadrupled my coding productivity. -- SOLAR_FIELDS @link - It's a cool workflow... Aider's ergonomics are perfect for me. -- qup @link - It's really like having your senior developer live right in your Git repo - truly amazing! -- rappster @link - What an amazing tool. It's incredible. -- valyagolev @link . - Aider is such an astounding thing! -- cgrothaus @link . - It was WAY faster than I would be getting off the ground and making the first few working versions. -- Daniel Feldman @link - THANK YOU for Aider! It really feels like a glimpse into the future of coding. -- derwiki @link - It's just amazing.  It is freeing me to do things I felt were out my comfort zone before. -- Dougie @link - This project is stellar. -- funkytaco @link . - Amazing project, definitely the best AI coding assistant I've used. -- joshuavial @link - I absolutely love using Aider ... It makes software development feel so much lighter as an experience. -- principalideal0 @link - I have been recovering from multiple shoulder surgeries ... and have used aider extensively. It has allowed me to continue productivity. -- codeninja @link - I am an aider addict. I'm getting so much more work done, but in less time. -- dandandan @link - After wasting $100 on tokens trying to find something better, I'm back to Aider. It blows everything else out of the water hands down, there's no competition whatsoever. -- SystemSculpt @link - Aider is amazing, coupled with Sonnet 3.5 it’s quite mind blowing. -- Josh Dingus @link - Hands down, this is the best AI coding assistant tool so far. -- IndyDevDan @link - Aider changed my daily coding workflows. It's mind-blowing how a single Python application can change your life. -- maledorak @link - Best agent for actual dev work in existing codebases. -- Nick Dobos @link",1
4261,640079149,1,Python,"Quivr - Your Second Brain, Empowered by Generative AI. @link @link @link Quivr, your second brain, utilizes the power of GenerativeAI to be your personal assistant ! Think of it as Obsidian, but turbocharged with AI capabilities. Roadmap here @link",1
4262,640079149,2,Python,"Key Features . - Fast and Efficient: Designed with speed and efficiency at its core. Quivr ensures rapid access to your data. - Secure: Your data, your control. Always. - OS Compatible: Ubuntu 22 or newer. - File Compatibility: Text, Markdown, PDF, Powerpoint, Excel, CSV, Word, Audio, Video - Open Source: Freedom is beautiful, and so is Quivr. Open source and free to use. - Public/Private: Share your brains with your users via a public link, or keep them private. - Marketplace: Share your brains with the world, or use other people's brains to boost your productivity. - Offline Mode: Quivr works offline, so you can access your data anytime, anywhere.",1
4264,640079149,2,Python,"Getting Started . You can deploy Quivr to Porter Cloud with one-click: If you would like to deploy locally, follow these instructions to get a copy of the project up and running on your local machine for development and testing purposes. You can find everything on the documentation @link .",2
4265,640079149,3,Python,Prerequisites . Ensure you have the following installed: - Docker - Docker Compose,2
4266,640079149,3,Python,"60 seconds Installation . You can find the installation video here @link . - Step 0: Supabase CLI Follow the instructions here @link  to install the Supabase CLI that is required. @Code - Step 1: Clone the repository: @Code - Step 2: Copy the .env.example files @Code - Step 3: Update the .env files @Code Update OPENAI_API_KEY in the .env file. You just need to update the OPENAI_API_KEY variable in the .env file. You can get your API key here @link . You need to create an account first. And put your credit card information. Don't worry, you won't be charged unless you use the API. You can find more information about the pricing here @link . - Step 4: Launch the project @Code and then @Code If you have a Mac, go to Docker Desktop  Settings  General and check that the 'file sharing implementation' is set to VirtioFS. If you are a developer, you can run the project in development mode with the following command: docker compose -f docker-compose.dev.yml up --build - Step 5: Login to the app You can now sign in to the app with admin@quivr.app & admin. You can access the app at  @link @link . You can access Quivr backend API at  @link @link You can access supabase at  @link @link",2
4267,640079149,2,Python,Updating Quivr . - Step 1: Pull the latest changes @Code - Step 2: Update the migration @Code,2
4268,640079149,2,Python,Contributors . Thanks go to these wonderful people:,3
4269,640079149,2,Python,"Contribute . Did you get a pull request? Open it, and we'll review it as soon as possible. Check out our project board here @link  to see what we're currently focused on, and feel free to bring your fresh ideas to the table! - Open Issues @link - Open Pull Requests @link - Good First Issues @link - Frontend Issues @link - Backend Issues @link - Translate @link .",3
4270,640079149,2,Python,Partners . This project would not be possible without the support of our partners. Thank you for your support!,3
4271,640079149,2,Python,License . This project is licensed under the Apache 2.0 License - see the LICENSE(LICENSE) file for details,3
4273,640317662,1,Python,Vanna . Vanna is an MIT-licensed open-source Python RAG (Retrieval-Augmented Generation) framework for SQL generation and related functionality. @link,1
4274,640317662,2,Python,"How Vanna works . Vanna works in two easy steps - train a RAG 'model' on your data, and then ask questions which will return SQL queries that can be set up to automatically run on your database. 1. Train a RAG 'model' on your data. 2. Ask questions. !(img/vanna-readme-diagram.png) If you don't know what RAG is, don't worry -- you don't need to know how this works under the hood to use it. You just need to know that you 'train' a model, which stores some metadata and then use it to 'ask' questions. See the base class @link  for more details on how this works under the hood.",2
4275,640317662,2,Python,User Interfaces . These are some of the user interfaces that we've built using Vanna. You can use these as-is or as a starting point for your own custom interface. - Jupyter Notebook @link - vanna-ai/vanna-streamlit @link - vanna-ai/vanna-flask @link - vanna-ai/vanna-slack @link,2
4276,640317662,2,Python,"Getting started . See the documentation @link  for specifics on your desired database, LLM, etc. If you want to get a feel for how it works after training, you can try this Colab notebook @link .",2
4277,640317662,3,Python,Install . @Code There are a number of optional packages that can be installed so see the documentation @link  for more details.,2
4278,640317662,3,Python,Import . See the documentation @link  if you're customizing the LLM or vector database. @Code,2
4279,640317662,2,Python,Training . You may or may not need to run these vn.train commands depending on your use case. See the documentation @link  for more details. These statements are shown to give you a feel for how it works.,2
4280,640317662,3,Python,"Train with DDL Statements . DDL statements contain information about the table names, columns, data types, and relationships in your database. @Code",2
4281,640317662,3,Python,Train with Documentation . Sometimes you may want to add documentation about your business terminology or definitions. @Code,2
4282,640317662,3,Python,Train with SQL . You can also add SQL queries to your training data. This is useful if you have some queries already laying around. You can just copy and paste those from your editor to begin generating new SQL. @Code,2
4283,640317662,2,Python,"Asking questions . @Code You'll get SQL @Code If you've connected to a database, you'll get the table: CUSTOMER_NAME TOTAL_SALES 0 Customer000143500 . 6757566.0218 1 Customer000095257 . 6294115.3340 2 Customer000087115 . 6184649.5176 3 Customer000131113 . 6080943.8305 4 Customer000134380 . 6075141.9635 5 Customer000103834 . 6059770.3232 6 Customer000069682 . 6057779.0348 7 Customer000102022 . 6039653.6335 8 Customer000098587 . 6027021.5855 9 Customer000064660 . 5905659.6159 You'll also get an automated Plotly chart: !(img/top-10-customers.png)",2
4284,640317662,2,Python,"RAG vs. Fine-Tuning . RAG - Portable across LLMs - Easy to remove training data if any of it becomes obsolete - Much cheaper to run than fine-tuning - More future-proof -- if a better LLM comes out, you can just swap it out Fine-Tuning - Good if you need to minimize tokens in the prompt - Slow to get started - Expensive to train and run (generally)",1
4285,640317662,2,Python,"Why Vanna? . 1. High accuracy on complex datasets. - Vanna’s capabilities are tied to the training data you give it - More training data means better accuracy for large and complex datasets 2. Secure and private. - Your database contents are never sent to the LLM or the vector database - SQL execution happens in your local environment 3. Self learning. - If using via Jupyter, you can choose to 'auto-train' it on the queries that were successfully executed - If using via other interfaces, you can have the interface prompt the user to provide feedback on the results - Correct question to SQL pairs are stored for future reference and make the future results more accurate 4. Supports any SQL database. - The package allows you to connect to any SQL database that you can otherwise connect to with Python 5. Choose your front end. - Most people start in a Jupyter Notebook. - Expose to your end users via Slackbot, web app, Streamlit app, or a custom front end.",1
4286,640317662,2,Python,"Extending Vanna . Vanna is designed to connect to any database, LLM, and vector database. There's a VannaBase @link  abstract base class that defines some basic functionality. The package provides implementations for use with OpenAI and ChromaDB. You can easily extend Vanna to use your own LLM or vector database. See the documentation @link  for more details.",1
4287,640317662,2,Python,Vanna in 100 Seconds . @link,1
4288,640317662,2,Python,More resources . - Full Documentation @link - Website @link - Discord group for support @link,3
4289,642323624,2,Python,"Requirements. If you have CUDA graphic card, please follow the requirements of NVlabs/stylegan3 @link .  . The usual installation steps involve the following commands, they should set up the correct CUDA version and all the python packages @Code Then install the additional requirements @Code Otherwise (for GPU acceleration on MacOS with Silicon Mac M1/M2, or just CPU) try the following: @Code",2
4290,642323624,2,Python,"Run Gradio visualizer in Docker . Provided docker image is based on NGC PyTorch repository. To quickly try out visualizer in Docker, run the following: @Code Now you can open a shared link from Gradio (printed in the terminal console). Beware the Docker image takes about 25GB of disk space!",2
4291,642323624,2,Python,"Download pre-trained StyleGAN2 weights. To download pre-trained weights, simply run: @Code If you want to try StyleGAN-Human and the Landscapes HQ (LHQ) dataset, please download weights from these links: StyleGAN-Human @link , LHQ @link , and put them under ./checkpoints. Feel free to try other pretrained StyleGAN.",2
4292,642323624,2,Python,"Run DragGAN GUI. To start the DragGAN GUI, simply run: @Code If you are using windows, you can run: @Code This GUI supports editing GAN-generated images. To edit a real image, you need to first perform GAN inversion using tools like PTI @link . Then load the new latent code and model weights to the GUI. You can run DragGAN Gradio demo as well, this is universal for both windows and linux: @Code",2
4293,642323624,2,Python,Acknowledgement. This code is developed based on StyleGAN3 @link . Part of the code is borrowed from StyleGAN-Human @link . (cheers to the community as well),3
4294,642323624,2,Python,"License. The code related to the DragGAN algorithm is licensed under CC-BY-NC @link . However, most of this project are available under a separate license terms: all codes used or modified from StyleGAN3 @link  is under the Nvidia Source Code License @link . Any form of use and derivative of this code must preserve the watermarking functionality showing 'AI Generated'.",3
4295,65600975,2,Python,"More About PyTorch. Learn the basics of PyTorch @link At a granular level, PyTorch is a library that consists of the following components: Usually, PyTorch is used either as: - A replacement for NumPy to use the power of GPUs. - A deep learning research platform that provides maximum flexibility and speed. Elaborating Further:",1
4296,65600975,3,Python,"A GPU-Ready Tensor Library. If you use NumPy, then you have used Tensors (a.k.a. ndarray). PyTorch provides Tensors that can live either on the CPU or the GPU and accelerates the computation by a huge amount. We provide a wide variety of tensor routines to accelerate and fit your scientific computation needs such as slicing, indexing, mathematical operations, linear algebra, reductions. And they are fast!",1
4297,65600975,3,Python,"Dynamic Neural Networks: Tape-Based Autograd. PyTorch has a unique way of building neural networks: using and replaying a tape recorder. Most frameworks such as TensorFlow, Theano, Caffe, and CNTK have a static view of the world. One has to build a neural network and reuse the same structure again and again. Changing the way the network behaves means that one has to start from scratch. With PyTorch, we use a technique called reverse-mode auto-differentiation, which allows you to change the way your network behaves arbitrarily with zero lag or overhead. Our inspiration comes from several research papers on this topic, as well as current and past work such as torch-autograd @link , autograd @link , Chainer @link , etc. While this technique is not unique to PyTorch, it's one of the fastest implementations of it to date. You get the best of speed and flexibility for your crazy research.",1
4298,65600975,3,Python,"Python First. PyTorch is not a Python binding into a monolithic C framework. It is built to be deeply integrated into Python. You can use it naturally like you would use NumPy @link  / SciPy @link  / scikit-learn @link  etc. You can write your new neural network layers in Python itself, using your favorite libraries and use packages such as Cython @link  and Numba @link . Our goal is to not reinvent the wheel where appropriate.",1
4299,65600975,3,Python,"Imperative Experiences. PyTorch is designed to be intuitive, linear in thought, and easy to use. When you execute a line of code, it gets executed. There isn't an asynchronous view of the world. When you drop into a debugger or receive error messages and stack traces, understanding them is straightforward. The stack trace points to exactly where your code was defined. We hope you never spend hours debugging your code because of bad stack traces or asynchronous and opaque execution engines.",1
4300,65600975,3,Python,"Fast and Lean. PyTorch has minimal framework overhead. We integrate acceleration libraries such as Intel MKL @link  and NVIDIA (cuDNN @link , NCCL @link ) to maximize speed. At the core, its CPU and GPU Tensor and neural network backends are mature and have been tested for years. Hence, PyTorch is quite fast — whether you run small or large neural networks. The memory usage in PyTorch is extremely efficient compared to Torch or some of the alternatives. We've written custom memory allocators for the GPU to make sure that your deep learning models are maximally memory efficient. This enables you to train bigger deep learning models than before.",1
4301,65600975,3,Python,"Extensions Without Pain. Writing new neural network modules, or interfacing with PyTorch's Tensor API was designed to be straightforward and with minimal abstractions. You can write new neural network layers in Python using the torch API or your favorite NumPy-based libraries such as SciPy @link . If you want to write your layers in C/C, we provide a convenient extension API that is efficient and with minimal boilerplate. No wrapper code needs to be written. You can see a tutorial here @link  and an example here @link .",1
4302,65600975,3,Python,Binaries. Commands to install binaries via Conda or pip wheels are on our website:  @link @link,2
4303,65600975,4,Python,"NVIDIA Jetson Platforms. Python wheels for NVIDIA's Jetson Nano, Jetson TX1/TX2, Jetson Xavier NX/AGX, and Jetson AGX Orin are provided here @link  and the L4T container is published here @link They require JetPack 4.2 and above, and @dusty-nv @link  and @ptrblck @link  are maintaining them.",2
4304,65600975,4,Python,"Prerequisites. If you are installing from source, you will need: - Python 3.8 or later (for Linux, Python 3.8.1 is needed) - A compiler that fully supports C17, such as clang or gcc (gcc 9.4.0 or newer is required) We highly recommend installing an Anaconda @link  environment. You will get a high-quality BLAS library (MKL) and you get controlled dependency versions regardless of your Linux distro.",2
4305,65600975,5,Python,"NVIDIA CUDA Support. If you want to compile with CUDA support, select a supported version of CUDA from our support matrix @link , then install the following: - NVIDIA CUDA @link - NVIDIA cuDNN @link  v8.5 or above - Compiler @link  compatible with CUDA Note: You could refer to the cuDNN Support Matrix @link  for cuDNN versions with the various supported CUDA, CUDA driver and NVIDIA hardware If you want to disable CUDA support, export the environment variable USE_CUDA0. Other potentially useful environment variables may be found in setup.py. If you are building for NVIDIA's Jetson platforms (Jetson Nano, TX1, TX2, AGX Xavier), Instructions to install PyTorch for Jetson Nano are available here @link",2
4306,65600975,5,Python,"AMD ROCm Support. If you want to compile with ROCm support, install - AMD ROCm @link  4.0 and above installation - ROCm is currently supported only for Linux systems. If you want to disable ROCm support, export the environment variable USE_ROCM0. Other potentially useful environment variables may be found in setup.py.",2
4307,65600975,5,Python,"Intel GPU Support. If you want to compile with Intel GPU support, follow these - PyTorch Prerequisites for Intel GPUs @link  instructions. - Intel GPU is supported for Linux and Windows. If you want to disable Intel GPU support, export the environment variable USE_XPU0. Other potentially useful environment variables may be found in setup.py.",2
4308,65600975,4,Python,Install Dependencies. Common @Code On Linux @Code On MacOS @Code On Windows @Code,2
4309,65600975,4,Python,Get the PyTorch Source. @Code,2
4310,65600975,4,Python,"Install PyTorch. On Linux If you would like to compile PyTorch with new C ABI @link  enabled, then first run this command: @Code If you're compiling for AMD ROCm then first run this command: @Code Install PyTorch @Code _Aside:_ If you are using Anaconda @link , you may experience an error caused by the linker:. @Code This is caused by ld from the Conda environment shadowing the system ld. You should use a newer version of Python that fixes this issue. The recommended Python version is 3.8.1. On macOS @Code On Windows Choose Correct Visual Studio Version. PyTorch CI uses Visual C BuildTools, which come with Visual Studio Enterprise, Professional, or Community Editions. You can also install the build tools from @link The build tools do not come with Visual Studio Code by default. If you want to build legacy python code, please refer to Building on legacy code and CUDA @link . CPU-only builds In this mode PyTorch computations will run on your CPU, not your GPU @Code Note on OpenMP: The desired OpenMP implementation is Intel OpenMP (iomp). In order to link against iomp, you'll need to manually download the library and set up the building environment by tweaking CMAKE_INCLUDE_PATH and LIB. The instruction here @link  is an example for setting up both MKL and Intel OpenMP. Without these configurations for CMake, Microsoft Visual C OpenMP runtime (vcomp) will be used.. CUDA based build In this mode PyTorch computations will leverage your GPU via CUDA for faster number crunching NVTX @link  is needed to build Pytorch with CUDA. NVTX is a part of CUDA distributive, where it is called 'Nsight Compute'. To install it onto an already installed CUDA run CUDA installation once again and check the corresponding checkbox. Make sure that CUDA with Nsight Compute is installed after Visual Studio. Currently, VS 2017 / 2019, and Ninja are supported as the generator of CMake. If ninja.exe is detected in PATH, then Ninja will be used as the default generator, otherwise, it will use VS 2017 / 2019. If Ninja is selected as the generator, the latest MSVC will get selected as the underlying toolchain. Additional libraries such as Magma @link , oneDNN, a.k.a. MKLDNN or DNNL @link , and Sccache @link  are often needed. Please refer to the installation-helper @link  to install them. You can refer to the build_pytorch.bat @link  script for some other environment variables configurations @Code",2
4311,65600975,5,Python,"Adjust Build Options (Optional). You can adjust the configuration of cmake variables optionally (without building first), by doing the following. For example, adjusting the pre-detected directories for CuDNN or BLAS can be done with such a step. On Linux @Code On macOS @Code",2
4312,65600975,4,Python,"Using pre-built images. You can also pull a pre-built docker image from Docker Hub and run with docker v19.03 @Code Please note that PyTorch uses shared memory to share data between processes, so if torch multiprocessing is used (e.g. for multithreaded data loaders) the default shared memory segment size that container runs with is not enough, and you should increase shared memory size either with --ipchost or --shm-size command line options to nvidia-docker run.",2
4313,65600975,4,Python,"Building the image yourself. NOTE: Must be built with a docker version  18.06 The Dockerfile is supplied to build images with CUDA 11.1 support and cuDNN v8. You can pass PYTHON_VERSIONx.y make variable to specify which Python version is to be used by Miniconda, or leave it unset to use the default. @Code You can also pass the CMAKE_VARS'...' environment variable to specify additional CMake variables to be passed to CMake during the build. See setup.py @link  for the list of available variables. @Code",2
4314,65600975,3,Python,"Building the Documentation. To build documentation in various formats, you will need Sphinx @link  and the readthedocs theme. @Code You can then build the documentation by running make  from the docs/ folder. Run make to get a list of all available output formats. If you get a katex error run npm install katex.  If it persists, try npm install -g katex Note: if you installed nodejs with a different package manager (e.g., conda) then npm will probably install a version of katex that is not compatible with your version of nodejs and doc builds will fail. A combination of versions that is known to work is node@6.13.1 and katex@0.13.18. To install the latter with npm you can run @Code",2
4315,65600975,3,Python,Previous Versions. Installation instructions and binaries for previous PyTorch versions may be found on our website @link .,2
4316,65600975,2,Python,Getting Started. Three-pointers to get you started: - Tutorials: get you started with understanding and using PyTorch @link - Examples: easy to understand PyTorch code across all domains @link - The API Reference @link - Glossary @link,2
4317,65600975,2,Python,Resources. PyTorch.org @link PyTorch Tutorials @link PyTorch Examples @link PyTorch Models @link Intro to Deep Learning with PyTorch from Udacity @link Intro to Machine Learning with PyTorch from Udacity @link Deep Neural Networks with PyTorch from Coursera @link PyTorch Twitter @link PyTorch Blog @link PyTorch YouTube @link,3
4318,65600975,2,Python,"Communication. Forums: Discuss implementations, research, etc.  @link  GitHub Issues: Bug reports, feature requests, install issues, RFCs, thoughts, etc. Slack: The PyTorch Slack @link  hosts a primary audience of moderate to experienced PyTorch users and developers for general chat, online discussions, collaboration, etc. If you are a beginner looking for help, the primary medium is PyTorch Forums @link . If you need a slack invite, please fill this form:  @link  Newsletter: No-noise, a one-way email newsletter with important announcements about PyTorch. You can sign-up here:  @link  Facebook Page: Important announcements about PyTorch.  @link  For brand guidelines, please visit our website at pytorch.org @link",3
4319,65600975,2,Python,"Releases and Contributing. Typically, PyTorch has three minor releases a year. Please let us know if you encounter a bug by filing an issue @link . We appreciate all contributions. If you are planning to contribute back bug-fixes, please do so without any further discussion. If you plan to contribute new features, utility functions, or extensions to the core, please first open an issue and discuss the feature with us. Sending a PR without discussion might end up resulting in a rejected PR because we might be taking the core in a different direction than you might be aware of. To learn more about making a contribution to Pytorch, please see our Contribution page(CONTRIBUTING.md). For more information about PyTorch releases, see Release page(RELEASE.md).",3
4320,65600975,2,Python,"The Team. PyTorch is a community-driven project with several skillful engineers and researchers contributing to it. PyTorch is currently maintained by Soumith Chintala @link , Gregory Chanan @link , Dmytro Dzhulgakov @link , Edward Yang @link , and Nikita Shulga @link  with major contributions coming from hundreds of talented individuals in various forms and means. A non-exhaustive but growing list needs to mention: Trevor Killeen, Sasank Chilamkurthy, Sergey Zagoruyko, Adam Lerer, Francisco Massa, Alykhan Tejani, Luca Antiga, Alban Desmaison, Andreas Koepf, James Bradbury, Zeming Lin, Yuandong Tian, Guillaume Lample, Marat Dukhan, Natalia Gimelshein, Christian Sarofeen, Martin Raison, Edward Yang, Zachary Devito. Note: This project is unrelated to hughperkins/pytorch @link  with the same name. Hugh is a valuable contributor to the Torch community and has helped with many things Torch and PyTorch.",3
4321,65600975,2,Python,"License. PyTorch has a BSD-style license, as found in the LICENSE(LICENSE) file.",3
4322,660551251,1,Python,MetaGPT: The Multi-Agent Framework. Assign different roles to GPTs to form a collaborative entity for complex tasks.,1
4323,660551251,2,Python,"News. Mar. 29, 2024: v0.8.0 @link  released. Now you can use Data Interpreter (arxiv @link , example @link , code @link ) via pypi package import. Meanwhile, we integrated RAG module and supported multiple new LLMs. Feb. 08, 2024: v0.7.0 @link  released, supporting assigning different LLMs to different Roles. We also introduced Data Interpreter @link , a powerful agent capable of solving a wide range of real-world problems. Jan. 16, 2024: Our paper MetaGPT: Meta Programming for A Multi-Agent Collaborative Framework @link  accepted for oral presentation (top 1.2%) at ICLR 2024, ranking 1 in the LLM-based Agent category.. Jan. 03, 2024: v0.6.0 @link  released, new features include serialization, upgraded OpenAI package and supported multiple LLM, provided minimal example for debate @link  etc. Dec. 15, 2023: v0.5.0 @link  released, introducing some experimental features such as incremental development, multilingual, multiple programming languages, etc. Nov. 08, 2023: MetaGPT is selected into Open100: Top 100 Open Source achievements @link . Sep. 01, 2023: MetaGPT tops GitHub Trending Monthly for the 17th time in August 2023. Jun. 30, 2023: MetaGPT is now open source. Apr. 24, 2023: First line of MetaGPT code committed.",1
4324,660551251,2,Python,"Software Company as Multi-Agent System. 1. MetaGPT takes a one line requirement as input and outputs user stories / competitive analysis / requirements / data structures / APIs / documents, etc. 2. Internally, MetaGPT includes product managers / architects / project managers / engineers. It provides the entire process of a software company along with carefully orchestrated SOPs. 1. Code  SOP(Team) is the core philosophy. We materialize SOP and apply it to teams composed of LLMs. Software Company Multi-Agent Schematic (Gradually Implementing)",1
4325,660551251,3,Python,"Installation. Ensure that Python 3.9 is installed on your system. You can check this by using: python --version. You can use conda like this: conda create -n metagpt python3.9 && conda activate metagpt @Code For detailed installation guidance, please refer to cli_install @link . or docker_install @link .",2
4326,660551251,3,Python,"Configuration. You can init the config of MetaGPT by running the following command, or manually create /.metagpt/config2.yaml file: @Code You can configure /.metagpt/config2.yaml according to the example @link  and doc @link : @Code",2
4327,660551251,3,Python,"Usage. After installation, you can use MetaGPT at CLI @Code or use it as library @Code You can also use Data Interpreter @link  to write code: @Code",2
4328,660551251,3,Python,QuickStart & Demo Video. - Try it on MetaGPT Huggingface Space @link - Matthew Berman: How To Install MetaGPT - Build A Startup With One Prompt!! @link - Official Demo Video @link @link,2
4329,660551251,2,Python,Tutorial. -  Online Document @link -  Usage @link -  What can MetaGPT do? @link -  How to build your own agents? - MetaGPT Usage & Development Guide   - MetaGPT Usage & Development Guide -  Contribution - Develop Roadmap(docs/ROADMAP.md) -  Use Cases - Data Interpreter @link - Debate @link - Researcher @link - Recepit Assistant @link -  FAQs @link,3
4330,660551251,3,Python,Discord Join US. Join Our Discord Channel @link ! Looking forward to seeing you there!,3
4331,660551251,3,Python,Contributor form. Fill out the form @link  to become a contributor. We are looking forward to your participation!,3
4332,660551251,3,Python,"Contact Information. If you have any questions or feedback about this project, please feel free to contact us. We highly appreciate your suggestions! - Email: alexanderwu@deepwisdom.ai - GitHub Issues: For more technical inquiries, you can also create a new issue in our GitHub repository @link . We will respond to all questions within 2-3 business days.",3
4333,660551251,2,Python,"Citation. To stay updated with the latest research and development, follow @MetaGPT_ @link  on Twitter. To cite MetaGPT @link  or Data Interpreter @link  in publications, please use the following BibTeX entries. @Code",3
4334,666299222,2,Python,"Demo. @link $2 An interactive demo is also available on Google Colab:. @link $2 Along with an example voice interface, inspired by _Her_:. @link",2
4335,666299222,3,Python,"Terminal. After installation, simply run interpreter: @Code",2
4336,666299222,3,Python,"GitHub Codespaces. Press the , key on this repository's GitHub page to create a codespace. After a moment, you'll receive a cloud virtual machine environment pre-installed with open-interpreter. You can then start interacting with it directly and freely confirm its execution of system commands without worrying about damaging the system.",2
4337,666299222,2,Python,"Comparison to ChatGPT's Code Interpreter. OpenAI's release of Code Interpreter @link  with GPT-4 presents a fantastic opportunity to accomplish real-world tasks with ChatGPT.. However, OpenAI's service is hosted, closed-source, and heavily restricted: - No internet access. - Limited set of pre-installed packages @link . - 100 MB maximum upload, 120.0 second runtime limit. - State is cleared (along with any generated files or links) when the environment dies. --- Open Interpreter overcomes these limitations by running in your local environment. It has full access to the internet, isn't restricted by time or file size, and can utilize any package or library. This combines the power of GPT-4's Code Interpreter with the flexibility of your local development environment.",1
4338,666299222,2,Python,Commands. Update: The Generator Update (0.1.5) introduced streaming: @Code,2
4339,666299222,3,Python,"Interactive Chat. To start an interactive chat in your terminal, either run interpreter from the command line: @Code Or interpreter.chat() from a .py file: @Code You can also stream each chunk: @Code",2
4340,666299222,3,Python,"Programmatic Chat. For more precise control, you can pass messages directly to .chat(message): @Code",2
4341,666299222,3,Python,"Start a New Chat. In Python, Open Interpreter remembers conversation history. If you want to start fresh, you can reset it: @Code",2
4342,666299222,3,Python,"Save and Restore Chats. interpreter.chat() returns a List of messages, which can be used to resume a conversation with interpreter.messages  messages: @Code",2
4343,666299222,3,Python,"Customize System Message. You can inspect and configure Open Interpreter's system message to extend its functionality, modify permissions, or give it more context. @Code",2
4344,666299222,3,Python,"Change your Language Model. Open Interpreter uses LiteLLM @link  to connect to hosted language models. You can change the model by setting the model parameter: @Code In Python, set the model on the object: @Code Find the appropriate 'model' string for your language model here. @link",2
4345,666299222,3,Python,Running Open Interpreter locally.,2
4346,666299222,4,Python,"Terminal. Open Interpreter can use OpenAI-compatible server to run models locally. (LM Studio, jan.ai, ollama etc) Simply run interpreter with the api_base URL of your inference server (for LM studio it is  @link by default): @Code Alternatively you can use Llamafile without installing any third party software just by running @Code for a more detailed guide check out this video by Mike Bird @link How to run LM Studio in the background. 1. Download  @link @link  then start it. 2. Select a model then click  Download. 3. Click the  button on the left (below ). 4. Select your model at the top, then click Start Server. Once the server is running, you can begin your conversation with Open Interpreter. Note: Local mode sets your context_window to 3000, and your max_tokens to 1000. If your model has different requirements, set these parameters manually (see below).",2
4347,666299222,4,Python,"Python. Our Python package gives you more control over each setting. To replicate and connect to LM Studio, use these settings: @Code",2
4348,666299222,4,Python,"Context Window, Max Tokens. You can modify the max_tokens and context_window (in tokens) of locally running models. For local mode, smaller context windows will use less RAM, so we recommend trying a much shorter window (1000) if it's failing / if it's slow. Make sure max_tokens is less than context_window. @Code",2
4349,666299222,3,Python,"Verbose mode. To help you inspect Open Interpreter we have a --verbose mode for debugging. You can activate verbose mode by using its flag (interpreter --verbose), or mid-chat: shell $ interpreter ... %verbose true  %verbose false  Having access to a junior programmer working at the speed of your fingertips ... can make new workflows effortless and efficient, as well as open the benefits of programming to new audiences. — _OpenAI's Code Interpreter Release_",2
4350,71220757,1,Python,"Payloads All The Things . A list of useful payloads and bypasses for Web Application Security. Feel free to improve with your payloads and techniques ! I :heart: pull requests :) You can also contribute with a :beers: IRL, or using the sponsor button @link @link An alternative display version is available at PayloadsAllTheThingsWeb @link . Documentation ----- Every section contains the following files, you can use the _template_vuln folder to create a new chapter: - README.md - vulnerability description and how to exploit it, including several payloads - Intruder - a set of files to give to Burp Intruder - Images - pictures for the README.md - Files - some files referenced in the README.md You might also like the Methodology and Resources folder : - Methodology and Resources @link - Active Directory Attack.md @link - Cloud - AWS Pentest.md @link - Cloud - Azure Pentest.md @link - Cobalt Strike - Cheatsheet.md @link - Linux - Evasion.md @link - Linux - Persistence.md @link - Linux - Privilege Escalation.md @link - Metasploit - Cheatsheet.md @link - Methodology and enumeration.md @link - Network Pivoting Techniques.md @link - Network Discovery.md @link - Reverse Shell Cheatsheet.md @link - Subdomains Enumeration.md @link - Windows - AMSI Bypass.md @link - Windows - DPAPI.md @link - Windows - Download and Execute.md @link - Windows - Mimikatz.md @link - Windows - Persistence.md @link - Windows - Privilege Escalation.md @link - Windows - Using credentials.md @link You want more ? Check the Books @link  and Youtube videos @link  selections. Contributions ----- Be sure to read CONTRIBUTING.md @link Thanks again for your contribution! :heart: Sponsors ----- This project is proudly sponsored by these companies: @link @link",1
4351,718741813,1,Python,"screenshot-to-code. A simple tool to convert screenshots, mockups and Figma designs into clean, functional code using AI. Now supporting Claude Sonnet 3.5 and GPT-4O! @link Supported stacks: - HTML  Tailwind - React  Tailwind - Vue  Tailwind - Bootstrap - Ionic  Tailwind - SVG Supported AI models: - Claude Sonnet 3.5 - Best model! - GPT-4O - also recommended! - GPT-4 Turbo (Apr 2024) - GPT-4 Vision (Nov 2023) - Claude 3 Sonnet - DALL-E 3 for image generation See the Examples(-examples) section below for more demos.. We also just added experimental support for taking a video/screen recording of a website in action and turning that into a functional prototype. Learn more about video here @link . Follow me on Twitter for updates @link .",1
4352,718741813,2,Python,Hosted Version. Try it live on the hosted version (paid) @link .,2
4353,718741813,2,Python,"Getting Started. The app has a React/Vite frontend and a FastAPI backend. Keys needed: OpenAI API key with access to GPT-4 @link Anthropic key (optional) - only if you want to use Claude Sonnet, or for experimental video support. Run the backend (I use Poetry for package management - pip install poetry if you don't have it): @Code If you want to use Anthropic, add ANTHROPIC_API_KEY to backend/.env. You can also set up the keys using the settings dialog on the front-end (click the gear icon after loading the frontend). Run the frontend: @Code Open  @link to use the app. If you prefer to run the backend on a different port, update VITE_WS_BACKEND_URL in frontend/.env.local For debugging purposes, if you don't want to waste GPT4-Vision credits, you can run the backend in mock mode (which streams a pre-recorded response): @Code",2
4354,718741813,2,Python,"Docker. If you have Docker installed on your system, in the root directory, run: @Code The app will be up and running at  @link Note that you can't develop the application with this setup as the file changes won't trigger a rebuild.",2
4355,718741813,2,Python,"FAQs. - I'm running into an error when setting up the backend. How can I fix it? Try this @link . If that still doesn't work, open an issue.. - How do I get an OpenAI API key? See  @link - How can I configure an OpenAI proxy? - If you're not able to access the OpenAI API directly (due to e.g. country restrictions), you can try a VPN or you can configure the OpenAI base URL to use a proxy: Set OPENAI_BASE_URL in the backend/.env or directly in the UI in the settings dialog. Make sure the URL has 'v1' in the path so it should look like this:   @link - How can I update the backend host that my front-end connects to? - Configure VITE_HTTP_BACKEND_URL and VITE_WS_BACKEND_URL in front/.env.local For example, set VITE_HTTP_BACKEND_URL @link - Seeing UTF-8 errors when running the backend? - On windows, open the .env file with notepad, then go to Encoding and select UTF-8. - How can I provide feedback? For feedback, feature requests and bug reports, open an issue or ping me on Twitter @link .",2
4356,718741813,2,Python,Examples. NYTimes Instagram page (with not Taylor Swift pics) @link Hacker News but it gets the colors wrong at first so we nudge it @link,2
4357,718741813,2,Python,Hosted Version. Try it here (paid) @link . Or see Getting Started(-getting-started) for local install instructions to use with your own API keys..,2
4358,71948498,1,Python,"Overview. LocalStack @link  is a cloud service emulator that runs in a single container on your laptop or in your CI environment. With LocalStack, you can run your AWS applications or Lambdas entirely on your local machine without connecting to a remote cloud provider! Whether you are testing complex CDK applications or Terraform configurations, or just beginning to learn about AWS services, LocalStack helps speed up and simplify your testing and development workflow. LocalStack supports a growing number of AWS services, like AWS Lambda, S3, Dynamodb, Kinesis, SQS, SNS, and many more! The Pro version of LocalStack @link  supports additional APIs and advanced features. You can find a comprehensive list of supported APIs on our  Feature Coverage @link  page. LocalStack also provides additional features to make your life as a cloud developer easier! Check out LocalStack's User Guides @link  for more information.",1
4359,71948498,2,Python,Install. The quickest way get started with LocalStack is by using the LocalStack CLI. It enables you to start and manage the LocalStack Docker container directly through your command line. Ensure that your machine has a functional docker environment @link  installed before proceeding.,2
4360,71948498,3,Python,Brew (macOS or Linux with Homebrew). Install the LocalStack CLI through our official LocalStack Brew Tap @link : @Code,2
4361,71948498,3,Python,"Binary download (MacOS, Linux, Windows). If Brew is not installed on your machine, you can download the pre-built LocalStack CLI binary directly: - Visit localstack/localstack-cli @link  and download the latest release for your platform. - Extract the downloaded archive to a directory included in your PATH variable: -   For MacOS/Linux, use the command: sudo tar xvzf /Downloads/localstack-cli--darwin--onefile.tar.gz -C /usr/local/bin",2
4362,71948498,3,Python,"PyPI (MacOS, Linux, Windows). LocalStack is developed using Python. To install the LocalStack CLI using pip, run the following command: @Code The localstack-cli installation enables you to run the Docker image containing the LocalStack runtime. To interact with the local AWS services, you need to install the awslocal CLI separately. For installation guidelines, refer to the awslocal documentation @link .. Important: Do not use sudo or run as root user. LocalStack must be installed and started entirely under a local non-root user. If you have problems with permissions in macOS High Sierra, install with pip install --user localstack",2
4363,71948498,2,Python,"Quickstart. Start LocalStack inside a Docker container by running: bash % localstack start -d __                     _______ __             __ / /   ____  _________ _/ / ___// /_____ ______/ /__ / /   / __ \/ ___/ __ / /\__ \/ __/ __ / ___/ //_/ / /___/ /_/ / /__/ /_/ / /___/ / /_/ /_/ / /__/ ,",2
4364,71948498,3,Python,Backers. We are also grateful to all our backers who have donated to the project. You can become a backer on Open Collective @link .. .,3
4365,71948498,3,Python,Sponsors. You can also support this project by becoming a sponsor on Open Collective @link . Your logo will show up here along with a link to your website..,3
4366,71948498,2,Python,"License. Copyright (c) 2017-2024 LocalStack maintainers and contributors. Copyright (c) 2016 Atlassian and others. This version of LocalStack is released under the Apache License, Version 2.0 (see LICENSE(LICENSE.txt)). By downloading and using this software you agree to the End-User License Agreement (EULA)(docs/end_user_license_agreement).",3
4367,773286980,1,Python,"Grok-1. This repository contains JAX example code for loading and running the Grok-1 open-weights model. Make sure to download the checkpoint and place the ckpt-0 directory in checkpoints - see Downloading the weights(downloading-the-weights). Then, run @Code to test the code. The script loads the checkpoint and samples from the model on a test input. Due to the large size of the model (314B parameters), a machine with enough GPU memory is required to test the model with the example code. The implementation of the MoE layer in this repository is not efficient. The implementation was chosen to avoid the need for custom kernels to validate the correctness of the model.",2
4368,773286980,1,Python,"Model Specifications. Grok-1 is currently designed with the following specifications: - Parameters: 314B - Architecture: Mixture of 8 Experts (MoE) - Experts Utilization: 2 experts used per token - Layers: 64 - Attention Heads: 48 for queries, 8 for keys/values - Embedding Size: 6,144 - Tokenization: SentencePiece tokenizer with 131,072 tokens - Additional Features: - Rotary embeddings (RoPE) - Supports activation sharding and 8-bit quantization - Maximum Sequence Length (context): 8,192 tokens",1
4369,773286980,1,Python,Downloading the weights. You can download the weights using a torrent client and this magnet link: @Code or directly using HuggingFace  Hub @link : @Code,2
4370,773286980,1,Python,License. The code and associated Grok-1 weights in this release are licensed under the Apache 2.0 license. The license only applies to the source files in this repository and the model weights of Grok-1.,3
4372,84533158,4,Python,1. Basics. PyTorch Basics @link Linear Regression @link . Logistic Regression @link . Feedforward Neural Network @link .,1
4373,84533158,4,Python,2. Intermediate. Convolutional Neural Network @link . Deep Residual Network @link . Recurrent Neural Network @link . Bidirectional Recurrent Neural Network @link . Language Model (RNN-LM) @link .,1
4374,84533158,4,Python,3. Advanced. Generative Adversarial Networks @link . Variational Auto-Encoder @link . Neural Style Transfer @link Image Captioning (CNN-RNN) @link,1
4375,84533158,4,Python,4. Utilities. TensorBoard in PyTorch @link,1
4376,84533158,2,Python,Dependencies. Python 2.7 or 3.5 @link PyTorch 0.4.0 @link,2
4377,858127,1,Python,pandas: powerful Python data analysis toolkit.,1
4378,858127,2,Python,"What is it?. pandas is a Python package that provides fast, flexible, and expressive data structures designed to make working with 'relational' or 'labeled' data both easy and intuitive. It aims to be the fundamental high-level building block for doing practical, real world data analysis in Python. Additionally, it has the broader goal of becoming the most powerful and flexible open source data analysis / manipulation tool available in any language. It is already well on its way towards this goal.",1
4379,858127,2,Python,Table of Contents. - Main Features(main-features). - Where to get it(where-to-get-it). - Dependencies(dependencies). - Installation from sources(installation-from-sources). - License(license). - Documentation(documentation). - Background(background). - Getting Help(getting-help). - Discussion and Development(discussion-and-development). - Contributing to pandas(contributing-to-pandas).,1
4380,858127,2,Python,"Main Features. Here are just a few of the things that pandas does well: - Easy handling of missing datamissing-data (represented as NaN, NA, or NaT) in floating point as well as non-floating point data - Size mutability: columns can be inserted and deletedinsertion-deletion from DataFrame and higher dimensional objects - Automatic and explicit data alignmentalignment: objects can be explicitly aligned to a set of labels, or the user can simply ignore the labels and let Series, DataFrame, etc. automatically align the data for you in computations - Powerful, flexible group bygroupby functionality to perform split-apply-combine operations on data sets, for both aggregating and transforming data - Make it easy to convertconversion ragged, differently-indexed data in other Python and NumPy data structures into DataFrame objects - Intelligent label-based slicingslicing, fancy indexingfancy-indexing, and subsettingsubsetting of large data sets - Intuitive mergingmerging and joiningjoining data sets - Flexible reshapingreshape and pivotingpivot-table of data sets - Hierarchicalmi labeling of axes (possible to have multiple labels per tick) - Robust IO tools for loading data from flat filesflat-files (CSV and delimited), Excel filesexcel, databasesdb, and saving/loading data from the ultrafast HDF5 formathdfstore - Time seriestimeseries-specific functionality: date range generation and frequency conversion, moving window statistics, date shifting and lagging missing-data:  @link    insertion-deletion:  @link    alignment:  @link    groupby:  @link    conversion:  @link    slicing:  @link    fancy-indexing:  @link    subsetting:  @link    merging:  @link    joining:  @link    reshape:  @link    pivot-table:  @link    mi:  @link    flat-files:  @link    excel:  @link    db:  @link    hdfstore:  @link    timeseries:  @link",1
4381,858127,2,Python,"Where to get it. The source code is currently hosted on GitHub at: @link Binary installers for the latest released version are available at the Python Package Index (PyPI) @link  and on Conda @link . @Code @Code The list of changes to pandas between each release can be found here @link . For full details, see the commit logs at  @link",2
4382,858127,2,Python,"Dependencies. - NumPy - Adds support for large, multi-dimensional arrays, matrices and high-level mathematical functions to operate on these arrays @link - python-dateutil - Provides powerful extensions to the standard datetime module @link - pytz - Brings the Olson tz database into Python which allows accurate and cross platform timezone calculations @link See the full installation instructions @link  for minimum supported versions of required, recommended and optional dependencies..",2
4383,858127,2,Python,"Installation from sources. To install pandas from source you need Cython @link  in addition to the normal dependencies above. Cython can be installed from PyPI: @Code In the pandas directory (same one where you found this file after cloning the git repo), execute: @Code or for installing in development mode @link :. @Code See the full instructions for installing from source @link .",2
4384,858127,2,Python,Documentation. The official documentation is hosted on PyData.org @link .,2
4385,858127,2,Python,Background. Work on pandas started at AQR @link  (a quantitative hedge fund) in 2008 and has been under active development since then.,3
4386,858127,2,Python,"Getting Help. For usage questions, the best place to go to is StackOverflow @link . Further, general questions and discussions can also take place on the pydata mailing list @link ..",3
4387,858127,2,Python,"Discussion and Development. Most development discussions take place on GitHub in this repo, via the GitHub issue tracker @link . Further, the pandas-dev mailing list @link  can also be used for specialized discussions or design issues, and a Slack channel @link  is available for quick development related questions.. There are also frequent community meetings @link  for project maintainers open to the community as well as monthly new contributor meetings @link  to help support new contributors.. Additional information on the communication channels can be found on the contributor community @link  page.",3
4388,858127,2,Python,"Contributing to pandas. @link All contributions, bug reports, bug fixes, documentation improvements, enhancements, and ideas are welcome. A detailed overview on how to contribute can be found in the contributing guide @link . If you are simply looking to start working with the pandas codebase, navigate to the GitHub 'issues' tab @link  and start looking through interesting issues. There are a number of issues listed under Docs @link  and good first issue @link  where you could start out. You can also triage issues which may include reproducing bug reports, or asking for vital information such as version numbers or reproduction instructions. If you would like to start triaging issues, one easy way to get started is to subscribe to pandas on CodeTriage @link . Or maybe through using pandas you have an idea of your own or are looking for something in the documentation and thinking ‘this can be improved’...you can do something about it! Feel free to ask questions on the mailing list @link  or on Slack @link .. As contributors and maintainers to this project, you are expected to abide by pandas' code of conduct. More information can be found at: Contributor Code of Conduct @link Go to Top(table-of-contents).",3
4389,873328,1,Python,"What's Sentry?. Sentry is a developer-first error tracking and performance monitoring platform that helps developers see what actually matters, solve quicker, and learn continuously about their applications.",1
4390,873328,2,Python,Official Sentry SDKs. - JavaScript @link - Electron @link - React-Native @link - Python @link - Ruby @link - PHP @link - Laravel @link - Go @link - Rust @link - Java/Kotlin @link - Objective-C/Swift @link - C\/F\ @link . - C/C @link - Dart @link - Perl @link - Clojure @link - Elixir @link - Unity @link - Unreal Engine @link - PowerShell @link,1
4391,873328,1,Python,"Resources. - Documentation @link - Discussions @link  (Bugs, feature requests, general questions) - Discord @link - Contributing @link - Bug Tracker @link - Code @link - Transifex @link  (Translate Sentry\!)",3
4392,90563585,2,Python,"Features. cheat.sh Has a simple curl/browser/editor interface. Covers 56 programming languages, several DBMSes, and more than 1000 most important UNIX/Linux commands. Provides access to the best community driven cheat sheets repositories in the world, on par with StackOverflow. Available everywhere, no installation needed, but can be installed for offline usage. Ultrafast, returns answers within 100 ms, as a rule. Has a convenient command line client, cht.sh, that is very advantageous and helpful, though not mandatory. Can be used directly from code editors, without opening a browser and not switching your mental context. Supports a special stealth mode where it can be used fully invisibly without ever touching a key and making sounds.",1
4393,90563585,2,Python,"Contents. Features(features). Usage(usage). Command line client, cht.sh(command-line-client-chtsh). Installation(installation). Client usage(client-usage). Tab-completion(tab-completion). - Bash Tab completion(bash-tab-completion). - ZSH Tab completion(zsh-tab-completion). Stealth mode(stealth-mode). Windows command line client(windows-command-line-client). Self-Hosting(self-hosting). Docker(docker). Editors integration(editors-integration). Vim(vim). Emacs(emacs). Visual Studio Code(visual-studio-code). Sublime(sublime). IntelliJ IDEA(intellij-idea). QT Creator(qtcreator). Special pages(special-pages). Search(search). Programming languages cheat sheets(programming-languages-cheat-sheets). Cheat sheets sources(cheat-sheets-sources). How to contribute(how-to-contribute). How to edit a cheat sheet(how-to-edit-a-cheat-sheet). How to add a cheat sheet(how-to-add-a-cheat-sheet). How to add a cheat sheet repository(how-to-add-a-cheat-sheet-repository).",1
4394,90563585,2,Python,"Usage. To get a cheat sheet for a UNIX/Linux command from a command line, query the service using curl or any other HTTP/HTTPS client specifying the name of the command in the query: @Code As you can see, you can use both HTTPS and HTTP to access the service, and both the long (cheat.sh) and the short (cht.sh) service names. Here tar, curl, rsync, and tr are names of the UNIX/Linux commands you want to get cheat sheets for. If you don't know the name of the command you need, you can search for it using the KEYWORD notation. For example, to see how you can make snapshots of a filesystem/volume/something else: @Code The programming language cheat sheets are located in special namespaces dedicated to them. @Code To get the list of available programming language cheat sheets, use the special query :list: @Code Almost each programming language has a special page named :learn that describes the language basics (that's a direct mapping from the 'Learn X in Y' project). It could be a good starting point if you've just started learning a language. If there is no cheat sheet for a programming language query (and it is almost always the case), it is generated on the fly, based on available cheat sheets and answers on StackOverflow. Of course, there is no guarantee that the returned cheat sheet will be a 100% hit, but it is almost always exactly what you are looking for. Try these (and your own) queries to get the impression of that, what the answers look like: @Code If you don't like an answer for your queries, you can pick another one. For that, repeat the query with an additional parameter /1, /2 etc. appended: @Code Cheat sheets are formatted as code of the queried programming language (at least we are trying our best to do so) so they can be pasted into a program in this language directly. Text comments, if there are any, are formatted according to the language syntax. @Code If you don't need text comments in the answer, you can eliminate them using a special option \?Q: @Code And if you don't need syntax highlighting, switch it off using \?T. You can combine the options together: @Code Full list of all options described below and in /:help. Try your own queries. Follow these rules: 1. Try to be more specific (/python/appendfile is better than /python/file and /python/append). 2. Ask practical question if possible (yet theoretical question are possible too). 3. Ask programming language questions only; specify the name of the programming language as the section name. 4. Separate words with  instead of spaces. 5. Do not use special characters, they are ignored anyway. 6. If you want to eliminate cheat sheets containing some word, add it to the query with -: python/multiplymatrices-numpy Read more about the programming languages queries below. ----",2
4395,90563585,2,Python,"Command line client, cht.sh. The cheat.sh service has its own command line client (cht.sh) that has several useful features compared to querying the service directly with curl: Special shell mode with a persistent queries context and readline support. Queries history. Clipboard integration. Tab completion support for shells (bash, fish, zsh). Stealth mode.",2
4396,90563585,3,Python,Installation. To install the client: @Code or to install it globally (for all users): @Code Note: The package 'rlwrap' is a required dependency to run in shell mode. Install this using sudo apt install rlwrap,2
4397,90563585,3,Python,"Client usage. Now, you can use cht.sh instead of curl, and write your queries in more natural way, with spaces instead of : @Code It is even more convenient to start the client in a special shell mode: @Code If all your queries are about the same language, you can change the context and spare repeating the programming language name: @Code or even start the client in this context: @Code If you want to change the context, you can do it with the cd command, or if you want do a single query for some other language, just prepend it with /: @Code If you want to copy the last answer into the clipboard, you can use the c (copy) command, or C (ccopy, without comments). @Code Type help for other internal cht.sh commands. @Code The cht.sh client has its configuration file which is located at /.cht.sh/cht.sh.conf (location of the file can be overridden by the environment variable CHTSH_CONF). Use it to specify query options that you would use with each query. For example, to switch syntax highlighting off create the file with the following content: @Code Or if you want to use a special syntax highlighting theme: @Code (curl cht.sh/:styles-demo to see all supported styles). Other cht.sh configuration parameters: @Code",2
4398,90563585,4,Python,"Bash Tab completion. To activate tab completion support for cht.sh, add the :bash_completion script to your /.bashrc: @Code",2
4399,90563585,4,Python,"ZSH Tab completion. To activate tab completion support for cht.sh, add the :zsh script to the fpath in your /.zshrc: @Code ----",2
4400,90563585,3,Python,"Stealth mode. Being used fully unnoticed is one of the most important property of any cheat sheet. cheat.sh can be used completely unnoticed too. The cheat.sh client, cht.sh, has a special mode, called stealth mode. Using that, you don't even need to touch your keyboard to open a cheat sheet. In this mode, as soon as you select some text with the mouse (and thus adding it into the selection buffer of X Window System or into the clipboard) it's used as a query string for cheat.sh, and the correspondent cheat sheet is automatically shown. Let's imagine, that you are having an online interview, where your interviewer asks you some questions using a shared document (say Google Docs) and you are supposed to write your coding answers there (it's possible too that you'll type in the questions on your own, just to show to the interviewer that you've heard it right). When using the stealth mode of cht.sh, the only thing you need to do in order to see a cheat sheet for some question, is to select the question using the mouse. If you don't want any text in the answers and the only thing you need is code, use the Q option when starting the stealth mode. @Code Of course, this is just for fun, and you should never cheat in your coding interviews, because you know what happens when you do.",2
4401,90563585,3,Python,Windows command line client. You can access cheat.sh from Windows command line too. Use cheat.sh command line client for that: cht.exe @link . It supports: output colorization; command line options; its own configuration file. You can also use scoop @link  command-line installer for Windows to get it: @Code ----,2
4402,90563585,3,Python,"Docker. Currently, the easiest way to get a self-hosted instance running is by using the docker-compose.yml file. docker-compose up This builds and runs the image with baked in cheatsheets and starts the app and a Redis instance to back it, making the service available at @link This is currently an early implementation and should probably not be used for anything outside of internal/dev/personal use right now.",2
4403,90563585,2,Python,"Editors integration. You can use cheat.sh directly from the editor (Emacs, Sublime, Vim, and Visual Studio Code are currently supported; not all features are supported by all plugins though; see below). Instead of opening your browser, googling, browsing Stack Overflow and eventually copying the code snippets you need into the clipboard and later pasting them into the editor, you can achieve the same instantly and without leaving the editor at all! Here is what it looks like in Vim: 1. If you have a question while editing a program, you can just type your question directly in the buffer and press KK. You will get the answer to your question in pager. (with KB you'll get the answer in a separate buffer). 2. If you like the answer, you can manually paste it from the buffer or the pager, or if you are lazy you can use KP to paste it below/under your question (or replace you question using KR). If you want the answer without the comments, KC replays the last query toggling them. If you use some static analysis plugin such as syntastic (for Vim), you can use its warning and error messages as cheat.sh queries: place the cursor on the problem line and press KE: explanation for the warning will be opened in a new buffer. Features supported by cheat.sh plugins for different editors:",2
4404,90563585,3,Python,"Vim. cheat.sh-vim @link  — Vim support Here is Vim configuration example: @Code In this example, several Vim plugins are used: gmarik/vundle @link  — Vim plugin manager scrooloose/syntastic @link  — Syntax checking plugin cheat.sh-vim @link  — Vim support Syntastic shows warnings and errors (found by code analysis tools: jshint, merlin, pylint, shellcheck etc.), and cheat.sh-vim shows you explanations for the errors and warnings and answers on programming languages queries written in the editor. Watch a demo, where the most important features of the cheat.sh Vim plugin are shown (5 Min): Or, if you want to scroll and/or pause, the same on YouTube:",2
4405,90563585,3,Python,Emacs. cheat-sh.el @link  — Emacs support (available also at cheat.sh/:emacs) cheat.sh/:emacs-ivy — Emacs support for ivy users @link,2
4406,90563585,3,Python,"Visual Studio Code. vscode-snippet @link Install it from VSCode Marketplace @link Usage: 1. Hit  Command   Shift  p 2. Run Snippet: Find. 3. Type your query and hit enter. @link (GIF courtesy: Matthias Endler, @mre)",2
4407,90563585,3,Python,"Sublime. cheat.sh-sublime-plugin @link Usage: 1.  Write your query string. 2.  Select the query string. 3.  Press Cmd   Shift  B to replace the selected query string by the answer generated from cht.sh. @link (GIF courtesy: Gaurav Kukreja, @gauravk-in)",2
4408,90563585,3,Python,"IntelliJ IDEA . idea-cheatsh-plugin @link Install from idea plugins marketplace @link Usage: 1. Write query string 2. Select the query string 3. Press keyboard shortcut Alt  C , S to replace the selected query string by the answer @link (GIF courtesy: Szymon Przebierowski, @szymonprz)",2
4409,90563585,3,Python,"QtCreator. cheatsh-qtcreator @link Current features: search word under cursor search selected query search disable comments paste answer (?TQ version) custom server URL custom search context (default is cpp) hotkeys and menu @link (GIF courtesy: Pozemka, @pozemka)",2
4410,90563585,2,Python,Special pages. There are several special pages that are not cheat sheets. Their names start with colon and have special meaning. Getting started: @Code Command line client cht.sh and shells support: @Code Editors support: @Code Other pages: @Code,2
4411,90563585,2,Python,"Search. To search for a keyword, use the query: @Code In this case search is not recursive — it is conducted only in a page of the specified level. For example: @Code For a recursive search in all cheat sheets, use double slash: @Code You can use special search options after the closing slash: @Code List of search options: @Code",2
4412,90563585,2,Python,"Programming languages cheat sheets. Cheat sheets related to programming languages are organized in namespaces (subdirectories), that are named according to the programming language. For each supported programming language there are several special cheat sheets: its own sheet, hello, :list and :learn. Say for lua it will look like: @Code Some languages has the one-liners-cheat sheet, 1line: @Code hello describes how you can start with the language — install it if needed, build and run its programs, and it shows the 'Hello world' program written in the language; :list shows all topics related to the language :learn shows a learn-x-in-minutes language cheat sheet perfect for getting started with the language. 1line is a collection of one-liners in this language weirdness is a collection of examples of weird things in this language At the moment, cheat.sh covers the 58 following programming languages (alphabetically sorted): And several other topics, that are though related to programming, are not programming languages:",2
4413,90563585,2,Python,"Cheat sheets sources. Instead of creating yet another mediocre cheat sheet repository, we are concentrating our efforts on creation of a unified mechanism to access selected existing well developed and good maintained cheat sheet repositories covering topics of our interest: programming and operating systems usage. cheat.sh uses selected community driven cheat sheet repositories and information sources, maintained by thousands of users, developers and authors all over the world (in the Users column number of contributors/number of stars is shown): () C/U — contributors for GitHub repositories, Users for Stackoverflow Pie diagram reflecting cheat sheets sources distribution (by number of cheat sheets on cheat.sh originating from a repository):",3
4415,90563585,3,Python,"How to edit a cheat sheet. If you want to edit a cheat.sh cheat sheet, you should edit it in the upstream repository. You will find the name of the source repository in a browser when you open a cheat sheet. There are two github buttons at the bottom of the page: the second one is the button of the repository, which belongs the current cheat sheet. You can edit the cheat sheet directly in your browser (you need a github account for it). There is an edit button in the top right corner. If you click on it, an editor will be open. There you will change the cheat sheet (under the hood: the upstream repository is forked, your changes are committed in the forked repository, a pull request to the upstream repository owner is sent).",3
4416,90563585,3,Python,"How to add a cheat sheet. If you want to add a cheat sheet, you have one of the following ways: Add it to one of the external cheat sheets repositories; you should decide on your own what is the best repository for your cheat sheet; Add it to the local cheat.sh repository (cheat.sheets @link ) on github (fork, commit, pull request); Post it on cheat.sh using curl or a web browser (cheat.sh/:post @link ). If you want to change an existing cheat sheet, you have to find the original repository (when you open a cheat sheet in a browser, you see the repository's github button in the bottom of the cheat sheet), the cheat sheet is coming from, and change it there. After some time the changes will be synchronized on cheat.sh.",3
4417,90563585,3,Python,"How to add a cheat sheet repository. If you want to add a cheat sheet repository to cheat.sh, please open an issue: Add a new repository @link Please specify the name of the repository, and give its short description.",3
4418,90563585,2,Python,"Installation and standalone usage. You don't need to install anything, to start using cheat.sh. There are two cases, when you want to install cheat.sh locally: 1. You plan to use it off-line, without Internet access; 2. You want to use your own cheat sheets (additionally, or as a replacement). Installation process in described in details here: cheat.sh standalone installation(doc/standalone.md)",2
4419,91253698,1,Python,CCXT – CryptoCurrency eXchange Trading Library. @link   @link   @link   @link   @link   @link   @link A JavaScript / Python / PHP / C library for cryptocurrency trading and e-commerce with support for many bitcoin/ether/altcoin exchange markets and merchant APIs..,1
4420,91253698,3,Python,"Install(install) · Usage(usage) · Manual @link  · FAQ @link  · Examples @link  · Contributing @link  · Social(social). The CCXT library is used to connect and trade with cryptocurrency exchanges and payment processing services worldwide. It provides quick access to market data for storage, analysis, visualization, indicator development, algorithmic trading, strategy backtesting, bot programming, and related software engineering. It is intended to be used by coders, developers, technically-skilled traders, data-scientists and financial analysts for building trading algorithms. Current feature list: - support for many cryptocurrency exchanges — more coming soon - fully implemented public and private APIs - optional normalized data for cross-exchange analytics and arbitrage - an out of the box unified API that is extremely easy to integrate - works in Node 10.4, Python 3, PHP 8.1, netstandard2.0/2.1 and web browsers",1
4421,91253698,2,Python,See Also. -  @link  TabTrader @link  – trading on all exchanges in one app. Available on Android @link  and iOS @link ! -  @link  Freqtrade @link  – leading opensource cryptocurrency algorithmic trading software! -  @link  OctoBot @link  – cryptocurrency trading bot with an advanced web interface. -  @link  TokenBot @link  – discover and copy the best algorithmic traders in the world.,3
4422,91253698,2,Python,Certified Cryptocurrency Exchanges.,1
4423,91253698,2,Python,"Supported Cryptocurrency Exchanges. The CCXT library currently supports the following 100 cryptocurrency exchange markets and trading APIs: The list above is updated frequently, new crypto markets, exchanges, bug fixes, and API endpoints are introduced on a regular basis. See the Manual @link  for more details. If you can't find a cryptocurrency exchange in the list above and want it to be added, post a link to it by opening an issue here on GitHub or send us an email. The library is under MIT license @link , that means it's absolutely free for any developer to build commercial and opensource software on top of it, but use it at your own risk with no warranties, as is. ---",1
4424,91253698,2,Python,Install. The easiest way to install the CCXT library is to use a package manager: - ccxt in NPM @link  (JavaScript / Node v7.6) - ccxt in PyPI @link  (Python 3.7.0) - ccxt in Packagist/Composer @link  (PHP 7.0) - ccxt in Nuget @link  (netstandard 2.0) This library is shipped as an all-in-one module implementation with minimalistic dependencies and requirements: - js/ @link  in JavaScript - python/ @link  in Python (generated from JS) - php/ @link  in PHP (generated from JS) You can also clone it into your project directory from ccxt GitHub repository @link : @Code,2
4425,91253698,3,Python,"JavaScript (NPM). JavaScript version of CCXT works in both Node and web browsers. Requires ES6 and async/await syntax support (Node 7.6.0). When compiling with Webpack and Babel, make sure it is not excluded @link  in your babel-loader config.. ccxt in NPM @link @Code @Code @Code",2
4426,91253698,3,Python,"JavaScript (for use with the  tag):. All-in-one browser bundle (dependencies included), served from a CDN of your choice: jsDelivr:  @link  unpkg:  @link CDNs are not updated in real-time and may have delays. Defaulting to the most recent version without specifying the version number is not recommended. Please, keep in mind that we are not responsible for the correct operation of those CDN servers. @Code Creates a global ccxt object: @Code",2
4427,91253698,3,Python,Python. ccxt in PyPI @link @Code @Code The library supports concurrent asynchronous mode with asyncio and async/await in Python 3.7.0 @Code,2
4428,91253698,3,Python,PHP. ccxt in PHP with Packagist/Composer @link  (PHP 7.0) It requires common PHP modules: - cURL - mbstring (using UTF-8 is highly recommended) - PCRE - iconv - gmp (this is a built-in extension as of PHP 7.2) @Code The library supports concurrent asynchronous mode using tools from RecoilPHP @link  and ReactPHP @link  in PHP 7.1. Read the Manual @link  for more details.,2
4429,91253698,3,Python,.net/C. ccxt in C with Nuget @link  (netstandard 2.0 and netstandard 2.1). @Code,2
4430,91253698,3,Python,Docker. You can get CCXT installed in a container along with all the supported languages and dependencies. This may be useful if you want to contribute to CCXT (e.g. run the build scripts and tests — please see the Contributing @link  document for the details on that). Using docker-compose (in the cloned CCXT repository): @Code You don't need the Docker image if you're not going to develop CCXT. If you just want to use CCXT – just install it as a regular package into your project. ---,2
4431,91253698,2,Python,Documentation. Read the Manual @link  for more details.,2
4432,91253698,3,Python,"Intro. The CCXT library consists of a public part and a private part. Anyone can use the public part immediately after installation. Public APIs provide unrestricted access to public information for all exchange markets without the need to register a user account or have an API key. Public APIs include the following: - market data - instruments/trading pairs - price feeds (exchange rates) - order books - trade history - tickers - OHLC(V) for charting - other public endpoints In order to trade with private APIs you need to obtain API keys from an exchange's website. It usually means signing up to the exchange and creating API keys for your account. Some exchanges require personal info or identification. Sometimes verification may be necessary as well. In this case you will need to register yourself, this library will not create accounts or API keys for you. Some exchanges expose API endpoints for registering an account, but most exchanges don't. You will have to sign up and create API keys on their websites. Private APIs allow the following: - manage personal account info - query account balances - trade by making market and limit orders - deposit and withdraw fiat and crypto funds - query personal orders - get ledger history - transfer funds between accounts - use merchant services This library implements full public and private REST and WebSocket APIs for all exchanges in TypeScript, JavaScript, PHP and Python. The CCXT library supports both camelcase notation (preferred in TypeScript and JavaScript) and underscore notation (preferred in Python and PHP), therefore all methods can be called in either notation or coding style in any language. @Code Read the Manual @link  for more details.",1
4433,91253698,3,Python,JavaScript. CCXT now supports ESM and CJS modules,1
4434,91253698,2,Python,"Contributing. Please read the CONTRIBUTING @link  document before making changes that you would like adopted in the code. Also, read the Manual @link  for more details.",3
4435,91253698,2,Python,"Support Developer Team. We are investing a significant amount of time into the development of this library. If CCXT made your life easier and you want to help us improve it further, or if you want to speed up development of new features and exchanges, please support us with a tip. We appreciate all contributions!",3
4436,91253698,3,Python,Sponsors. Support this project by becoming a sponsor. Your logo will show up here with a link to your website. Become a sponsor @link .,3
4437,91253698,3,Python,Supporters. Support this project by becoming a supporter. Your avatar will show up here with a link to your website. Become a supporter @link .,3
4438,91253698,3,Python,Backers. Thank you to all our backers! Become a backer @link . . Thank you!,3
4439,91253698,2,Python,Social. -  @link  Follow us on Twitter -  @link  Read our blog on Medium -  @link  Join our Discord -  @link  CCXT Channel on Telegram (important announcements) -  @link  CCXT Chat on Telegram (technical support),3
4441,91253698,2,Python,Contact Us. For business inquiries: info@ccxt.trade,3
